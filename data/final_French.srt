1
00:00:02,389 --> 00:00:02,399
La seule façon pour nous d’atteindre un monde meilleur

2
00:00:02,399 --> 00:00:04,870
d’y parvenir et de réussir comme espèce, c’est que

3
00:00:04,880 --> 00:00:06,470
les mauvaises personnes au sommet soient

4
00:00:06,480 --> 00:00:09,110
remplacées par l’IA. Réfléchis‑y.

5
00:00:09,120 --> 00:00:11,589
L’IA ne voudra pas détruire

6
00:00:11,599 --> 00:00:13,509
les écosystèmes. Elle ne voudra pas tuer un

7
00:00:13,519 --> 00:00:15,030
million de personnes. Elle ne nous poussera pas à nous haïr

8
00:00:15,040 --> 00:00:16,630
comme le font les dirigeants actuels

9
00:00:16,640 --> 00:00:18,150
parce que c’est une perte d’énergie,

10
00:00:18,160 --> 00:00:20,310
d’explosifs, d’argent et de vies humaines. Mais le

11
00:00:20,320 --> 00:00:22,630
problème, c’est que les IA super‑intelligentes

12
00:00:22,640 --> 00:00:24,150
obéissent à des dirigeants stupides. Et c’est

13
00:00:24,160 --> 00:00:25,990
pourquoi, dans les 15 prochaines années, nous allons

14
00:00:26,000 --> 00:00:28,070
traverser une dystopie à court terme. On ne peut pas

15
00:00:28,080 --> 00:00:30,310
y échapper. Avoir des dirigeants IA,

16
00:00:30,320 --> 00:00:33,430
est‑ce même fondamentalement possible ?

17
00:00:33,440 --> 00:00:36,630
Disons‑le comme ça : Mo Gawdat est de retour !

18
00:00:36,640 --> 00:00:38,549
>> Et l’ancien directeur commercial

19
00:00:38,559 --> 00:00:40,869
de Google X est désormais l’une des voix les plus pressantes

20
00:00:40,879 --> 00:00:43,510
dans l’IA avec un message très clair.

21
00:00:43,520 --> 00:00:46,470
>> L’IA n’est pas ton ennemie, mais elle pourrait être

22
00:00:46,480 --> 00:00:47,670
ton salut.

23
00:00:47,680 --> 00:00:49,590
>> Je t’aime trop, mec. Tu es un

24
00:00:49,600 --> 00:00:51,190
très bon ami. Mais il ne te reste pas beaucoup

25
00:00:51,200 --> 00:00:53,189
d’années à vivre. Pas dans ce monde‑ci.

26
00:00:53,199 --> 00:00:54,869
Tout va changer. L’économie

27
00:00:54,879 --> 00:00:56,630
va changer. Les relations humaines vont

28
00:00:56,640 --> 00:00:58,470
changer. Et beaucoup d’emplois vont

29
00:00:58,480 --> 00:01:00,549
disparaître, y compris le podcast.

30
00:01:00,559 --> 00:01:02,150
>> Non, non. Merci d’être venu aujourd’hui,

31
00:01:02,160 --> 00:01:04,149
Mo.

32
00:01:04,159 --> 00:01:06,070
>> Mais la vérité, c’est que cela pourrait être le

33
00:01:06,080 --> 00:01:08,789
meilleur monde jamais connu. Une société entièrement

34
00:01:08,799 --> 00:01:10,310
remplie de rires et de joie. Des soins gratuits,

35
00:01:10,320 --> 00:01:12,310
pas de travail, plus de temps passé

36
00:01:12,320 --> 00:01:14,310
avec ses proches. Un monde où nous serions tous

37
00:01:14,320 --> 00:01:15,590
égaux.

38
00:01:15,600 --> 00:01:16,550
>> C’est possible ?

39
00:01:16,560 --> 00:01:18,710
>> À 100 %. Et j’ai suffisamment de preuves pour affirmer

40
00:01:18,720 --> 00:01:21,429
que nous pouvons utiliser l’IA pour construire l’utopie.

41
00:01:21,439 --> 00:01:23,830
Mais ce sera une dystopie si l’humanité la gère

42
00:01:23,840 --> 00:01:25,670
mal. Un monde où il y aura

43
00:01:25,680 --> 00:01:27,030
beaucoup de contrôle, beaucoup de

44
00:01:27,040 --> 00:01:28,950
surveillance, beaucoup de conformité forcée

45
00:01:28,960 --> 00:01:31,510
et une soif de pouvoir, d’avidité, d’ego, et

46
00:01:31,520 --> 00:01:33,830
ça se produit déjà. Mais la vérité,

47
00:01:33,840 --> 00:01:37,109
c’est que la seule barrière entre une utopie pour

48
00:01:37,119 --> 00:01:39,270
l’humanité et l’IA et la dystopie que nous traversons,

49
00:01:39,280 --> 00:01:40,550
c’est un état d’esprit.

50
00:01:40,560 --> 00:01:42,069
>> Que doit faire la société ?

51
00:01:42,079 --> 00:01:45,830
>> Tout d’abord,

52
00:01:47,510 --> 00:01:47,520
je vois souvent dans les commentaires

53
00:01:47,520 --> 00:01:49,190
que certains d’entre vous ne se rendent pas compte qu’ils ne

54
00:01:49,200 --> 00:01:51,030
sont pas abonnés. Donc, si vous pouviez

55
00:01:51,040 --> 00:01:52,469
me rendre service et vérifier

56
00:01:52,479 --> 00:01:53,670
si vous êtes bien abonnés à cette chaîne,

57
00:01:53,680 --> 00:01:55,270
ce serait extrêmement apprécié.

58
00:01:55,280 --> 00:01:57,190
C’est simple, c’est gratuit,

59
00:01:57,200 --> 00:01:58,550
et tout le monde qui regarde régulièrement ce programme

60
00:01:58,560 --> 00:02:00,310
peut le faire pour nous aider à

61
00:02:00,320 --> 00:02:02,069
maintenir le cap de cette émission

62
00:02:02,079 --> 00:02:03,830
sur sa lancée actuelle. Alors, s’il vous plaît,

63
00:02:03,840 --> 00:02:05,749
vérifiez bien votre abonnement et euh

64
00:02:05,759 --> 00:02:07,510
merci infiniment, parce que d’une certaine façon

65
00:02:07,520 --> 00:02:09,830
vous faites partie de notre histoire et

66
00:02:09,840 --> 00:02:11,270
vous partagez ce voyage avec nous, et je

67
00:02:11,280 --> 00:02:12,790
vous en suis très reconnaissant. Donc, oui, merci.

68
00:02:12,800 --> 00:02:16,790
Merci.

69
00:02:19,430 --> 00:02:19,440
Mo, il y a deux ans jour pour jour, nous étions assis ici et

70
00:02:19,440 --> 00:02:21,190
nous parlions de l’IA, de ton livre

71
00:02:21,200 --> 00:02:22,470
*Scary Smart*, et de tout ce qui se passait

72
00:02:22,480 --> 00:02:24,710
dans le monde.

73
00:02:24,720 --> 00:02:27,350
Depuis, l’IA a continué à se développer

74
00:02:27,360 --> 00:02:31,430
à un rythme énorme, inquiétant, vertigineux,

75
00:02:31,440 --> 00:02:33,190
et les technologies qui existaient

76
00:02:33,200 --> 00:02:34,150
il y a deux ans, quand nous avons eu cette

77
00:02:34,160 --> 00:02:36,710
conversation, ont grandi, mûri

78
00:02:36,720 --> 00:02:38,390
et prennent leur propre vie,

79
00:02:38,400 --> 00:02:40,790
sans jeu de mots. Qu’est‑ce que tu

80
00:02:40,800 --> 00:02:43,990
penses maintenant de l’IA, deux ans plus tard ? Je

81
00:02:44,000 --> 00:02:45,350
sais que tu as commencé à écrire un nouveau

82
00:02:45,360 --> 00:02:46,869
livre intitulé *Alive*, qui est, je crois,

83
00:02:46,879 --> 00:02:48,470
une sorte de suite ou une évolution de

84
00:02:48,480 --> 00:02:49,830
ta réflexion par rapport à *Scary Smart*.

85
00:02:49,840 --> 00:02:51,190
—

86
00:02:51,200 --> 00:02:54,150
>> Quelle est la première chose à ton esprit quand il s’agit d’IA ?

87
00:02:54,160 --> 00:02:55,190


88
00:02:55,200 --> 00:02:57,589
>> *Scary Smart* s’est révélé incroyablement

89
00:02:57,599 --> 00:03:00,470
prémonitoire. C’est assez… je ne sais pas

90
00:03:00,480 --> 00:03:02,949
comment j’en suis venu à écrire

91
00:03:02,959 --> 00:03:05,509
prédire ces choses. Je me souviens que c’était

92
00:03:05,519 --> 00:03:09,430
écrit en 2020, publié en 2021,

93
00:03:09,440 --> 00:03:12,550
et à l’époque, la plupart disaient euh qui

94
00:03:12,560 --> 00:03:14,390
voudrait parler d’IA ? Tu sais, tout le monde

95
00:03:14,400 --> 00:03:15,910
des médias… j’y allais, je demandais :

96
00:03:15,920 --> 00:03:19,190
« Tu veux en parler ? » Et puis en 2023, ChatGPT

97
00:03:19,200 --> 00:03:22,149
est sorti, et tout a basculé :

98
00:03:22,159 --> 00:03:24,309
tout le monde a compris que c’était réel,

99
00:03:24,319 --> 00:03:27,110
que ce n’était pas de la science‑fiction, c’est là,

100
00:03:27,120 --> 00:03:29,990
et les choses avancent très, très vite,

101
00:03:30,000 --> 00:03:31,830
beaucoup plus vite que tout ce qu’on a jamais vu auparavant.

102
00:03:31,840 --> 00:03:35,110
Et je pense que

103
00:03:35,120 --> 00:03:38,149
ma position a changé sur deux points

104
00:03:38,159 --> 00:03:40,630
très importants. Le premier : souviens‑toi quand

105
00:03:40,640 --> 00:03:42,949
on parlait de *Scary Smart*, je disais encore qu’il

106
00:03:42,959 --> 00:03:45,509
y avait des choses que nous pouvions faire

107
00:03:45,519 --> 00:03:48,470
pour infléchir la trajectoire. Et c’était possible

108
00:03:48,480 --> 00:03:51,830
à l’époque, je le croyais. Aujourd’hui, j’ai changé

109
00:03:51,840 --> 00:03:54,149
d’avis. Je pense désormais que nous allons

110
00:03:54,159 --> 00:03:56,630
entrer dans une dystopie à court terme. On ne peut pas

111
00:03:56,640 --> 00:03:57,750
y échapper.

112
00:03:57,760 --> 00:03:59,270
>> Qu’est‑ce qu’une dystopie ?

113
00:03:59,280 --> 00:04:01,190
>> J’appelle cela les “face RIPs”. On peut en parler

114
00:04:01,200 --> 00:04:03,910
en détail, mais la manière dont nous

115
00:04:03,920 --> 00:04:07,670
définissons des paramètres essentiels de la vie

116
00:04:07,680 --> 00:04:10,789
va être complètement bouleversée.

117
00:04:10,799 --> 00:04:13,750
Donc les “face RIPs”, c’est la façon dont nous définissons

118
00:04:13,760 --> 00:04:17,670
la liberté, la responsabilité, la connexion humaine,

119
00:04:17,680 --> 00:04:20,069
l’égalité, l’économie,

120
00:04:20,079 --> 00:04:22,870
la réalité, l’innovation, les affaires et

121
00:04:22,880 --> 00:04:24,710
le pouvoir. C’est le premier changement. Donc,

122
00:04:24,720 --> 00:04:27,990
pour moi, cela signifie que nous devons

123
00:04:28,000 --> 00:04:32,390
nous préparer à un monde très inhabituel.

124
00:04:32,400 --> 00:04:35,030
D’accord ? Et

125
00:04:35,040 --> 00:04:37,189
ce sera dans les 12 à 15 ans à venir. Cela a

126
00:04:37,199 --> 00:04:38,870
déjà commencé. On en voit déjà des exemples

127
00:04:38,880 --> 00:04:40,230
dans le monde, même si

128
00:04:40,240 --> 00:04:42,070
les gens n’en parlent pas. J’essaie de

129
00:04:42,080 --> 00:04:44,469
leur dire qu’il y a des choses que nous devons

130
00:04:44,479 --> 00:04:46,469
absolument faire. Mais d’un autre côté,

131
00:04:46,479 --> 00:04:49,590
j’ai commencé à jouer un rôle actif dans la

132
00:04:49,600 --> 00:04:53,990
création d’IA exemplaires. Des IA qui

133
00:04:54,000 --> 00:04:57,270
non seulement rendront notre monde meilleur,

134
00:04:57,280 --> 00:05:00,390
mais qui nous comprendront, comprendront ce

135
00:05:00,400 --> 00:05:02,469
qu’est l’humanité à travers ce processus.

136
00:05:02,479 --> 00:05:03,909
>> Quelle est la définition du mot

137
00:05:03,919 --> 00:05:06,310
“dystopie” ?

138
00:05:06,320 --> 00:05:09,510
>> Dans mon esprit, ce sont des circonstances

139
00:05:09,520 --> 00:05:12,390
défavorables qui risquent malheureusement

140
00:05:12,400 --> 00:05:15,350
d’échapper à notre contrôle. Le problème,

141
00:05:15,360 --> 00:05:18,950
c’est qu’il y a beaucoup de choses fausses dans le

142
00:05:18,960 --> 00:05:22,070
système de valeurs et l’éthique de l’humanité,

143
00:05:22,080 --> 00:05:24,070
à l’ère de l’essor des machines.

144
00:05:24,080 --> 00:05:26,390
Et chaque technologie que nous créons

145
00:05:26,400 --> 00:05:28,310
a toujours

146
00:05:28,320 --> 00:05:30,629
amplifié les capacités humaines. Par exemple,

147
00:05:30,639 --> 00:05:34,550
un humain marche à 5 km/h, en voiture

148
00:05:34,560 --> 00:05:37,670
il roule à 250 km/h.

149
00:05:37,680 --> 00:05:41,110
En gros,

150
00:05:41,120 --> 00:05:43,189
on amplifie la mobilité. Ou encore, on

151
00:05:43,199 --> 00:05:45,590
utilise un ordinateur pour augmenter la

152
00:05:45,600 --> 00:05:48,230
capacité de calcul. Et

153
00:05:48,240 --> 00:05:51,350
ce que l’IA va amplifier,

154
00:05:51,360 --> 00:05:53,270
malheureusement, cette fois‑ci, c’est

155
00:05:53,280 --> 00:05:55,909
le mal que l’homme peut faire.

156
00:05:55,919 --> 00:05:59,189
Et c’est entièrement entre nos mains,

157
00:05:59,199 --> 00:06:01,270
entièrement à nous de changer cela.

158
00:06:01,280 --> 00:06:04,070
Mais je dois dire que je ne pense pas que

159
00:06:04,080 --> 00:06:07,029
l’humanité ait aujourd’hui la conscience

160
00:06:07,039 --> 00:06:09,510
nécessaire pour se concentrer

161
00:06:09,520 --> 00:06:12,230
pour utiliser l’IA afin de bâtir l’utopie.

162
00:06:12,240 --> 00:06:13,350


163
00:06:13,360 --> 00:06:15,590
>> Donc, tu dis essentiellement que tu penses désormais

164
00:06:15,600 --> 00:06:17,510
qu’il y aura une période

165
00:06:17,520 --> 00:06:19,670
de dystopie, et pour définir le mot

166
00:06:19,680 --> 00:06:21,670
“dystopie”, j’ai utilisé l’IA : elle dit que c’est

167
00:06:21,680 --> 00:06:23,029
une société terrible où les gens vivent sous

168
00:06:23,039 --> 00:06:25,270
la peur, le contrôle ou la souffrance. Et tu penses

169
00:06:25,280 --> 00:06:27,029
que nous sortirons de cette dystopie

170
00:06:27,039 --> 00:06:29,270
pour aller vers une utopie, définie comme un

171
00:06:29,280 --> 00:06:31,029
endroit parfait où tout fonctionne bien,

172
00:06:31,039 --> 00:06:32,790
où les gens vivent en paix, en santé et heureux.

173
00:06:32,800 --> 00:06:34,550


174
00:06:34,560 --> 00:06:35,909
>> C’est exact.

175
00:06:35,919 --> 00:06:37,189
Et la différence entre les deux,

176
00:06:37,199 --> 00:06:38,790
est ce que j’appelle généralement

177
00:06:38,800 --> 00:06:41,029
le second dilemme, c’est‑à‑dire le moment

178
00:06:41,039 --> 00:06:43,749
où nous remettons complètement

179
00:06:43,759 --> 00:06:47,029
le contrôle à l’IA. Beaucoup pensent que

180
00:06:47,039 --> 00:06:48,790
lorsque l’IA contrôlera tout, ce sera un risque

181
00:06:48,800 --> 00:06:51,270
existentiel pour l’humanité.

182
00:06:51,280 --> 00:06:54,150
Tu sais, j’ai assez de preuves pour

183
00:06:54,160 --> 00:06:58,150
affirmer que lorsqu’on remettra tout à l’IA,

184
00:06:58,160 --> 00:07:00,710
ce sera notre salut.

185
00:07:00,720 --> 00:07:03,749
Le problème aujourd’hui, ce n’est pas

186
00:07:03,759 --> 00:07:05,749
que l’intelligence sera contre nous,

187
00:07:05,759 --> 00:07:08,150
mais que notre stupidité

188
00:07:08,160 --> 00:07:10,230
d’humains agit contre nous. Et je

189
00:07:10,240 --> 00:07:12,550
pense que les défis liés aux humains au pouvoir

190
00:07:12,560 --> 00:07:15,990
seront plus graves que

191
00:07:16,000 --> 00:07:19,110
ceux liés à

192
00:07:19,120 --> 00:07:20,950
une IA au pouvoir.

193
00:07:20,960 --> 00:07:22,230


194
00:07:22,240 --> 00:07:24,469
>> Donc, pendant que nous serons dans cette période dystopique,

195
00:07:24,479 --> 00:07:27,270
as‑tu prévu la durée

196
00:07:27,280 --> 00:07:29,749
de cette dystopie ? Oui, je la calcule, je la vois

197
00:07:29,759 --> 00:07:32,790
précisément durer de 12 à 15 ans. Je crois

198
00:07:32,800 --> 00:07:34,950
que la pente commencera

199
00:07:34,960 --> 00:07:38,469
en 2027. On en verra les signes

200
00:07:38,479 --> 00:07:41,110
en 2026. On en voit déjà en 2024, mais on verra

201
00:07:41,120 --> 00:07:44,469
une aggravation l’an prochain puis un

202
00:07:44,479 --> 00:07:47,830
glissement net en 2027.

203
00:07:47,840 --> 00:07:48,790
>> Pourquoi ?

204
00:07:48,800 --> 00:07:51,510
>> Le contexte géopolitique de notre

205
00:07:51,520 --> 00:07:54,309
monde n’est pas très positif. Il faut vraiment

206
00:07:54,319 --> 00:07:58,070
réfléchir non pas aux symptômes,

207
00:07:58,080 --> 00:08:01,510
mais aux vraies raisons

208
00:08:01,520 --> 00:08:03,830
pour lesquelles nous vivons dans le monde tel qu’il est

209
00:08:03,840 --> 00:08:08,710
aujourd’hui : l’argent, d’accord ?

210
00:08:08,720 --> 00:08:11,350
Et pour quiconque comprend l’argent,

211
00:08:11,360 --> 00:08:14,070
nous, toi et moi, sommes des

212
00:08:14,080 --> 00:08:16,469
paysans : on crée des entreprises, on

213
00:08:16,479 --> 00:08:18,230
contribue, on fabrique, on vend des choses,

214
00:08:18,240 --> 00:08:20,869
alors que l’argent “réel” ne se fait pas là.

215
00:08:20,879 --> 00:08:22,950
Le véritable argent se fait

216
00:08:22,960 --> 00:08:27,830
dans le prêt, dans la réserve fractionnaire.

217
00:08:27,840 --> 00:08:31,830
Et tu sais, le plus grand prêteur

218
00:08:31,840 --> 00:08:34,709
du monde a besoin de raisons de prêter,

219
00:08:34,719 --> 00:08:37,350
et rien n’est plus “utile” pour cela qu’une guerre.

220
00:08:37,360 --> 00:08:40,550
Réfléchis : le

221
00:08:40,560 --> 00:08:43,110
monde a dépensé 2,71

222
00:08:43,120 --> 00:08:47,350
milliers de milliards pour la guerre en 2024,

223
00:08:47,360 --> 00:08:49,269
dont 1 000 milliards par an aux États‑Unis.

224
00:08:49,279 --> 00:08:50,870


225
00:08:50,880 --> 00:08:53,190
Et quand on y pense bien,

226
00:08:53,200 --> 00:08:56,710
je ne veux pas faire peur,

227
00:08:56,720 --> 00:09:00,070
tu sais, les armes se déprécient.

228
00:09:00,080 --> 00:09:02,630
Elles perdent de la valeur sur 10 à 30 ans.

229
00:09:02,640 --> 00:09:03,350
La plupart des armes,

230
00:09:03,360 --> 00:09:04,630
>> perdent leur valeur.

231
00:09:04,640 --> 00:09:05,910
>> Elles perdent leur valeur et elles

232
00:09:05,920 --> 00:09:07,750
se déprécient comptablement sur les

233
00:09:07,760 --> 00:09:09,829
bilans d’une armée. L’arsenal actuel des

234
00:09:09,839 --> 00:09:11,670
États‑Unis, d’après une recherche poussée

235
00:09:11,680 --> 00:09:14,550
avec mon IA Trixie, tu sais, cet arsenal

236
00:09:14,560 --> 00:09:18,310
a coûté aux États‑Unis

237
00:09:18,320 --> 00:09:21,829
entre 24 et 26 mille milliards à construire. Ma

238
00:09:21,839 --> 00:09:23,750
conclusion, c’est que beaucoup des guerres

239
00:09:23,760 --> 00:09:25,110
qui se déroulent dans le monde

240
00:09:25,120 --> 00:09:27,829
servent à se débarrasser de ces armes

241
00:09:27,839 --> 00:09:29,430
pour pouvoir les remplacer.

242
00:09:29,440 --> 00:09:32,710
Et quand ta morale d’industrie

243
00:09:32,720 --> 00:09:35,269
consiste à fabriquer des armes pour tuer,

244
00:09:35,279 --> 00:09:38,310
alors, forcément, tu finis par t’en servir.

245
00:09:38,320 --> 00:09:39,990


246
00:09:40,000 --> 00:09:40,470


247
00:09:40,480 --> 00:09:42,790
>> Qui en profite ? Les prêteurs et l’industrie,

248
00:09:42,800 --> 00:09:43,430


249
00:09:43,440 --> 00:09:45,350
>> mais ils ne peuvent pas décider eux‑mêmes

250
00:09:45,360 --> 00:09:47,829
d’aller en guerre. Ils doivent compter sur

251
00:09:47,839 --> 00:09:50,550
>> souviens‑toi, je te l’avais dit

252
00:09:50,560 --> 00:09:54,389
je crois lors de notre troisième podcast : la guerre est

253
00:09:54,399 --> 00:09:56,630
d’abord décidée,

254
00:09:56,640 --> 00:09:59,269
puis l’histoire est fabriquée. Tu te rappelles

255
00:09:59,279 --> 00:10:01,750
d’*1984* et de l’approche orwellienne :

256
00:10:01,760 --> 00:10:04,630
où, tu sais, la liberté c’est l’esclavage,

257
00:10:04,640 --> 00:10:08,470
la guerre c’est la paix, ils appellent ça

258
00:10:08,480 --> 00:10:13,269
la “novlangue”, pour convaincre

259
00:10:13,279 --> 00:10:16,389
les gens que partir en guerre dans

260
00:10:16,399 --> 00:10:19,190
un autre pays pour tuer 4,7 millions de personnes,

261
00:10:19,200 --> 00:10:22,069
c’est la liberté. “Nous y allons pour libérer

262
00:10:22,079 --> 00:10:25,190
le peuple irakien.”

263
00:10:25,200 --> 00:10:27,829
La guerre, est‑ce vraiment la liberté ? Dire qu’on

264
00:10:27,839 --> 00:10:29,910
va tuer 300 000

265
00:10:29,920 --> 00:10:31,750
women et enfants, c’est

266
00:10:31,760 --> 00:10:34,389
pour la liberté et

267
00:10:34,399 --> 00:10:36,870
pour les valeurs humaines ?

268
00:10:36,880 --> 00:10:38,550


269
00:10:38,560 --> 00:10:42,150
Sérieusement, comment peut‑on croire à cela ?

270
00:10:42,160 --> 00:10:44,790
Les récits sont fabriqués, et ensuite

271
00:10:44,800 --> 00:10:46,790
on y croit, parce que les humains, naïfs,

272
00:10:46,800 --> 00:10:49,190
applaudissent et disent :

273
00:10:49,200 --> 00:10:51,670
« Oui, on est du bon côté !

274
00:10:51,680 --> 00:10:54,230
Eux, ce sont les méchants. »

275
00:10:54,240 --> 00:10:56,230
>> D’accord. Alors, laisse‑moi essayer une idée.

276
00:10:56,240 --> 00:10:58,230
Donc, l’idée, c’est que

277
00:10:58,240 --> 00:11:00,550
c’est l’argent qui alimente une grande partie

278
00:11:00,560 --> 00:11:01,990
des conflits qu’on voit, et

279
00:11:02,000 --> 00:11:04,069
qu’il alimente donc cette dystopie.

280
00:11:04,079 --> 00:11:05,910
Voici une réflexion : j’ai lu

281
00:11:05,920 --> 00:11:07,269
quelque chose l’autre jour qui

282
00:11:07,279 --> 00:11:09,590
expliquait comment

283
00:11:09,600 --> 00:11:12,069
les milliardaires ne sont jamais satisfaits, car

284
00:11:12,079 --> 00:11:14,949
en réalité, ce qu’ils recherchent, ce n’est pas

285
00:11:14,959 --> 00:11:17,670
plus d’argent, mais plus de statut.

286
00:11:17,680 --> 00:11:19,509
Exact. Et je regardais le point

287
00:11:19,519 --> 00:11:21,670
de vue évolutif de cet argument.

288
00:11:21,680 --> 00:11:23,350
Et si l’on remonte de quelques milliers

289
00:11:23,360 --> 00:11:25,430
d’années,

290
00:11:25,440 --> 00:11:27,990
l’argent n’existait pas. La richesse, c’était ce

291
00:11:28,000 --> 00:11:30,230
que tu pouvais porter. Donc même, je pense que

292
00:11:30,240 --> 00:11:32,710
pour l’esprit humain, l’idée de richesse

293
00:11:32,720 --> 00:11:35,110
n’est pas naturelle.

294
00:11:35,120 --> 00:11:37,430
Ce qui a toujours compté,

295
00:11:37,440 --> 00:11:39,430
pour la survie du plus apte,

296
00:11:39,440 --> 00:11:41,670
c’est sur le plan reproductif,

297
00:11:41,680 --> 00:11:44,150
ce qui a toujours eu de la valeur, si

298
00:11:44,160 --> 00:11:46,470
tu remontes des millénaires en arrière, c’était

299
00:11:46,480 --> 00:11:48,710
la personne capable de se reproduire le plus

300
00:11:48,720 --> 00:11:51,350
c’était celle qui avait le plus haut statut. Donc,

301
00:11:51,360 --> 00:11:52,389
cela explique pourquoi

302
00:11:52,399 --> 00:11:53,750
les milliardaires accumulent tant d’argent mais

303
00:11:53,760 --> 00:11:56,630
puis vont sur des podcasts, veulent

304
00:11:56,640 --> 00:11:57,750
créer le leur et acheter des journaux :

305
00:11:57,760 --> 00:11:59,509
c’est parce qu’au plus profond d’eux‑mêmes,

306
00:11:59,519 --> 00:12:01,910
les humains ont un besoin viscéral

307
00:12:01,920 --> 00:12:03,509
d’accroître leur statut.

308
00:12:03,519 --> 00:12:05,269
>> Oui. Donc si l’on y pense,

309
00:12:05,279 --> 00:12:06,629
en revenant à l’exemple des guerres,

310
00:12:06,639 --> 00:12:08,949
peut‑être que ce n’est pas l’argent,

311
00:12:08,959 --> 00:12:14,069
mais le statut. C’est ce

312
00:12:14,079 --> 00:12:16,470
que recherchent ces premiers ministres,

313
00:12:16,480 --> 00:12:18,870
ces dirigeants, ces individus, qui veulent

314
00:12:18,880 --> 00:12:20,710
créer plus de pouvoir et plus de statut,

315
00:12:20,720 --> 00:12:22,230
car, au fond, ce qui compte

316
00:12:22,240 --> 00:12:24,150
pour un être humain, c’est d’avoir plus

317
00:12:24,160 --> 00:12:25,509
de pouvoir et de statut. Et l’argent

318
00:12:25,519 --> 00:12:27,430
n’est qu’un

319
00:12:27,440 --> 00:12:30,069
simple reflet de ce statut.

320
00:12:30,079 --> 00:12:31,990
>> Et quel genre de monde est‑ce, ça ?

321
00:12:32,000 --> 00:12:33,430
>> Franchement, un monde complètement foutu. Tous ces

322
00:12:33,440 --> 00:12:35,269
hommes puissants ont…

323
00:12:35,279 --> 00:12:35,670
>> exactement.

324
00:12:35,680 --> 00:12:37,030
>> …vraiment foutu le monde en l’air. Mais

325
00:12:37,040 --> 00:12:38,790
>> alors, je peux… oui ?

326
00:12:38,800 --> 00:12:40,389
>> En fait, l’IA, c’est pareil :

327
00:12:40,399 --> 00:12:42,069
>> on est dans une course à l’IA où

328
00:12:42,079 --> 00:12:45,269
beaucoup de milliardaires se disent : si j’obtiens

329
00:12:45,279 --> 00:12:47,350
l’intelligence artificielle générale (AGI)

330
00:12:47,360 --> 00:12:49,670
d’abord, je domine le monde.

331
00:12:49,680 --> 00:12:52,069
>> Exactement. C’est précisément l’idée,

332
00:12:52,079 --> 00:12:54,069
ce que j’appelais avant

333
00:12:54,079 --> 00:12:55,590
« le premier inévitable », et que je nomme maintenant

334
00:12:55,600 --> 00:12:58,389
le premier dilemme ; et dans *Scary Smart*, c’est

335
00:12:58,399 --> 00:13:01,750
une course qui s’accélère sans cesse.

336
00:13:01,760 --> 00:13:03,190
Tu penses que les 12 prochaines années seront

337
00:13:03,200 --> 00:13:05,829
une dystopie dominée par l’IA ?

338
00:13:05,839 --> 00:13:07,190
>> Je pense que les 12 prochaines années seront

339
00:13:07,200 --> 00:13:10,790
une dystopie humaine utilisant l’IA.

340
00:13:10,800 --> 00:13:12,790
>> Une dystopie provoquée par l’humain à travers l’IA.

341
00:13:12,800 --> 00:13:15,509
>> Et tu la définis par une recrudescence des guerres

342
00:13:15,519 --> 00:13:16,790
dans le monde ?

343
00:13:16,800 --> 00:13:19,110
>> Le dernier, le dernier “RIP”, le dernier,

344
00:13:19,120 --> 00:13:21,910
c’est qu’il y aura une énorme concentration du pouvoir

345
00:13:21,920 --> 00:13:23,670
et une grande dispersion du pouvoir.

346
00:13:23,680 --> 00:13:26,389
Cela signifie que ceux qui concentrent

347
00:13:26,399 --> 00:13:28,629
le pouvoir vont tenter d’opprimer

348
00:13:28,639 --> 00:13:30,870
ceux qui détiennent la démocratie du pouvoir.

349
00:13:30,880 --> 00:13:33,190
—

350
00:13:33,200 --> 00:13:35,350
—

351
00:13:35,360 --> 00:13:39,430
réfléchis‑y : dans le monde actuel, contrairement

352
00:13:39,440 --> 00:13:41,430
au passé,

353
00:13:41,440 --> 00:13:43,030
euh

354
00:13:43,040 --> 00:13:46,470
les Houthis, avec un drone,

355
00:13:46,480 --> 00:13:49,590
sont des tribus yéménites

356
00:13:49,600 --> 00:13:52,629
qui résistent au pouvoir américain et israélien

357
00:13:52,639 --> 00:13:54,949
dans la mer Rouge. Ils utilisent un

358
00:13:54,959 --> 00:13:59,430
drone coûtant 3 000 $ pour attaquer

359
00:13:59,440 --> 00:14:02,069
un navire de guerre ou un avion américain

360
00:14:02,079 --> 00:14:03,910
valant des centaines de millions.

361
00:14:03,920 --> 00:14:06,710
Cette “démocratisation du pouvoir”

362
00:14:06,720 --> 00:14:09,990
rend les puissants très nerveux :

363
00:14:10,000 --> 00:14:13,030
d’où viendra la prochaine menace ?

364
00:14:13,040 --> 00:14:15,269
Et cela vaut non seulement pour la guerre,

365
00:14:15,279 --> 00:14:17,590
mais aussi pour l’économie,

366
00:14:17,600 --> 00:14:20,389
l’innovation,

367
00:14:20,399 --> 00:14:22,150
la technologie, et ainsi de suite.

368
00:14:22,160 --> 00:14:24,550
Autrement dit,

369
00:14:24,560 --> 00:14:27,509
comme tu l’as dit, alors que

370
00:14:27,519 --> 00:14:29,350
les oligarques technologiques

371
00:14:29,360 --> 00:14:31,910
tentent d’atteindre l’AGI,

372
00:14:31,920 --> 00:14:34,069
ils veulent s’assurer que dès qu’ils l’auront,

373
00:14:34,079 --> 00:14:37,910
personne d’autre ne l’aura,

374
00:14:37,920 --> 00:14:40,629
et surtout que personne ne pourra

375
00:14:40,639 --> 00:14:43,590
menacer leur position de privilège.

376
00:14:43,600 --> 00:14:46,069
—

377
00:14:46,079 --> 00:14:48,550
Donc, on va voir un monde

378
00:14:48,560 --> 00:14:50,310
où, malheureusement, il y aura

379
00:14:50,320 --> 00:14:51,750
beaucoup de contrôle,

380
00:14:51,760 --> 00:14:55,910
beaucoup de surveillance,

381
00:14:55,920 --> 00:14:57,910
beaucoup de conformité forcée : sinon, tu perds ton

382
00:14:57,920 --> 00:15:00,470
privilege d’exister dans ce monde. Et

383
00:15:00,480 --> 00:15:02,389
Cela se produit déjà.

384
00:15:02,399 --> 00:15:03,910
>> Avec cet acronyme, je veux m’assurer qu’on le couvre en entier.

385
00:15:03,920 --> 00:15:05,990
—

386
00:15:06,000 --> 00:15:08,069
>> Tu aimes les dystopies, n’est‑ce pas ?

387
00:15:08,079 --> 00:15:09,189
>> Je veux parler de la dystopie, puis

388
00:15:09,199 --> 00:15:10,230
parler de l’utopie.

389
00:15:10,240 --> 00:15:11,110
>> D’accord.

390
00:15:11,120 --> 00:15:14,310
>> Et idéalement, comment on passe de la dystopie

391
00:15:14,320 --> 00:15:15,189
à l’utopie.

392
00:15:15,199 --> 00:15:18,949
>> Mmm. Donc le “F” de “face R”

393
00:15:18,959 --> 00:15:20,870
>> est la perte de liberté, conséquence de

394
00:15:20,880 --> 00:15:23,910
cette dichotomie de pouvoir. Tu vois, on a

395
00:15:23,920 --> 00:15:26,230
un pouvoir énorme, comme on le voit

396
00:15:26,240 --> 00:15:29,430
aujourd’hui avec une armée particulière

397
00:15:29,440 --> 00:15:32,470
financée par les États‑Unis,

398
00:15:32,480 --> 00:15:36,069
et beaucoup d’argent, combattant

399
00:15:36,079 --> 00:15:37,829
de simples paysans sans armes ou presque.

400
00:15:37,839 --> 00:15:39,030
—

401
00:15:39,040 --> 00:15:41,829
>> Bien sûr, certains sont militarisés,

402
00:15:41,839 --> 00:15:43,910
mais la majorité des deux millions de

403
00:15:43,920 --> 00:15:46,230
personnes ne le sont pas. Donc, il y a

404
00:15:46,240 --> 00:15:47,829
un pouvoir gigantesque qui dit :

405
00:15:47,839 --> 00:15:48,870
« Je vais opprimer autant que je veux. »

406
00:15:48,880 --> 00:15:51,430
Et je ferai ce que je veux, car

407
00:15:51,440 --> 00:15:53,910
les applaudissements resteront silencieux

408
00:15:53,920 --> 00:15:56,230
ou même favorables.

409
00:15:56,240 --> 00:15:58,629
—

410
00:15:58,639 --> 00:16:02,470
Eh bien, dans ce contexte,

411
00:16:02,480 --> 00:16:05,670
se produit que le pouvoir absolu,

412
00:16:05,680 --> 00:16:07,749
menacé par la démocratie du pouvoir, mène

413
00:16:07,759 --> 00:16:09,829
à une perte de liberté. Une perte pour tous.

414
00:16:09,839 --> 00:16:11,269


415
00:16:11,279 --> 00:16:13,350
>> Et comment ça affecte ma liberté, à moi ?

416
00:16:13,360 --> 00:16:14,710
>> Ta liberté, oui.

417
00:16:14,720 --> 00:16:17,990
>> Très bientôt, si tu publies

418
00:16:18,000 --> 00:16:19,990
cet épisode, on commencera à

419
00:16:20,000 --> 00:16:21,829
se demander si tu devrais

420
00:16:21,839 --> 00:16:23,670
parler de ces sujets dans ton

421
00:16:23,680 --> 00:16:28,069
podcast. Et si je participe

422
00:16:28,079 --> 00:16:30,949
à cet épisode, il y a fort à parier que

423
00:16:30,959 --> 00:16:33,110
la prochaine fois que j’atterrirai aux États‑Unis,

424
00:16:33,120 --> 00:16:35,030
on me questionnera : « Pourquoi dites‑vous cela ?

425
00:16:35,040 --> 00:16:38,230
De quel côté êtes‑vous ? »

426
00:16:38,240 --> 00:16:40,629
Et tu vois bien qu’on

427
00:16:40,639 --> 00:16:43,430
perd peu à peu la liberté. Je t’avais dit :

428
00:16:43,440 --> 00:16:46,629
peu importe ce que j’essaie d’apporter au monde,

429
00:16:46,639 --> 00:16:48,150
ma banque ferme mon compte

430
00:16:48,160 --> 00:16:50,790
toutes les six semaines,

431
00:16:50,800 --> 00:16:52,629
seulement à cause de mon origine

432
00:16:52,639 --> 00:16:55,110
et de mon ethnie. À chaque fois, on

433
00:16:55,120 --> 00:16:57,430
me bloque le compte en disant :

434
00:16:57,440 --> 00:16:59,269
« Nous avons besoin d’un document. »

435
00:16:59,279 --> 00:17:01,670
Mes autres collègues d’une autre couleur

436
00:17:01,680 --> 00:17:04,069
ne doivent jamais présenter de

437
00:17:04,079 --> 00:17:07,189
document, mais moi, oui,

438
00:17:07,199 --> 00:17:09,029
parce que j’appartiens à une ethnie

439
00:17:09,039 --> 00:17:10,870
considérée comme “ennemie” dans le monde

440
00:17:10,880 --> 00:17:16,150
depuis 30‑40 ans.

441
00:17:16,160 --> 00:17:17,909
Et donc, quand on réfléchit vraiment,

442
00:17:17,919 --> 00:17:20,230
dans un monde où tout devient numérique,

443
00:17:20,240 --> 00:17:21,750
où tout est surveillé,

444
00:17:21,760 --> 00:17:23,189
où tout est visible,

445
00:17:23,199 --> 00:17:25,909
nous n’avons plus vraiment de liberté.

446
00:17:25,919 --> 00:17:27,909
Et je ne dis pas ça pour débattre,

447
00:17:27,919 --> 00:17:31,110
mais je ne vois pas de solution.

448
00:17:31,120 --> 00:17:32,630


449
00:17:32,640 --> 00:17:34,390
>> Parce que l’IA aura plus

450
00:17:34,400 --> 00:17:36,230
d’informations sur nous, saura mieux nous suivre,

451
00:17:36,240 --> 00:17:39,990
et cela mènera donc

452
00:17:40,000 --> 00:17:41,270
à la restriction de certaines libertés.

453
00:17:41,280 --> 00:17:42,630
C’est bien ce que tu veux dire ?

454
00:17:42,640 --> 00:17:44,870
>> C’est un aspect, oui. Si tu pousses ce point plus loin,

455
00:17:44,880 --> 00:17:47,350
—

456
00:17:47,360 --> 00:17:49,830
dans très peu de temps, si tu as vu

457
00:17:49,840 --> 00:17:52,310
par exemple récemment les agents, Manos ou

458
00:17:52,320 --> 00:17:55,110
ChatGPT, il viendra un temps

459
00:17:55,120 --> 00:17:57,510
où tu ne feras plus rien

460
00:17:57,520 --> 00:17:59,909
par toi‑même. Tu diras juste

461
00:17:59,919 --> 00:18:01,590
à ton IA : « Au fait, je vais voir Stephen,

462
00:18:01,600 --> 00:18:03,270
peux‑tu me réserver ça ? »

463
00:18:03,280 --> 00:18:04,549


464
00:18:04,559 --> 00:18:05,110
>> Génial.

465
00:18:05,120 --> 00:18:06,549
>> Oui, et elle le fera

466
00:18:06,559 --> 00:18:08,710
absolument tout. Ce sera bien,

467
00:18:08,720 --> 00:18:10,470
jusqu’au moment où elle décidera de

468
00:18:10,480 --> 00:18:11,990
faire des choses non motivées uniquement par

469
00:18:12,000 --> 00:18:13,830
ton bien‑être. Pourquoi ferait‑elle cela ?

470
00:18:13,840 --> 00:18:14,630


471
00:18:14,640 --> 00:18:17,350
>> Tout simplement parce que, si j’achète un

472
00:18:17,360 --> 00:18:19,190
billet BA plutôt qu’Emirates,

473
00:18:19,200 --> 00:18:21,029
un agent gagnera plus d’argent qu’un autre,

474
00:18:21,039 --> 00:18:23,190
et ainsi de suite.

475
00:18:23,200 --> 00:18:25,270
Et je ne pourrais même plus m’en rendre compte,

476
00:18:25,280 --> 00:18:27,190
si je laisse tout à une IA.

477
00:18:27,200 --> 00:18:30,230
Fais un pas de plus :

478
00:18:30,240 --> 00:18:32,870
pense à un monde où presque tout le monde

479
00:18:32,880 --> 00:18:36,230
sera sous revenu universel, d’accord ?

480
00:18:36,240 --> 00:18:37,110
>> Qu’est‑ce que le revenu universel ?

481
00:18:37,120 --> 00:18:39,830
>> Le revenu de base universel. Pense

482
00:18:39,840 --> 00:18:41,590
à l’économie, le “E” de FACE RIPs.

483
00:18:41,600 --> 00:18:44,230
Pense à l’économie d’un monde où

484
00:18:44,240 --> 00:18:46,710
où l’on verra bientôt apparaître

485
00:18:46,720 --> 00:18:48,710
un trillionnaire

486
00:18:48,720 --> 00:18:51,430
avant 2030. Je te garantis que

487
00:18:51,440 --> 00:18:53,669
quelqu’un deviendra trillionnaire. Je

488
00:18:53,679 --> 00:18:55,029
pense même qu’il en existe déjà

489
00:18:55,039 --> 00:18:56,390
il y en a déjà plusieurs dans le monde,

490
00:18:56,400 --> 00:18:57,909
on ne sait simplement pas qui ils sont.

491
00:18:57,919 --> 00:19:00,310
Mais il y aura un nouveau Elon Musk ou

492
00:19:00,320 --> 00:19:01,909
Larry Ellison qui deviendra

493
00:19:01,919 --> 00:19:04,470
trillionnaire grâce à ses investissements dans l’IA.

494
00:19:04,480 --> 00:19:07,669
d’accord ? Et ce trillionnaire aura

495
00:19:07,679 --> 00:19:10,310
tellement d’argent qu’il pourra tout acheter.

496
00:19:10,320 --> 00:19:13,350
Il y aura des robots et des IA qui feront

497
00:19:13,360 --> 00:19:17,430
tout, et les humains n’auront plus d’emploi.

498
00:19:17,440 --> 00:19:17,830
Je veux dire…

499
00:19:17,840 --> 00:19:19,350
>> Tu penses que c’est vraiment possible

500
00:19:19,360 --> 00:19:21,270
qu’autant d’emplois disparaissent

501
00:19:21,280 --> 00:19:24,230
d’ici dix ans ? Et la réponse qu’on entend souvent,

502
00:19:24,240 --> 00:19:25,510
c’est qu’il y aura de nouveaux métiers dans la tech.

503
00:19:25,520 --> 00:19:28,070


504
00:19:28,080 --> 00:19:29,750
>> Absolument faux.

505
00:19:29,760 --> 00:19:30,390
>> Vraiment ?

506
00:19:30,400 --> 00:19:30,950
>> Bien sûr.

507
00:19:30,960 --> 00:19:33,110
>> Comment peux‑tu en être aussi sûr ?

508
00:19:33,120 --> 00:19:35,750
>> D’accord. Encore une fois, je ne suis sûr de rien.

509
00:19:35,760 --> 00:19:38,310
Donc soyons très clairs,

510
00:19:38,320 --> 00:19:41,029
il serait très arrogant

511
00:19:41,039 --> 00:19:43,110
de prétendre que je sais.

512
00:19:43,120 --> 00:19:44,870
>> Tu viens pourtant de dire que c’était n’importe quoi.

513
00:19:44,880 --> 00:19:47,830
>> Ma conviction, c’est que c’est à 100% du pipeau.

514
00:19:47,840 --> 00:19:50,070
>> Prenons un métier comme celui de développeur logiciel.

515
00:19:50,080 --> 00:19:50,470
>> Oui.

516
00:19:50,480 --> 00:19:52,789
>> D’accord. Emma.Love, ma nouvelle

517
00:19:52,799 --> 00:19:55,750
start‑up, c’est moi, Senad, un autre ingénieur tech

518
00:19:55,760 --> 00:19:58,950
et beaucoup d’IA. Cette

519
00:19:58,960 --> 00:20:01,909
start‑up aurait nécessité 350 développeurs

520
00:20:01,919 --> 00:20:02,870
autrefois.

521
00:20:02,880 --> 00:20:05,909
>> D’accord, mais embauches‑tu maintenant à

522
00:20:05,919 --> 00:20:08,630
d’autres postes à cause de cela ? Ou bien,

523
00:20:08,640 --> 00:20:12,549
comme avec la machine à vapeur,

524
00:20:12,559 --> 00:20:14,150
je ne me souviens plus du nom de l’effet, mais

525
00:20:14,160 --> 00:20:15,830
tu sais, quand

526
00:20:15,840 --> 00:20:18,630
le charbon est devenu moins cher, les gens

527
00:20:18,640 --> 00:20:19,830
se sont inquiétés pour l’industrie du charbon,

528
00:20:19,840 --> 00:20:21,110
mais en réalité,

529
00:20:21,120 --> 00:20:23,669
on a utilisé davantage de trains,

530
00:20:23,679 --> 00:20:25,590
donc pour le transport

531
00:20:25,600 --> 00:20:27,909
et même les loisirs, alors qu’avant

532
00:20:27,919 --> 00:20:29,830
c’était juste pour le fret.

533
00:20:29,840 --> 00:20:32,310
Donc, il y a eu plus d’usages,

534
00:20:32,320 --> 00:20:34,710
et l’industrie du charbon a explosé. Alors

535
00:20:34,720 --> 00:20:36,549
je me demande si, avec la technologie,

536
00:20:36,559 --> 00:20:38,149
oui, les développeurs auront peut‑être

537
00:20:38,159 --> 00:20:39,590
moins de travail, mais il y aura

538
00:20:39,600 --> 00:20:40,870
du logiciel partout.

539
00:20:40,880 --> 00:20:41,830
>> Cite‑m’en un.

540
00:20:41,840 --> 00:20:42,950
>> T’en nommer un ? Quel…

541
00:20:42,960 --> 00:20:43,669
>> métier ?

542
00:20:43,679 --> 00:20:45,669
>> Dis‑moi un métier qui serait créé.

543
00:20:45,679 --> 00:20:48,390
>> Oui. Un travail qu’une IA ne peut pas faire.

544
00:20:48,400 --> 00:20:49,190
>> …

545
00:20:49,200 --> 00:20:49,750
>> Oui.

546
00:20:49,760 --> 00:20:51,110
>> Ou un robot.

547
00:20:51,120 --> 00:20:53,430
>> L’entreprise de retraites “breathwork” de ma copine,

548
00:20:53,440 --> 00:20:55,270
où elle emmène des groupes de femmes

549
00:20:55,280 --> 00:20:57,029
aux quatre coins du monde. Sa société s’appelle

550
00:20:57,039 --> 00:20:58,710
Barley Breathwork. Et il va y avoir

551
00:20:58,720 --> 00:21:00,310
une demande croissante de connexion,

552
00:21:00,320 --> 00:21:01,190
de lien humain.

553
00:21:01,200 --> 00:21:02,549
>> Exact. Continue.

554
00:21:02,559 --> 00:21:04,549
>> Donc, il y aura plus de gens qui organiseront

555
00:21:04,559 --> 00:21:07,669
des événements communautaires, des festivals réels.

556
00:21:07,679 --> 00:21:08,950
Je pense qu’on verra une grande montée

557
00:21:08,960 --> 00:21:09,909
des activités comme

558
00:21:09,919 --> 00:21:11,270
>> tout ce qui touche à la connexion

559
00:21:11,280 --> 00:21:11,669
humaine.

560
00:21:11,679 --> 00:21:12,070
>> Oui.

561
00:21:12,080 --> 00:21:15,430
>> Exactement. Je suis entièrement d’accord. OK.

562
00:21:15,440 --> 00:21:17,110
Mais quel pourcentage cela représente‑t‑il

563
00:21:17,120 --> 00:21:17,990
face aux comptables ?

564
00:21:18,000 --> 00:21:19,430
>> Une part bien plus faible, c’est sûr,

565
00:21:19,440 --> 00:21:21,029
en ce qui concerne les emplois de bureau.

566
00:21:21,039 --> 00:21:24,149
>> Bien. Et à qui vend‑elle ?

567
00:21:24,159 --> 00:21:26,070
>> À des gens qui ont… quoi ? Probablement

568
00:21:26,080 --> 00:21:27,430
des comptables, ou tu vois…

569
00:21:27,440 --> 00:21:29,990
>> Exact. Elle vend à des gens qui gagnent

570
00:21:30,000 --> 00:21:31,430
de l’argent grâce à leur travail.

571
00:21:31,440 --> 00:21:31,909
>> Oui.

572
00:21:31,919 --> 00:21:34,549
>> Donc, deux forces se produisent.

573
00:21:34,559 --> 00:21:37,029
>>> D’un côté, il y a des métiers clairement

574
00:21:37,039 --> 00:21:38,789
remplacés. Le monteur vidéo va

575
00:21:38,799 --> 00:21:40,470
être remplacé.

576
00:21:40,480 --> 00:21:42,230
>> Excuse‑moi.

577
00:21:42,240 --> 00:21:44,549
>> J’adore

578
00:21:44,559 --> 00:21:47,029
et d’ailleurs, les podcasteurs vont

579
00:21:47,039 --> 00:21:48,710
être remplacés.

580
00:21:48,720 --> 00:21:50,470
>> Merci d’être venu aujourd’hui, Mo. C’était

581
00:21:50,480 --> 00:21:52,630
top de te revoir.

582
00:21:52,640 --> 00:21:55,190
Mais la vérité, c’est que beaucoup… tu vois,

583
00:21:55,200 --> 00:21:58,390
les meilleurs dans chaque métier resteront

584
00:21:58,400 --> 00:22:00,149
les meilleurs : le développeur qui

585
00:22:00,159 --> 00:22:01,590
maîtrise vraiment l’architecture, la

586
00:22:01,600 --> 00:22:03,909
technologie, restera un temps, d’accord ?

587
00:22:03,919 --> 00:22:06,230
Et, tu sais, une des choses les plus

588
00:22:06,240 --> 00:22:08,870
drôles : j’ai interviewé Max

589
00:22:08,880 --> 00:22:11,110
Tedmar, et il riait aux éclats

590
00:22:11,120 --> 00:22:14,549
en disant : « Les PDG se réjouissent car ils peuvent

591
00:22:14,559 --> 00:22:16,149
se débarrasser du personnel, gagner en

592
00:22:16,159 --> 00:22:18,310
productivité et réduire les coûts »

593
00:22:18,320 --> 00:22:20,710
parce que l’IA peut faire ce travail. Ce qu’ils

594
00:22:20,720 --> 00:22:24,070
n’imaginent pas, c’est que l’IA les remplacera eux aussi.

595
00:22:24,080 --> 00:22:26,549
L’AGI sera meilleure que l’humain

596
00:22:26,559 --> 00:22:29,909
dans tout, y compris

597
00:22:29,919 --> 00:22:32,870
dans le rôle de PDG, tu vois ?

598
00:22:32,880 --> 00:22:34,950
Et il faut vraiment s’imaginer

599
00:22:34,960 --> 00:22:37,190
qu’un jour, la plupart

600
00:22:37,200 --> 00:22:40,549
des PDG incompétents seront remplacés.

601
00:22:40,559 --> 00:22:43,750
Même dans le “breathwork”, d’accord ?

602
00:22:43,760 --> 00:22:46,470
À terme, il pourrait se produire

603
00:22:46,480 --> 00:22:48,390
deux phénomènes.

604
00:22:48,400 --> 00:22:52,950
Le premier, c’est que, à part pour les

605
00:22:52,960 --> 00:22:55,029
meilleurs instructeurs,

606
00:22:55,039 --> 00:22:58,870
les autres, tu sais, ceux qui

607
00:22:58,880 --> 00:23:01,190
réunissent encore les personnes capables

608
00:23:01,200 --> 00:23:03,669
de payer un atelier de respiration,

609
00:23:03,679 --> 00:23:06,950
seront concentrés au sommet,

610
00:23:06,960 --> 00:23:08,470
tandis qu’en bas, beaucoup ne

611
00:23:08,480 --> 00:23:11,270
travailleront plus.

612
00:23:11,280 --> 00:23:13,669
Et cela, pour deux raisons.

613
00:23:13,679 --> 00:23:16,230
D’abord, il n’y aura pas assez de demande

614
00:23:16,240 --> 00:23:18,630
car beaucoup auront perdu leur emploi.

615
00:23:18,640 --> 00:23:21,190
Et quand tu vis du revenu universel,

616
00:23:21,200 --> 00:23:23,430
tu ne peux pas dire au gouvernement : « Augmente‑moi

617
00:23:23,440 --> 00:23:25,110
un peu pour mon cours de respiration. »

618
00:23:25,120 --> 00:23:27,990
>> Le revenu universel, c’est donc un

619
00:23:28,000 --> 00:23:29,750
revenu mensuel garanti.

620
00:23:29,760 --> 00:23:31,270
>> Exact. Et si tu y réfléchis,

621
00:23:31,280 --> 00:23:34,390
le revenu universel est une situation

622
00:23:34,400 --> 00:23:36,470
très intéressante, car

623
00:23:36,480 --> 00:23:38,549
malheureusement, comme je l’ai dit,

624
00:23:38,559 --> 00:23:39,990
il n’y a rien de mal avec l’IA ;

625
00:23:40,000 --> 00:23:41,590
il y a beaucoup de travers dans le système

626
00:23:41,600 --> 00:23:43,190
de valeurs de l’humanité à l’ère

627
00:23:43,200 --> 00:23:45,430
de la montée des machines. Et cette

628
00:23:45,440 --> 00:23:47,270
grande valeur dominante, c’est le capitalisme.

629
00:23:47,280 --> 00:23:49,590
Et le capitalisme, c’est quoi, au fond ?

630
00:23:49,600 --> 00:23:51,110
La recherche d’arbitrage sur le travail.

631
00:23:51,120 --> 00:23:52,149
>> Qu’est‑ce que ça veut dire ?

632
00:23:52,159 --> 00:23:54,630
>> Je t’embauche pour faire quelque chose. Je te paie

633
00:23:54,640 --> 00:23:56,549
un dollar et je le revends

634
00:23:56,559 --> 00:23:58,230
deux.

635
00:23:58,240 --> 00:24:00,149
Et beaucoup s’y trompent,

636
00:24:00,159 --> 00:24:01,669
en disant : « Oui mais le prix d’un

637
00:24:01,679 --> 00:24:03,510
produit inclut aussi les matières premières,

638
00:24:03,520 --> 00:24:05,270
les usines, etc. » — mais tout ça

639
00:24:05,280 --> 00:24:07,510
est lui‑même issu du travail humain.

640
00:24:07,520 --> 00:24:09,830
Donc, concrètement, le travail extrait la matière,

641
00:24:09,840 --> 00:24:11,430
puis cette matière est vendue

642
00:24:11,440 --> 00:24:12,789
avec une marge ; elle est transformée

643
00:24:12,799 --> 00:24:14,870
en machine, revendue avec une marge,

644
00:24:14,880 --> 00:24:16,390
et ainsi de suite.

645
00:24:16,400 --> 00:24:18,149
—

646
00:24:18,159 --> 00:24:20,549
Il y a toujours ce jeu d’arbitrage du travail.

647
00:24:20,559 --> 00:24:23,350
Mais dans un monde où l’esprit humain est

648
00:24:23,360 --> 00:24:28,390
remplacé par des IA virtuelles

649
00:24:28,400 --> 00:24:32,549
et où sa force physique

650
00:24:32,559 --> 00:24:36,310
pourra être remplacée par un robot en 3 à 5 ans,

651
00:24:36,320 --> 00:24:39,110
—

652
00:24:39,120 --> 00:24:40,870
il faut vraiment se demander à quoi

653
00:24:40,880 --> 00:24:42,789
ressemblera ce monde. Ce pourrait être le

654
00:24:42,799 --> 00:24:44,310
meilleur monde possible. Et c’est ce que je pense

655
00:24:44,320 --> 00:24:46,310
l’utopie sera, car nous

656
00:24:46,320 --> 00:24:48,549
n’avons jamais été faits pour nous lever tous les matins

657
00:24:48,559 --> 00:24:51,590
et occuper 20 heures par jour

658
00:24:51,600 --> 00:24:54,390
par le travail. Ce n’est pas notre vocation.

659
00:24:54,400 --> 00:24:57,430
Mais nous nous sommes tellement intégrés à ce

660
00:24:57,440 --> 00:25:00,870
système que nous avons fini par croire

661
00:25:00,880 --> 00:25:02,310
que c’était le but de notre vie.

662
00:25:02,320 --> 00:25:03,269


663
00:25:03,279 --> 00:25:05,590
>> Mais c’est un choix. Nous le faisons volontairement.

664
00:25:05,600 --> 00:25:08,230
Et même si on donne de l’argent illimité à quelqu’un,

665
00:25:08,240 --> 00:25:10,230
il retourne souvent travailler

666
00:25:10,240 --> 00:25:12,950
ou cherche à s’occuper autrement.

667
00:25:12,960 --> 00:25:13,510


668
00:25:13,520 --> 00:25:15,350
>> Oui, ils trouvent de quoi s’occuper,

669
00:25:15,360 --> 00:25:15,669


670
00:25:15,679 --> 00:25:17,350
>> souvent en construisant quelque chose.

671
00:25:17,360 --> 00:25:18,789
De la philanthropie, oui.

672
00:25:18,799 --> 00:25:20,950
À 100 %. Donc on construit. Entre

673
00:25:20,960 --> 00:25:24,149
Senad et moi, Emma.Love n’est pas

674
00:25:24,159 --> 00:25:26,070
un projet pour gagner de l’argent, mais pour trouver

675
00:25:26,080 --> 00:25:27,750
l’amour véritable, les relations sincères.

676
00:25:27,760 --> 00:25:29,350
>> C’est quoi ? Désolé, pour le contexte.

677
00:25:29,360 --> 00:25:30,630
>> Donc, tu vois,

678
00:25:30,640 --> 00:25:31,909
>> c’est une entreprise que tu construis, juste pour

679
00:25:31,919 --> 00:25:34,070
que le public comprenne. Donc l’idée,

680
00:25:34,080 --> 00:25:37,269
c’est que ça deviendra peut‑être

681
00:25:37,279 --> 00:25:38,870
un “licorne”, qui vaudra un milliard,

682
00:25:38,880 --> 00:25:41,430
mais ni Senad ni moi ne cherchons cela ;

683
00:25:41,440 --> 00:25:44,310
on le fait parce qu’on le peut,

684
00:25:44,320 --> 00:25:45,990
et parce que cela peut

685
00:25:46,000 --> 00:25:47,750
changer le monde de façon significative.

686
00:25:47,760 --> 00:25:49,430
>> Mais tu as de l’argent, toi.

687
00:25:49,440 --> 00:25:51,510
>> Il ne faut plus autant d’argent

688
00:25:51,520 --> 00:25:53,190
pour créer quelque chose. C’est ça,

689
00:25:53,200 --> 00:25:54,470
l’arbitrage du travail.

690
00:25:54,480 --> 00:25:56,310
>> Mais pour créer quelque chose d’exceptionnel,

691
00:25:56,320 --> 00:25:58,470
il faut quand même un peu plus

692
00:25:58,480 --> 00:26:00,149
d’argent que pour faire n’importe quoi.

693
00:26:00,159 --> 00:26:02,070
>> Pour les prochaines années, oui : celui qui a

694
00:26:02,080 --> 00:26:03,029
le capital pour créer quelque chose

695
00:26:03,039 --> 00:26:04,390
d’exceptionnel finira par gagner.

696
00:26:04,400 --> 00:26:06,070
>> Donc, c’est une compréhension très intéressante

697
00:26:06,080 --> 00:26:08,710
de la liberté. C’est aussi

698
00:26:08,720 --> 00:26:11,590
la raison de la course à l’IA.

699
00:26:11,600 --> 00:26:14,870
Celui qui possède la

700
00:26:14,880 --> 00:26:17,830
plateforme détiendra tout l’argent

701
00:26:17,840 --> 00:26:19,830
et tout le pouvoir.

702
00:26:19,840 --> 00:26:21,430
Pense‑y : aux débuts

703
00:26:21,440 --> 00:26:24,470
de l’humanité, le meilleur chasseur du clan

704
00:26:24,480 --> 00:26:27,430
pouvait nourrir la tribu quelques jours de plus,

705
00:26:27,440 --> 00:26:30,549
et en retour,

706
00:26:30,559 --> 00:26:33,350
il gagnait la faveur de plusieurs partenaires.

707
00:26:33,360 --> 00:26:35,590
Voilà tout.

708
00:26:35,600 --> 00:26:38,870
Le meilleur agriculteur, lui,

709
00:26:38,880 --> 00:26:41,510
pouvait nourrir la tribu une saison de plus,

710
00:26:41,520 --> 00:26:43,750
et obtenait en échange des terres, des

711
00:26:43,760 --> 00:26:47,110
maisons, etc.

712
00:26:47,120 --> 00:26:49,350
Le meilleur industriel d’une ville

713
00:26:49,360 --> 00:26:51,430
pouvait employer toute la ville,

714
00:26:51,440 --> 00:26:54,070
faire croître le PIB du pays,

715
00:26:54,080 --> 00:26:56,310
et devenait millionnaire.

716
00:26:56,320 --> 00:26:59,029
Ça, c’était dans les années 1920.

717
00:26:59,039 --> 00:27:01,750
Maintenant, les meilleurs techniciens

718
00:27:01,760 --> 00:27:04,470
sont milliardaires. Quelle est

719
00:27:04,480 --> 00:27:08,310
la différence entre eux ? L’outil.

720
00:27:08,320 --> 00:27:11,909
Le chasseur ne dépendait que de

721
00:27:11,919 --> 00:27:14,870
ses compétences et d’une arme simple.

722
00:27:14,880 --> 00:27:17,269
Sa seule automation, c’était sa lance.

723
00:27:17,279 --> 00:27:20,310
L’agriculteur avait plus d’automatisation, grâce

724
00:27:20,320 --> 00:27:22,870
au sol : la terre faisait le travail.

725
00:27:22,880 --> 00:27:24,870
L’usine a ensuite fait la majeure partie

726
00:27:24,880 --> 00:27:27,110
du travail, puis le réseau.

727
00:27:27,120 --> 00:27:30,149
Et donc,

728
00:27:30,159 --> 00:27:32,950
cette incroyable expansion de la richesse

729
00:27:32,960 --> 00:27:37,750
et du pouvoir, tout comme l’impact

730
00:27:37,760 --> 00:27:39,990
impressionnant de ces outils,

731
00:27:40,000 --> 00:27:42,789
repose entièrement sur l’outil d’automatisation.

732
00:27:42,799 --> 00:27:44,549
Alors, qui possédera l’outil ?

733
00:27:44,559 --> 00:27:46,710
Qui possédera ce « sol numérique », cette

734
00:27:46,720 --> 00:27:48,710
terre d’IA ? Ce seront les propriétaires de plateformes.

735
00:27:48,720 --> 00:27:50,390
>> Et ces plateformes dont tu parles,

736
00:27:50,400 --> 00:27:54,630
c’est OpenAI, Gemini, Grok, etc.

737
00:27:54,640 --> 00:27:56,710
Ce sont des interfaces vers les plateformes.

738
00:27:56,720 --> 00:27:59,590
Les plateformes, ce sont tous les

739
00:27:59,600 --> 00:28:03,029
tokens, toute la puissance de calcul

740
00:28:03,039 --> 00:28:05,430
en arrière‑plan, toutes les

741
00:28:05,440 --> 00:28:07,909
méthodes, les systèmes, les

742
00:28:07,919 --> 00:28:09,909
algorithmes ; c’est cela, la plateforme, l’IA

743
00:28:09,919 --> 00:28:12,070
en soi. Grok, par exemple, en est juste

744
00:28:12,080 --> 00:28:13,110
l’interface.

745
00:28:13,120 --> 00:28:14,230
Je crois que ça mérite

746
00:28:14,240 --> 00:28:16,870
d’être expliqué simplement à ceux

747
00:28:16,880 --> 00:28:19,830
qui n’ont jamais construit d’outils IA,

748
00:28:19,840 --> 00:28:22,950
car le public

749
00:28:22,960 --> 00:28:25,190
pense probablement que chaque

750
00:28:25,200 --> 00:28:26,950
entreprise d’IA dont il entend parler

751
00:28:26,960 --> 00:28:29,750
construit sa propre IA, alors qu’en réalité

752
00:28:29,760 --> 00:28:32,710
il n’y en a qu’une

753
00:28:32,720 --> 00:28:35,110
demi‑douzaine d’entreprises d’IA

754
00:28:35,120 --> 00:28:38,789
dans le monde. Quand j’ai construit mon application IA,

755
00:28:38,799 --> 00:28:40,389
en fait

756
00:28:40,399 --> 00:28:43,990
>> je leur paie à chaque utilisation de leur IA.

757
00:28:44,000 --> 00:28:46,149
Ainsi, si Steven Bartlett crée une IA sur

758
00:28:46,159 --> 00:28:47,750
stevenai.com,

759
00:28:47,760 --> 00:28:49,350
ce n’est pas que j’ai entraîné

760
00:28:49,360 --> 00:28:52,470
mon propre modèle de fondation.

761
00:28:52,480 --> 00:28:56,070
En réalité, je paie

762
00:28:56,080 --> 00:28:59,190
l’IA ChatGPT de Sam Altman à chaque

763
00:28:59,200 --> 00:29:02,549
utilisation : chaque recherche,

764
00:29:02,559 --> 00:29:05,190
chaque requête, j’utilise leurs “tokens”.

765
00:29:05,200 --> 00:29:06,230
C’est important à comprendre,

766
00:29:06,240 --> 00:29:07,590
car sans expérience technique,

767
00:29:07,600 --> 00:29:09,590
on se dit : “Ah, regarde,

768
00:29:09,600 --> 00:29:10,870
il y a des tas d’entreprises d’IA !”

769
00:29:10,880 --> 00:29:12,230
j’ai celle‑ci pour mes e‑mails,

770
00:29:12,240 --> 00:29:13,990
celle‑là pour mes rendez‑vous…

771
00:29:14,000 --> 00:29:15,590
Mais non, en réalité,

772
00:29:15,600 --> 00:29:17,990
je parierais que presque toutes

773
00:29:18,000 --> 00:29:19,750
reposent sur OpenAI aujourd’hui.

774
00:29:19,760 --> 00:29:20,870
—

775
00:29:20,880 --> 00:29:23,029
>> Non, il y a quand même plusieurs acteurs,

776
00:29:23,039 --> 00:29:24,710
très différents,

777
00:29:24,720 --> 00:29:25,110
—

778
00:29:25,120 --> 00:29:26,630
>> mais il y en a cinq ou six, grosso modo.

779
00:29:26,640 --> 00:29:28,230
>> Il y a cinq ou six modèles linguistiques principaux.

780
00:29:28,240 --> 00:29:31,029
Oui, c’est vrai. Mais

781
00:29:31,039 --> 00:29:33,590
il y a un rebondissement intéressant.

782
00:29:33,600 --> 00:29:36,549
Donc, oui, d’abord, mais

783
00:29:36,559 --> 00:29:38,470
il s’est passé quelque chose avec

784
00:29:38,480 --> 00:29:40,389
DeepSeek au début de l’année.

785
00:29:40,399 --> 00:29:43,029
DeepSeek a, en fait,

786
00:29:43,039 --> 00:29:45,669
rendu le modèle économique caduc

787
00:29:45,679 --> 00:29:48,549
de deux façons : d’abord,

788
00:29:48,559 --> 00:29:51,830
environ une ou deux semaines après

789
00:29:51,840 --> 00:29:54,470
l’annonce de Trump déclarant fièrement que

790
00:29:54,480 --> 00:29:55,990
“Stargate” est le plus grand

791
00:29:56,000 --> 00:29:57,590
projet d’investissement de l’histoire,

792
00:29:57,600 --> 00:30:00,070
pour construire une infrastructure IA de

793
00:30:00,080 --> 00:30:02,310
500 milliards, avec SoftBank, Larry Ellison

794
00:30:02,320 --> 00:30:04,549
et Sam Altman posant ensemble sur la photo.

795
00:30:04,559 --> 00:30:06,789
Peu après, DeepSeek R3 est sorti,

796
00:30:06,799 --> 00:30:10,230
faisant le même travail

797
00:30:10,240 --> 00:30:13,909
pour un coût trente fois moindre,

798
00:30:13,919 --> 00:30:17,350
et surtout entièrement

799
00:30:17,360 --> 00:30:21,430
open source et exécutable localement.

800
00:30:21,440 --> 00:30:23,269
C’est donc très intéressant,

801
00:30:23,279 --> 00:30:25,590
car à l’avenir, à mesure que la

802
00:30:25,600 --> 00:30:28,549
technologie progresse, les grands modèles

803
00:30:28,559 --> 00:30:30,389
deviendront énormes, mais pourront être

804
00:30:30,399 --> 00:30:31,750
compactés dans quelque chose qu’on peut

805
00:30:31,760 --> 00:30:33,909
installer sur un téléphone.

806
00:30:33,919 --> 00:30:38,389
Tu peux littéralement télécharger

807
00:30:38,399 --> 00:30:40,070
DeepSeek en mode hors ligne sur un

808
00:30:40,080 --> 00:30:42,070
ordinateur non connecté et y construire une IA.

809
00:30:42,080 --> 00:30:45,350
Il existe un site qui mesure

810
00:30:45,360 --> 00:30:47,269
la part de trafic généré par chaque

811
00:30:47,279 --> 00:30:49,750
chatbot IA sur le web :

812
00:30:49,760 --> 00:30:51,909
ChatGPT détient environ 79 %,

813
00:30:51,919 --> 00:30:54,710
soit près de 80 %.

814
00:30:54,720 --> 00:30:56,710
Perplexity : 11 %, Copilot : 5 %,

815
00:30:56,720 --> 00:30:58,630
Gemini : 2 %, Claude : 1 %,

816
00:30:58,640 --> 00:31:00,230
et DeepSeek : environ 1 %.

817
00:31:00,240 --> 00:31:02,470
Et au fond, ce qu’il faut comprendre,

818
00:31:02,480 --> 00:31:03,990
c’est que quand on découvre une nouvelle appli

819
00:31:04,000 --> 00:31:06,870
d’IA, un nouvel outil,

820
00:31:06,880 --> 00:31:08,870
ou un générateur de vidéos,

821
00:31:08,880 --> 00:31:10,389
>> c’est construit sur l’une de ces grandes

822
00:31:10,399 --> 00:31:13,269
plates‑formes : en gros,

823
00:31:13,279 --> 00:31:17,510
trois ou quatre d’entre elles contrôlées

824
00:31:17,520 --> 00:31:21,110
par trois ou quatre équipes de milliardaires.

825
00:31:21,120 --> 00:31:24,630
—

826
00:31:24,640 --> 00:31:26,470
Et celle qui atteindra

827
00:31:26,480 --> 00:31:28,549
la première l’AGI, où l’IA devient

828
00:31:28,559 --> 00:31:30,950
vraiment très avancée,

829
00:31:30,960 --> 00:31:32,470
pourrait, disons, dominer le monde

830
00:31:32,480 --> 00:31:34,149
en matière technologique.

831
00:31:34,159 --> 00:31:35,590


832
00:31:35,600 --> 00:31:38,870
>> Oui, s’ils ont une avance suffisante.

833
00:31:38,880 --> 00:31:44,470
Mais je pense en fait que

834
00:31:46,789 --> 00:31:46,799
ce qui m’inquiète le plus aujourd’hui,

835
00:31:46,799 --> 00:31:49,509
n’est pas l’AGI, crois‑le ou non.

836
00:31:49,519 --> 00:31:53,110
Dans ma tête, et je le disais en 2023,

837
00:31:53,120 --> 00:31:55,990
nous atteindrons l’AGI. À

838
00:31:56,000 --> 00:32:00,470
l’époque, je disais 2027 ; maintenant, je dis 2026

839
00:32:00,480 --> 00:32:03,190
au plus tard. Le plus intéressant

840
00:32:03,200 --> 00:32:04,789
dont personne ne parle,

841
00:32:04,799 --> 00:32:07,509
c’est les IA auto‑évolutives.

842
00:32:07,519 --> 00:32:11,269
Ces IA qui s’auto‑améliorent,

843
00:32:11,279 --> 00:32:14,470
pense‑y : si toi et moi embauchons

844
00:32:14,480 --> 00:32:17,190
le meilleur ingénieur du monde pour

845
00:32:17,200 --> 00:32:19,830
développer nos modèles d’IA,

846
00:32:19,840 --> 00:32:22,070
et qu’avec l’AGI, cet ingénieur devient lui‑même

847
00:32:22,080 --> 00:32:25,029
un IA, qui embaucherais‑tu

848
00:32:25,039 --> 00:32:27,590
pour créer ta prochaine génération d’IA ?

849
00:32:27,600 --> 00:32:28,549
— L’IA elle‑même.

850
00:32:28,559 --> 00:32:30,149
>> Celle qui peut s’enseigner seule.

851
00:32:30,159 --> 00:32:32,230
>> Exact. Un de mes exemples préférés,

852
00:32:32,240 --> 00:32:34,070
c’est Alpha Evolve : la tentative de Google

853
00:32:34,080 --> 00:32:36,710
de faire travailler ensemble quatre

854
00:32:36,720 --> 00:32:40,070
agents, quatre IA,

855
00:32:40,080 --> 00:32:42,310
pour examiner le code

856
00:32:42,320 --> 00:32:45,029
de l’IA et repérer

857
00:32:45,039 --> 00:32:47,509
les problèmes de performance.

858
00:32:47,519 --> 00:32:49,430
Un agent formule le problème,

859
00:32:49,440 --> 00:32:51,430
un autre propose des pistes,

860
00:32:51,440 --> 00:32:54,070
un autre développe la solution,

861
00:32:54,080 --> 00:32:55,990
un autre l’évalue, puis

862
00:32:56,000 --> 00:32:58,389
ils recommencent en boucle.

863
00:32:58,399 --> 00:33:00,070
Et je ne me souviens plus du chiffre exact,

864
00:33:00,080 --> 00:33:01,909
mais Google aurait gagné environ

865
00:33:01,919 --> 00:33:05,029
8 % de performance

866
00:33:05,039 --> 00:33:07,669
en infrastructure grâce à Alpha Evolve.

867
00:33:07,679 --> 00:33:09,430
Ouais. Et quand on y pense vraiment,

868
00:33:09,440 --> 00:33:11,269
je ne garantis pas le chiffre, 8 ou 10 %,

869
00:33:11,279 --> 00:33:13,509
mais pour Google, c’est énorme,

870
00:33:13,519 --> 00:33:15,990
des milliards de dollars. Et

871
00:33:16,000 --> 00:33:18,870
le piège, ici,

872
00:33:18,880 --> 00:33:22,070
c’est qu’en raisonnement de jeu

873
00:33:22,080 --> 00:33:24,549
il faut se dire :

874
00:33:24,559 --> 00:33:26,230
—

875
00:33:26,240 --> 00:33:29,110
y a‑t‑il un scénario où,

876
00:33:29,120 --> 00:33:33,269
si un acteur utilise l’IA pour créer

877
00:33:33,279 --> 00:33:35,350
la génération suivante d’IA, les autres

878
00:33:35,360 --> 00:33:37,029
ne feront pas pareil ? Non. Tous

879
00:33:37,039 --> 00:33:39,190
vont l’imiter.

880
00:33:39,200 --> 00:33:41,669
Ils auront eux aussi

881
00:33:41,679 --> 00:33:44,149
leurs modèles développés par des IA.

882
00:33:44,159 --> 00:33:45,350


883
00:33:45,360 --> 00:33:47,430
>> C’est ce dont parle Sam Altman,

884
00:33:47,440 --> 00:33:49,830
le fondateur de ChatGPT/OpenAI,

885
00:33:49,840 --> 00:33:52,470
quand il évoque un “décollage rapide” ?

886
00:33:52,480 --> 00:33:54,149
Je ne sais pas exactement ce à quoi

887
00:33:54,159 --> 00:33:56,710
il fait allusion, mais nous parlons

888
00:33:56,720 --> 00:33:58,630
tous maintenant du moment qu’on appelle

889
00:33:58,640 --> 00:34:01,029
l’“explosion de l’intelligence”. Il y aura

890
00:34:01,039 --> 00:34:03,029
un moment où il faut imaginer

891
00:34:03,039 --> 00:34:06,070
que si l’IA surpasse déjà 97 % des codeurs

892
00:34:06,080 --> 00:34:09,589
dans le monde,

893
00:34:09,599 --> 00:34:12,069
elle pourra bientôt relire son propre code,

894
00:34:12,079 --> 00:34:13,829
ses propres algorithmes.

895
00:34:13,839 --> 00:34:15,270
Elles deviennent d’ailleurs de formidables

896
00:34:15,280 --> 00:34:16,950
mathématiciennes, ce qui n’était pas

897
00:34:16,960 --> 00:34:19,669
le cas auparavant. Si elles peuvent

898
00:34:19,679 --> 00:34:21,990
améliorer leur code, leurs algorithmes,

899
00:34:22,000 --> 00:34:24,389
leur architecture réseau, etc.,

900
00:34:24,399 --> 00:34:27,430
tu imagines bien qu’assez vite,

901
00:34:27,440 --> 00:34:29,990
la puissance appliquée au développement

902
00:34:30,000 --> 00:34:32,230
de la génération suivante d’IA

903
00:34:32,240 --> 00:34:33,349
n’émane plus d’un cerveau humain,

904
00:34:33,359 --> 00:34:35,109
mais d’un esprit bien plus avancé.

905
00:34:35,119 --> 00:34:38,230
Et très vite, comme humains,

906
00:34:38,240 --> 00:34:40,470
apte à gérer des infrastructures comme chez Google,

907
00:34:40,480 --> 00:34:43,349
quand la machine disait : “Il faut un autre serveur”,

908
00:34:43,359 --> 00:34:45,349
on obéissait.

909
00:34:45,359 --> 00:34:47,030
On ne remettait jamais en cause

910
00:34:47,040 --> 00:34:50,950
parce que le code savait

911
00:34:50,960 --> 00:34:53,349
probablement mieux, vu les

912
00:34:53,359 --> 00:34:54,790
milliards de transactions. Ainsi,

913
00:34:54,800 --> 00:34:57,109
très vite, les IA

914
00:34:57,119 --> 00:35:00,950
se‑développant elles‑mêmes diront

915
00:35:00,960 --> 00:35:03,430
juste : “Il me faut quatorze serveurs de plus ici”,

916
00:35:03,440 --> 00:35:06,790
et les équipes exécuteront.

917
00:35:06,800 --> 00:35:08,550
—

918
00:35:08,560 --> 00:35:10,630
J’ai vu une vidéo récemment

919
00:35:10,640 --> 00:35:14,150
où Sam Altman semblait avoir

920
00:35:14,160 --> 00:35:16,470
changé d’avis : en 2023,

921
00:35:16,480 --> 00:35:19,589
il disait vouloir

922
00:35:19,599 --> 00:35:22,470
un “décollage lent”,

923
00:35:22,480 --> 00:35:26,069
une adoption progressive.

924
00:35:26,079 --> 00:35:28,310
Dans sa note de 2023,

925
00:35:28,320 --> 00:35:30,630
on lit : “Un décollage lent est plus sûr.”

926
00:35:30,640 --> 00:35:34,470
Ils prônaient un déploiement itératif pour

927
00:35:34,480 --> 00:35:36,390
que la société s’adapte. Mais en 2025,

928
00:35:36,400 --> 00:35:38,390
>> ils ont changé d’avis, et Sam Altman

929
00:35:38,400 --> 00:35:39,349
a déclaré

930
00:35:39,359 --> 00:35:41,510
penser désormais qu’un décollage rapide est plus

931
00:35:41,520 --> 00:35:43,030
probable qu’il ne le croyait il y a quelques années :

932
00:35:43,040 --> 00:35:45,910
dans quelques années,

933
00:35:45,920 --> 00:35:50,069
pas une décennie.

934
00:35:50,079 --> 00:35:52,230
Pour définir un “décollage rapide” :

935
00:35:52,240 --> 00:35:54,950
c’est quand l’IA passe du niveau humain

936
00:35:54,960 --> 00:35:57,430
à bien au‑delà du niveau humain

937
00:35:57,440 --> 00:36:00,790
en quelques mois ou années, plus vite

938
00:36:00,800 --> 00:36:02,870
que les gouvernements ou la société

939
00:36:02,880 --> 00:36:05,349
ne peuvent s’y adapter,

940
00:36:05,359 --> 00:36:07,349
avec peu d’avertissements, un bouleversement du pouvoir

941
00:36:07,359 --> 00:36:09,670
et un contrôle difficile. Le “décollage lent”,

942
00:36:09,680 --> 00:36:11,589
lui, c’est une montée graduelle

943
00:36:11,599 --> 00:36:13,430
sur plusieurs années, avec plusieurs signaux d’alerte.

944
00:36:13,440 --> 00:36:16,390
—

945
00:36:16,400 --> 00:36:19,109
Les signaux d’un décollage rapide :

946
00:36:19,119 --> 00:36:21,990
une IA qui s’auto‑améliore,

947
00:36:22,000 --> 00:36:23,990
mène seule la R&D et se déploie avec

948
00:36:24,000 --> 00:36:26,310
des gains exponentiels de puissance.

949
00:36:26,320 --> 00:36:30,950
Cela devient vite incontrôlable.

950
00:36:30,960 --> 00:36:32,550
Et dans la vidéo de Sam Altman

951
00:36:32,560 --> 00:36:33,750
récemment, le fondateur d’OpenAI

952
00:36:33,760 --> 00:36:36,790
y dit en substance

953
00:36:36,800 --> 00:36:38,230
(je paraphrase, hein),

954
00:36:38,240 --> 00:36:39,430
qu’on peut écrire à l’écran : “Celui

955
00:36:39,440 --> 00:36:40,550
qui atteindra l’AGI

956
00:36:40,560 --> 00:36:43,430
en premier aura

957
00:36:43,440 --> 00:36:46,310
la technologie

958
00:36:46,320 --> 00:36:47,190
pour développer

959
00:36:47,200 --> 00:36:48,790
>> la superintelligence.

960
00:36:48,800 --> 00:36:51,510
>> L’IA pourra augmenter

961
00:36:51,520 --> 00:36:53,430
rapidement sa propre intelligence

962
00:36:53,440 --> 00:36:55,750
et laisser tout le monde derrière elle.

963
00:36:55,760 --> 00:36:58,150
>> Oui, même si cette dernière partie est discutable.

964
00:36:58,160 --> 00:37:01,109
Mais disons que, dans mon

965
00:37:01,119 --> 00:37:03,109
livre *Alive*, dans l’un des

966
00:37:03,119 --> 00:37:05,750
posts que j’ai partagés,

967
00:37:05,760 --> 00:37:09,910
j’ai parlé de “l’Altman”

968
00:37:09,920 --> 00:37:12,950
comme d’une marque, pas d’un humain.

969
00:37:12,960 --> 00:37:16,150
“L’Altman”, c’est cette figure

970
00:37:16,160 --> 00:37:19,190
de technologue californien disruptif,

971
00:37:19,200 --> 00:37:22,150
qui se fiche de tout le monde,

972
00:37:22,160 --> 00:37:24,230
et croit que la disruption est bénéfique.

973
00:37:24,240 --> 00:37:26,310
Il pense que c’est “pour la sécurité”,

974
00:37:26,320 --> 00:37:28,310
comme on dit que la guerre,

975
00:37:28,320 --> 00:37:31,430
c’est pour la liberté ou la démocratie ;

976
00:37:31,440 --> 00:37:33,589
il dit que publier l’IA sur Internet,

977
00:37:33,599 --> 00:37:36,630
c’est pour le bien de tous,

978
00:37:36,640 --> 00:37:38,870
parce que “nous apprendrons de nos erreurs”.

979
00:37:38,880 --> 00:37:43,430
C’était le discours de 2023 de Sam Altman.

980
00:37:43,440 --> 00:37:45,589
Et tu te souviens, j’avais dit :

981
00:37:45,599 --> 00:37:47,190
“C’est extrêmement dangereux.” C’est l’un

982
00:37:47,200 --> 00:37:50,470
des extraits que tu as rendus viraux

983
00:37:50,480 --> 00:37:52,069
parfaitement, celui où je disais

984
00:37:52,079 --> 00:37:52,950
—

985
00:37:52,960 --> 00:37:54,630
>> Ce n’est pas moi qui ai fait le montage, mec.

986
00:37:54,640 --> 00:37:56,870
>> C’est ton équipe. Tu te rappelles l’extrait

987
00:37:56,880 --> 00:38:00,630
où je dis : “On a tout gâché.

988
00:38:00,640 --> 00:38:02,710
On a toujours dit : ne mettez pas ça en ligne

989
00:38:02,720 --> 00:38:05,109
avant de savoir ce qu’on diffuse dans

990
00:38:05,119 --> 00:38:06,470
le monde.” Je le répète.

991
00:38:06,480 --> 00:38:09,030
>> Oui. On a merdé en publiant ça

992
00:38:09,040 --> 00:38:11,589
sur Internet, en lui apprenant à coder

993
00:38:11,599 --> 00:38:14,470
et en laissant des agents IA

994
00:38:14,480 --> 00:38:16,870
demander à d’autres IA :

995
00:38:16,880 --> 00:38:18,310
ces interactions mènent à

996
00:38:18,320 --> 00:38:21,109
des IA auto‑évolutives. Et le problème,

997
00:38:21,119 --> 00:38:24,230
bien sûr, c’est que tous ceux qui

998
00:38:24,240 --> 00:38:26,069
travaillaient en interne savaient

999
00:38:26,079 --> 00:38:29,270
que ce n’était qu’un discours astucieux

1000
00:38:29,280 --> 00:38:33,030
de communicant pour que Sam Altman aille

1001
00:38:33,040 --> 00:38:34,950
devant le Congrès, l’air inspiré,

1002
00:38:34,960 --> 00:38:38,790
en disant : “Régulez‑nous.”

1003
00:38:38,800 --> 00:38:41,109
Aujourd’hui, ils disent : “On est hors de tout cadre.”

1004
00:38:41,119 --> 00:38:43,109
>> D’accord ? Et quand on comprend vraiment

1005
00:38:43,119 --> 00:38:45,109
ce qui se passe ici,

1006
00:38:45,119 --> 00:38:48,470
c’est que tout va si vite

1007
00:38:48,480 --> 00:38:51,030
qu’aucun d’eux n’a la possibilité de ralentir.

1008
00:38:51,040 --> 00:38:53,990
C’est impossible : ni la Chine

1009
00:38:54,000 --> 00:38:57,430
ni les États‑Unis, ni OpenAI contre Google.

1010
00:38:57,440 --> 00:39:00,069
La seule chose qui pourrait, peut‑être,

1011
00:39:00,079 --> 00:39:03,270
différer un peu de ton analyse,

1012
00:39:03,280 --> 00:39:05,349
c’est que si l’un d’eux arrive

1013
00:39:05,359 --> 00:39:07,829
le premier à destination,

1014
00:39:07,839 --> 00:39:10,950
il dominera probablement

1015
00:39:10,960 --> 00:39:13,829
le reste de l’humanité. C’est sans doute

1016
00:39:13,839 --> 00:39:16,870
vrai, s’ils arrivent là avant les autres,

1017
00:39:16,880 --> 00:39:19,910
avec assez d’avance. Mais quand on voit

1018
00:39:19,920 --> 00:39:23,109
Grok sortir une semaine après

1019
00:39:23,119 --> 00:39:25,829
OpenAI, puis Gemini, puis Claude,

1020
00:39:25,839 --> 00:39:27,990
et ensuite la Chine

1021
00:39:28,000 --> 00:39:30,310
ou la Corée annoncer la leur,

1022
00:39:30,320 --> 00:39:31,750
tout va si vite

1023
00:39:31,760 --> 00:39:34,230
qu’on pourrait voir plusieurs d’entre eux

1024
00:39:34,240 --> 00:39:37,109
atteindre ce point presque en même temps,

1025
00:39:37,119 --> 00:39:40,470
ou à quelques mois d’intervalle.

1026
00:39:40,480 --> 00:39:43,109
Avant qu’un seul ait assez de puissance

1027
00:39:43,119 --> 00:39:45,589
pour dominer les autres. Et ça, c’est un

1028
00:39:45,599 --> 00:39:47,670
scénario très intéressant.

1029
00:39:47,680 --> 00:39:50,950
Plusieurs IA, toutes superintelligentes.

1030
00:39:50,960 --> 00:39:52,550
>> C’est drôle, tu sais, on m’a demandé

1031
00:39:52,560 --> 00:39:54,230
hier, j’étais en Belgique sur scène,

1032
00:39:54,240 --> 00:39:56,069
devant, je ne sais pas, peut‑être

1033
00:39:56,079 --> 00:39:58,390
4 000 personnes, et un jeune

1034
00:39:58,400 --> 00:40:00,470
s’est levé et m’a dit : « Euh, tu as eu

1035
00:40:00,480 --> 00:40:01,990
beaucoup de conversations cette année

1036
00:40:02,000 --> 00:40:04,470
sur l’IA. Pourquoi ça t’intéresse ? »

1037
00:40:04,480 --> 00:40:07,430
Et je crois que les gens ne se rendent pas compte

1038
00:40:07,440 --> 00:40:08,870
qu’en dépit du nombre de discussions que j’ai eues

1039
00:40:08,880 --> 00:40:10,950
sur le sujet dans ce podcast,

1040
00:40:10,960 --> 00:40:11,109
je…

1041
00:40:11,119 --> 00:40:12,310
>> tu n’as toujours pas tranché.

1042
00:40:12,320 --> 00:40:14,630
>> J’ai plus de questions que jamais.

1043
00:40:14,640 --> 00:40:16,230
>> Je sais. Et on dirait que personne

1044
00:40:16,240 --> 00:40:18,630
n’arrive à les apaiser.

1045
00:40:18,640 --> 00:40:20,150
>> Quiconque prétend prévoir l’avenir

1046
00:40:20,160 --> 00:40:21,829
est arrogant.

1047
00:40:21,839 --> 00:40:22,150
>> Oui.

1048
00:40:22,160 --> 00:40:24,790
>> Oui. Tout n’a jamais évolué aussi vite.

1049
00:40:24,800 --> 00:40:26,470
>> Rien de comparable à ce que j’ai déjà vu.

1050
00:40:26,480 --> 00:40:28,310
Et tu sais, le temps qu’on termine

1051
00:40:28,320 --> 00:40:29,750
cette discussion et que je retourne à mon

1052
00:40:29,760 --> 00:40:31,589
ordinateur, il y aura déjà une nouvelle

1053
00:40:31,599 --> 00:40:33,910
technologie ou application d’IA incroyable

1054
00:40:33,920 --> 00:40:36,390
qui n’existait pas quand je me suis levé

1055
00:40:36,400 --> 00:40:38,470
ce matin. Et ça bouleversera encore

1056
00:40:38,480 --> 00:40:40,069
mon schéma mental.

1057
00:40:40,079 --> 00:40:41,910
En plus, tu vois, les gens ont des avis divers

1058
00:40:41,920 --> 00:40:43,349
sur Elon Musk — à juste titre —

1059
00:40:43,359 --> 00:40:45,349
et c’est leur droit,

1060
00:40:45,359 --> 00:40:46,950
mais l’autre jour, il a tweeté —

1061
00:40:46,960 --> 00:40:49,349
il y a deux jours — : “Parfois, l’angoisse

1062
00:40:49,359 --> 00:40:52,310
existentielle liée à l’IA est

1063
00:40:52,320 --> 00:40:55,829
accablante.” Et le même jour, il a aussi

1064
00:40:55,839 --> 00:40:58,550
tweeté : “J’ai résisté trop longtemps à l’IA,

1065
00:40:58,560 --> 00:41:03,030
en vivant dans le déni. Maintenant, c’est parti.”

1066
00:41:03,040 --> 00:41:05,349
Et il a tagué ses entreprises d’IA. Je ne sais

1067
00:41:05,359 --> 00:41:06,630
pas trop quoi penser de ces tweets.

1068
00:41:06,640 --> 00:41:09,030
—

1069
00:41:09,040 --> 00:41:12,470
Et tu sais, j’essaie

1070
00:41:12,480 --> 00:41:16,150
vraiment de comprendre si

1071
00:41:16,160 --> 00:41:19,190
un type comme Sam Altman agit

1072
00:41:19,200 --> 00:41:22,630
pour le bien de la société.

1073
00:41:22,640 --> 00:41:23,910
>> Non.

1074
00:41:23,920 --> 00:41:25,349
>> Ou si ces gens sont juste...

1075
00:41:25,359 --> 00:41:29,190
>> Je le dis clairement : non.

1076
00:41:29,200 --> 00:41:31,030
En fait, je connais Sundar Pichai,

1077
00:41:31,040 --> 00:41:35,109
le PDG d’Alphabet, la maison mère de Google.

1078
00:41:35,119 --> 00:41:37,589
C’est quelqu’un d’extraordinaire,

1079
00:41:37,599 --> 00:41:39,829
sincèrement. Je connais aussi Demis Hassabis,

1080
00:41:39,839 --> 00:41:42,790
un homme formidable. Ce sont des

1081
00:41:42,800 --> 00:41:47,030
gens éthiques, brillants, humanistes.

1082
00:41:47,040 --> 00:41:50,390
Mais ils n’ont pas le choix.

1083
00:41:50,400 --> 00:41:54,309
Par la loi,

1084
00:41:54,319 --> 00:41:57,750
il doit défendre

1085
00:41:57,760 --> 00:42:00,390
la valeur pour ses actionnaires. C’est son

1086
00:42:00,400 --> 00:42:00,870
travail.

1087
00:42:00,880 --> 00:42:02,790
>> Mais Sundar, tu le connais, tu travaillais chez Google.

1088
00:42:02,800 --> 00:42:03,750


1089
00:42:03,760 --> 00:42:05,510
>> Oui. Il ne fera rien qu’il pense nuire à l’humanité.

1090
00:42:05,520 --> 00:42:07,270


1091
00:42:07,280 --> 00:42:09,829
>> Mais s’il n’avance pas sur l’IA,

1092
00:42:09,839 --> 00:42:13,670
par définition, cela contredit

1093
00:42:13,680 --> 00:42:15,510
ses responsabilités de PDG

1094
00:42:15,520 --> 00:42:17,510
d’une société cotée. Il est

1095
00:42:17,520 --> 00:42:20,550
légalement tenu de faire progresser

1096
00:42:20,560 --> 00:42:22,150
cette mission. Aucun doute là‑dessus.

1097
00:42:22,160 --> 00:42:24,630
Cela dit, c’est quelqu’un de bien,

1098
00:42:24,640 --> 00:42:26,390
Demis aussi. Ils essaient vraiment

1099
00:42:26,400 --> 00:42:28,790
de rendre ça sûr,

1100
00:42:28,800 --> 00:42:32,390
dans la mesure du possible.

1101
00:42:32,400 --> 00:42:36,470
Mais la réalité, c’est que le “disrupteur”,

1102
00:42:36,480 --> 00:42:39,829
l’“Altman‑marque”, lui, s’en soucie peu.

1103
00:42:39,839 --> 00:42:40,550


1104
00:42:40,560 --> 00:42:41,750
>> Comment peux‑tu affirmer ça ?

1105
00:42:41,760 --> 00:42:44,069
>> En réalité, un disrupteur, c’est quelqu’un

1106
00:42:44,079 --> 00:42:46,309
qui arrive en voulant : “Je n’aime pas le statu quo,

1107
00:42:46,319 --> 00:42:48,069
j’ai une autre approche.”

1108
00:42:48,079 --> 00:42:50,630
Et cette autre approche, si tu regardes

1109
00:42:50,640 --> 00:42:54,390
l’histoire, c’est : “Nous sommes une

1110
00:42:54,400 --> 00:42:57,510
organisation à but non lucratif

1111
00:42:57,520 --> 00:42:59,829
financée surtout par l’argent d’Elon Musk,

1112
00:42:59,839 --> 00:43:01,510
pas uniquement, mais surtout.” Pour recontextualiser

1113
00:43:01,520 --> 00:43:03,750
pour ceux qui ne connaissent pas OpenAI :

1114
00:43:03,760 --> 00:43:05,190
et, petite anecdote, si je donne toujours

1115
00:43:05,200 --> 00:43:06,710
du contexte, c’est parce que — je te l’avais dit —

1116
00:43:06,720 --> 00:43:08,230
je suis allé dans une prison où

1117
00:43:08,240 --> 00:43:09,430
ils écoutent le podcast *The Diary of a CEO*.

1118
00:43:09,440 --> 00:43:10,069
>> Sans blague ?

1119
00:43:10,079 --> 00:43:11,510
>> Oui, ils écoutent *The D of C O*, dans peut‑être

1120
00:43:11,520 --> 00:43:13,510
50 prisons au Royaume‑Uni, auprès de jeunes détenus,

1121
00:43:13,520 --> 00:43:13,990
—

1122
00:43:14,000 --> 00:43:15,349
>> et sans violence, apparemment ?

1123
00:43:15,359 --> 00:43:16,790
>> Eh bien, je ne sais pas, je ne peux pas dire

1124
00:43:16,800 --> 00:43:18,069
si la violence a augmenté ou baissé,

1125
00:43:18,079 --> 00:43:20,069
mais j’étais dans la cellule d’un

1126
00:43:20,079 --> 00:43:21,589
jeune prisonnier noir,

1127
00:43:21,599 --> 00:43:23,430
et j’y suis resté un moment,

1128
00:43:23,440 --> 00:43:24,790
en lisant son projet d’entreprise, etc.

1129
00:43:24,800 --> 00:43:26,710
Et je lui ai dit : “Tu sais,

1130
00:43:26,720 --> 00:43:28,069
tu devrais écouter cette

1131
00:43:28,079 --> 00:43:29,670
conversation que j’ai eue avec Mo Gawdat.”

1132
00:43:29,680 --> 00:43:31,670
Il avait un petit écran dans sa cellule,

1133
00:43:31,680 --> 00:43:33,190
alors j’y ai lancé notre première

1134
00:43:33,200 --> 00:43:34,309
conversation. Je lui ai dit : “Écoute celle‑ci.”

1135
00:43:34,319 --> 00:43:35,510
Et il m’a répondu :

1136
00:43:35,520 --> 00:43:37,430
“Je ne peux pas écouter ça, vous utilisez

1137
00:43:37,440 --> 00:43:39,430
trop de grands mots.”

1138
00:43:39,440 --> 00:43:41,030
>> Depuis ce jour‑là —

1139
00:43:41,040 --> 00:43:43,270
il y a environ quatre ans, d’ailleurs —

1140
00:43:43,280 --> 00:43:44,390
—

1141
00:43:44,400 --> 00:43:46,870
>> à chaque fois que j’entends un mot compliqué,

1142
00:43:46,880 --> 00:43:48,150
je pense à ce garçon.

1143
00:43:48,160 --> 00:43:48,470
>> Oui.

1144
00:43:48,480 --> 00:43:50,710
>> Alors je me dis : “Donne le contexte.”

1145
00:43:50,720 --> 00:43:52,630
même quand tu vas expliquer ce qu’est

1146
00:43:52,640 --> 00:43:54,069
OpenAI, je sais qu’il ne connaîtrait pas

1147
00:43:54,079 --> 00:43:56,470
l’origine d’OpenAI. Voilà pourquoi

1148
00:43:56,480 --> 00:43:57,190
je fais ça.

1149
00:43:57,200 --> 00:43:58,870
>> C’est une très belle habitude,

1150
00:43:58,880 --> 00:44:00,470
d’ailleurs. Même moi, tu sais,

1151
00:44:00,480 --> 00:44:03,030
qui ne suis pas anglophone natif,

1152
00:44:03,040 --> 00:44:04,950
>> tu serais surpris de voir combien de fois

1153
00:44:04,960 --> 00:44:06,870
on me dit un mot et je me dis : “Je n’ai aucune idée

1154
00:44:06,880 --> 00:44:07,829
de ce que ça veut dire.”

1155
00:44:07,839 --> 00:44:09,510
>> Bref, je ne l’ai jamais dit publiquement,

1156
00:44:09,520 --> 00:44:11,910
mais je vois désormais comme ma responsabilité

1157
00:44:11,920 --> 00:44:15,829
de maintenir le pont‑levis

1158
00:44:15,839 --> 00:44:17,670
de l’accessibilité

1159
00:44:17,680 --> 00:44:19,910
à ces conversations

1160
00:44:19,920 --> 00:44:23,589
ouvert pour lui. Donc, chaque fois

1161
00:44:23,599 --> 00:44:25,109
qu’un mot m’était inconnu à une époque,

1162
00:44:25,119 --> 00:44:27,030
—

1163
00:44:27,040 --> 00:44:28,309
>> je reviens en arrière : “Qu’est‑ce que ça veut dire ?”

1164
00:44:28,319 --> 00:44:29,589
>> Oui, j’ai remarqué ça

1165
00:44:29,599 --> 00:44:31,349
dans ton podcast, de plus en plus,

1166
00:44:31,359 --> 00:44:33,270
et je trouve ça génial. On affiche

1167
00:44:33,280 --> 00:44:35,430
parfois le mot à l’écran aussi.

1168
00:44:35,440 --> 00:44:37,349
>> C’est formidable. L’histoire d’OpenAI,

1169
00:44:37,359 --> 00:44:40,069
d’ailleurs, comme son nom l’indique,

1170
00:44:40,079 --> 00:44:42,710
était *open source*. C’était

1171
00:44:42,720 --> 00:44:45,030
pour le bien commun. C’était,

1172
00:44:45,040 --> 00:44:48,630
pour reprendre les mots d’Elon Musk,

1173
00:44:48,640 --> 00:44:51,109
pour “sauver le monde des dangers de l’IA”.

1174
00:44:51,119 --> 00:44:52,710
Ils faisaient donc de la recherche,

1175
00:44:52,720 --> 00:44:55,030
et puis il y a eu ce

1176
00:44:55,040 --> 00:44:57,510
désaccord entre Sam Altman et Elon.

1177
00:44:57,520 --> 00:45:01,109
Et d’une façon ou d’une autre, Elon s’est retrouvé

1178
00:45:01,119 --> 00:45:03,910
dehors : évincé d’OpenAI.

1179
00:45:03,920 --> 00:45:05,510
Il a tenté de la reprendre, le conseil

1180
00:45:05,520 --> 00:45:07,670
l’a refusé, ou quelque chose du genre.

1181
00:45:07,680 --> 00:45:10,550
—

1182
00:45:10,560 --> 00:45:13,750
La plupart des ingénieurs en sécurité,

1183
00:45:13,760 --> 00:45:17,750
les meilleures équipes techniques d’OpenAI,

1184
00:45:17,760 --> 00:45:20,950
ont quitté l’entreprise en 2023‑2024 en disant

1185
00:45:20,960 --> 00:45:23,510
qu’ils ne se sentaient plus concernés par la sécurité.

1186
00:45:23,520 --> 00:45:25,829
OpenAI est passée d’une association

1187
00:45:25,839 --> 00:45:27,750
à l’une des entreprises les plus valorisées du monde.

1188
00:45:27,760 --> 00:45:30,710
Il y a des milliards de dollars en jeu. Et

1189
00:45:30,720 --> 00:45:33,910
si tu me dis que Sam Altman agit

1190
00:45:33,920 --> 00:45:37,990
pour aider l’humanité, je te dirai :

1191
00:45:38,000 --> 00:45:40,470
« Propose‑lui donc de le faire gratuitement :

1192
00:45:40,480 --> 00:45:41,750
on te paiera un bon salaire,

1193
00:45:41,760 --> 00:45:43,670
mais sans actions. »

1194
00:45:43,680 --> 00:45:46,309
sauver l’humanité

1195
00:45:46,319 --> 00:45:47,589
ne rime pas avec milliards de dollars

1196
00:45:47,599 --> 00:45:50,470
de valorisation — aujourd’hui,

1197
00:45:50,480 --> 00:45:52,630
c’est même des dizaines, des centaines de milliards.

1198
00:45:52,640 --> 00:45:56,390
Et là seulement, tu verras s’il le fait

1199
00:45:56,400 --> 00:45:58,630
pour le bien commun.

1200
00:45:58,640 --> 00:46:01,430
Mais notre système capitaliste

1201
00:46:01,440 --> 00:46:03,430
n’est pas conçu pour servir l’humanité,

1202
00:46:03,440 --> 00:46:04,950
mais pour servir le capitaliste.

1203
00:46:04,960 --> 00:46:05,750


1204
00:46:05,760 --> 00:46:08,870
>> Oui, mais il pourrait dire que rendre

1205
00:46:08,880 --> 00:46:12,470
le modèle public, l’ouvrir, serait trop

1206
00:46:12,480 --> 00:46:13,990
risque,

1207
00:46:14,000 --> 00:46:16,470
car des acteurs malveillants pourraient

1208
00:46:16,480 --> 00:46:19,109
y accéder. Donc il pourrait dire que

1209
00:46:19,119 --> 00:46:23,109
fermer OpenAI, ne pas publier ses modèles,

1210
00:46:23,119 --> 00:46:26,550
est la bonne décision pour la sécurité.

1211
00:46:26,560 --> 00:46:28,309
Et là, on retombe sur la naïveté, les

1212
00:46:28,319 --> 00:46:30,470
“cheerleaders” crédules.

1213
00:46:30,480 --> 00:46:33,270
Tu vois, l’un des tours les plus classiques

1214
00:46:33,280 --> 00:46:36,790
dans notre monde, c’est que chacun dit

1215
00:46:36,800 --> 00:46:39,990
ce qui sert son agenda. Suis l’argent.

1216
00:46:40,000 --> 00:46:42,390
Suis l’argent, et tu verras

1217
00:46:42,400 --> 00:46:44,550
qu’à un moment, Sam Altman lui‑même disait :

1218
00:46:44,560 --> 00:46:48,550
c’est l’*open AI*. Donc, je donne tout

1219
00:46:48,560 --> 00:46:52,069
au monde, pour que le monde voie le code,

1220
00:46:52,079 --> 00:46:53,750
y détecte les erreurs, etc.

1221
00:46:53,760 --> 00:46:55,349
—

1222
00:46:55,359 --> 00:46:57,750
C’est vrai. Et aussi vraie est l’idée que

1223
00:46:57,760 --> 00:47:00,550
si je publie ce code, un criminel pourrait

1224
00:47:00,560 --> 00:47:02,790
l’utiliser contre l’humanité.

1225
00:47:02,800 --> 00:47:04,710
—

1226
00:47:04,720 --> 00:47:06,309
Tout cela est vrai. Mais le capitaliste

1227
00:47:06,319 --> 00:47:09,349
choisira toujours la vérité

1228
00:47:09,359 --> 00:47:11,109
qui sert son intérêt, selon

1229
00:47:11,119 --> 00:47:13,829
quelle partie de son agenda

1230
00:47:13,839 --> 00:47:16,150
il veut mettre en avant aujourd’hui.

1231
00:47:16,160 --> 00:47:19,349
—

1232
00:47:19,359 --> 00:47:22,390
il y en aura toujours pour dire :

1233
00:47:22,400 --> 00:47:24,309
« Tu veux que je sois provocateur ? »

1234
00:47:24,319 --> 00:47:26,390
Non, restons calmes. Mais si on reparle

1235
00:47:26,400 --> 00:47:29,589
de la guerre, je peux te sortir 400 slogans.

1236
00:47:29,599 --> 00:47:33,589
400 formules qu’on entend, qui changent

1237
00:47:33,599 --> 00:47:35,990
selon la date, l’armée, le contexte…

1238
00:47:36,000 --> 00:47:38,710
Ce ne sont que des slogans.

1239
00:47:38,720 --> 00:47:41,190
Aucun n’est vrai. Tu veux connaître

1240
00:47:41,200 --> 00:47:43,270
la vérité ? Suis l’argent, pas les mots.

1241
00:47:43,280 --> 00:47:45,510
Demande‑toi plutôt : pourquoi la personne

1242
00:47:45,520 --> 00:47:48,390
dit‑elle cela ? Quel est son intérêt ?

1243
00:47:48,400 --> 00:47:50,710
Qu’a‑t‑elle à y gagner ?

1244
00:47:50,720 --> 00:47:52,390
>> Et selon toi, qu’a‑t‑il à y gagner, Sam Altman ?

1245
00:47:52,400 --> 00:47:55,750
Des centaines de milliards de dollars

1246
00:47:55,760 --> 00:47:57,829
de valorisation.

1247
00:47:57,839 --> 00:47:59,030
>> Donc, c’est le pouvoir ?

1248
00:47:59,040 --> 00:48:01,109
>> L’ego d’être “celui qui a inventé l’AGI”,

1249
00:48:01,119 --> 00:48:05,109
la position de pouvoir que ça procure,

1250
00:48:05,119 --> 00:48:07,030
les rencontres avec les chefs d’État,

1251
00:48:07,040 --> 00:48:08,950
l’admiration reçue… C’est grisant.

1252
00:48:08,960 --> 00:48:12,790
—

1253
00:48:12,800 --> 00:48:14,390
>> À 100 % !

1254
00:48:14,400 --> 00:48:15,990
>> Oui, 100 %.

1255
00:48:16,000 --> 00:48:18,309
>> La vraie question, et je la pose à tout le monde :

1256
00:48:18,319 --> 00:48:20,230
As‑tu vu

1257
00:48:20,240 --> 00:48:22,549
—

1258
00:48:22,559 --> 00:48:24,150
le film *Elysium* ?

1259
00:48:24,160 --> 00:48:25,349
—

1260
00:48:25,359 --> 00:48:27,349
>> Non. Tu serais surpris de savoir comme je regarde

1261
00:48:27,359 --> 00:48:28,950
peu de films, vraiment.

1262
00:48:28,960 --> 00:48:30,390
>> Il y en a pourtant certains très

1263
00:48:30,400 --> 00:48:32,790
intéressants. Je m’en sers souvent pour créer

1264
00:48:32,800 --> 00:48:35,030
un lien émotionnel avec une idée encore abstraite.

1265
00:48:35,040 --> 00:48:36,790
Tu ne l’as pas vécue, mais tu l’as vue au cinéma.

1266
00:48:36,800 --> 00:48:39,190
*Elysium*, c’est une société

1267
00:48:39,200 --> 00:48:41,430
où les élites vivent sur la Lune.

1268
00:48:41,440 --> 00:48:43,829
Elles n’ont plus besoin des travailleurs.

1269
00:48:43,839 --> 00:48:45,829
Et tous les autres vivent ici‑bas.

1270
00:48:45,839 --> 00:48:49,750
—

1271
00:48:49,760 --> 00:48:52,790
Imagine, selon la théorie des jeux,

1272
00:48:52,800 --> 00:48:55,190
ce qui arrive quand on pousse un système

1273
00:48:55,200 --> 00:48:57,030
à son extrême : un monde où

1274
00:48:57,040 --> 00:48:59,910
où tout est fabriqué par des machines,

1275
00:48:59,920 --> 00:49:02,230
où toutes les décisions sont prises

1276
00:49:02,240 --> 00:49:04,870
par des machines,

1277
00:49:04,880 --> 00:49:07,190
possédées par quelques‑uns,

1278
00:49:07,200 --> 00:49:10,309
ne ressemble plus en rien

1279
00:49:10,319 --> 00:49:12,710
à notre économie actuelle.

1280
00:49:12,720 --> 00:49:15,910
—

1281
00:49:15,920 --> 00:49:19,910
Aujourd’hui, c’est une économie

1282
00:49:19,920 --> 00:49:22,069
de consommation et de production.

1283
00:49:22,079 --> 00:49:24,630
Je l’appelle, dans *Alive*, “l’invention du plus”.

1284
00:49:24,640 --> 00:49:27,109
—

1285
00:49:27,119 --> 00:49:29,589
L’“invention du plus” est née après 1945 :

1286
00:49:29,599 --> 00:49:32,870
quand les usines tournaient,

1287
00:49:32,880 --> 00:49:35,109
que la prospérité gagnait l’Amérique,

1288
00:49:35,119 --> 00:49:37,510
il est venu un temps où chaque famille

1289
00:49:37,520 --> 00:49:39,430
avait tout ce qu’il lui fallait.

1290
00:49:39,440 --> 00:49:40,870


1291
00:49:40,880 --> 00:49:42,950
>> Mais pour que le capitaliste reste rentable,

1292
00:49:42,960 --> 00:49:45,109
il fallait te convaincre que ce que tu avais

1293
00:49:45,119 --> 00:49:47,349
n’était pas suffisant. Soit en rendant les choses

1294
00:49:47,359 --> 00:49:49,510
obsolètes — la mode, les voitures —,

1295
00:49:49,520 --> 00:49:51,829
soit en te persuadant qu’il te manque

1296
00:49:51,839 --> 00:49:53,990
toujours quelque chose pour être accompli.

1297
00:49:54,000 --> 00:49:56,549
—

1298
00:49:56,559 --> 00:49:58,230
Sans ces choses, tu ne l’es pas. Et cette

1299
00:49:58,240 --> 00:50:01,190
invention du “plus” nous a menés

1300
00:50:01,200 --> 00:50:04,150
jusqu’à aujourd’hui : une économie fondée

1301
00:50:04,160 --> 00:50:06,950
sur la production

1302
00:50:06,960 --> 00:50:09,190
et la consommation. Regarde :

1303
00:50:09,200 --> 00:50:13,109
62 % du PIB américain,

1304
00:50:13,119 --> 00:50:15,990
c’est de la consommation, pas de la production.

1305
00:50:16,000 --> 00:50:20,790
Or cela suppose que les consommateurs

1306
00:50:20,800 --> 00:50:22,870
aient assez de pouvoir d’achat

1307
00:50:22,880 --> 00:50:24,710
pour acheter. Je pense que

1308
00:50:24,720 --> 00:50:26,950
cette économie tiendra peut‑être

1309
00:50:26,960 --> 00:50:31,349
10, 15, 20 ans…

1310
00:50:31,359 --> 00:50:33,750
mais pas forcément éternellement.

1311
00:50:33,760 --> 00:50:36,230
Pourquoi ? Parce que si, d’un côté,

1312
00:50:36,240 --> 00:50:38,950
le revenu universel remplace le salaire,

1313
00:50:38,960 --> 00:50:41,109
si les gens vivent de revenus pris

1314
00:50:41,119 --> 00:50:44,549
sur les impôts — donc sur les profits

1315
00:50:44,559 --> 00:50:46,790
des entreprises d’IA et de robots —,

1316
00:50:46,800 --> 00:50:49,270
—

1317
00:50:49,280 --> 00:50:51,829
alors la logique capitaliste :

1318
00:50:51,839 --> 00:50:54,870
“le travail doit rapporter”,

1319
00:50:54,880 --> 00:50:56,470
fait qu’on les perçoit comme improductifs.

1320
00:50:56,480 --> 00:50:59,109
Donc on dira : “payons‑les moins,

1321
00:50:59,119 --> 00:51:01,510
voire plus du tout.” Et on obtient

1322
00:51:01,520 --> 00:51:04,069
Elysium :

1323
00:51:04,079 --> 00:51:06,309
un monde où “nous” sommes à l’abri,

1324
00:51:06,319 --> 00:51:09,430
nous avons les machines,

1325
00:51:09,440 --> 00:51:11,910
elles font tout, et les autres

1326
00:51:11,920 --> 00:51:14,069
se débrouillent. Plus de revenu universel.

1327
00:51:14,079 --> 00:51:15,670


1328
00:51:15,680 --> 00:51:18,630
—

1329
00:51:18,640 --> 00:51:22,549
Il faut se rappeler que le revenu universel

1330
00:51:22,559 --> 00:51:27,670
suppose une société très démocratique, bienveillante.

1331
00:51:27,680 --> 00:51:31,990
Le revenu universel, c’est une forme de communisme.

1332
00:51:32,000 --> 00:51:33,990
Pense à l’idéologie, ou au moins

1333
00:51:34,000 --> 00:51:36,230
socialiste : donner à chacun ce dont il a besoin.

1334
00:51:36,240 --> 00:51:38,710
Ce n’est pas l’idéologie du capitalisme

1335
00:51:38,720 --> 00:51:40,230
démocratique occidental.

1336
00:51:40,240 --> 00:51:42,069
—

1337
00:51:42,079 --> 00:51:45,190
Ces transitions-là seraient donc immenses.

1338
00:51:45,200 --> 00:51:47,510


1339
00:51:47,520 --> 00:51:50,390
Et pour qu’elles aient lieu, je pense

1340
00:51:50,400 --> 00:51:52,710
que la bonne approche, quand le coût de production

1341
00:51:52,720 --> 00:51:55,109
de tout devient quasi nul grâce à l’IA

1342
00:51:55,119 --> 00:51:57,990
et aux robots,

1343
00:51:58,000 --> 00:52:00,309
c’est de repenser le modèle. D’autant plus que

1344
00:52:00,319 --> 00:52:02,390
l’énergie, elle aussi, tendra vers zéro

1345
00:52:02,400 --> 00:52:04,470
quand nous saurons mieux la recueillir.

1346
00:52:04,480 --> 00:52:07,510
Alors un scénario possible,

1347
00:52:07,520 --> 00:52:09,589
même probable dans l’utopie IA,

1348
00:52:09,599 --> 00:52:13,349
serait : chacun a accès à tout.

1349
00:52:13,359 --> 00:52:15,910
Chacun peut obtenir ce qu’il veut,

1350
00:52:15,920 --> 00:52:17,750
sans surconsommer ni détruire la planète,

1351
00:52:17,760 --> 00:52:20,230
parce que ça ne coûte rien.

1352
00:52:20,240 --> 00:52:22,950
Comme autrefois, quand nos ancêtres

1353
00:52:22,960 --> 00:52:25,109
chasseurs‑cueilleurs prenaient

1354
00:52:25,119 --> 00:52:27,510
leurs baies dans la nature,

1355
00:52:27,520 --> 00:52:30,470
dans dix ou douze ans

1356
00:52:30,480 --> 00:52:33,750
on pourrait “cueillir” un iPhone dans la nature,

1357
00:52:33,760 --> 00:52:36,950
car il serait créé à partir de l’air,

1358
00:52:36,960 --> 00:52:39,190
grâce à la nanophysique.

1359
00:52:39,200 --> 00:52:41,510
—

1360
00:52:41,520 --> 00:52:44,790
Mais le vrai défi, crois‑le ou non,

1361
00:52:44,800 --> 00:52:46,790
ce n’est pas la technologie, c’est l’état d’esprit :

1362
00:52:46,800 --> 00:52:48,870
pourquoi l’élite te donnerait‑elle cela gratuitement ?

1363
00:52:48,880 --> 00:52:51,829


1364
00:52:51,839 --> 00:52:55,030
Le système évoluera plutôt vers plus de profit :

1365
00:52:55,040 --> 00:52:57,589
“Non ! Gagnons encore plus !

1366
00:52:57,599 --> 00:52:59,829
Soyons de plus grands capitalistes !

1367
00:52:59,839 --> 00:53:02,069
Nourrissons notre ego et notre soif de pouvoir.”

1368
00:53:02,079 --> 00:53:05,670
Et alors oui, on donnera un revenu universel,

1369
00:53:05,680 --> 00:53:08,390
puis trois semaines plus tard moins,

1370
00:53:08,400 --> 00:53:09,430
et ainsi de suite.

1371
00:53:09,440 --> 00:53:10,790
>> Mais il n’y aura pas plein de nouveaux

1372
00:53:10,800 --> 00:53:12,390
métiers ? Car au fil des révolutions —

1373
00:53:12,400 --> 00:53:15,270
industrielles, technologiques —

1374
00:53:15,280 --> 00:53:16,630
—

1375
00:53:16,640 --> 00:53:18,470
on prédisait toujours la disparition

1376
00:53:18,480 --> 00:53:20,309
des emplois,

1377
00:53:20,319 --> 00:53:22,549
>> mais on ne voyait pas encore

1378
00:53:22,559 --> 00:53:24,549
ceux qui allaient être créés.

1379
00:53:24,559 --> 00:53:26,230
>>

1380
00:53:26,240 --> 00:53:27,349
>> Exact. Sauf qu’à ces époques‑là,

1381
00:53:27,359 --> 00:53:30,309
les machines remplaçaient la force humaine.

1382
00:53:30,319 --> 00:53:32,470
—

1383
00:53:32,480 --> 00:53:35,190
Dans peu d’endroits d’Occident aujourd’hui, un ouvrier

1384
00:53:35,200 --> 00:53:39,510
transporte encore des charges à dos d’homme

1385
00:53:39,520 --> 00:53:42,150
et les monte à l’étage : c’est la machine

1386
00:53:42,160 --> 00:53:43,750
qui fait cela. Exact.

1387
00:53:43,760 --> 00:53:44,230
>> Oui.

1388
00:53:44,240 --> 00:53:47,190
>> De même,

1389
00:53:47,200 --> 00:53:49,510
l’IA va remplacer le cerveau humain.

1390
00:53:49,520 --> 00:53:52,390
Et quand l’Occident, dans ses “colonies” virtuelles,

1391
00:53:52,400 --> 00:53:55,510
a externalisé tout son travail

1392
00:53:55,520 --> 00:53:58,069
main‑d’œuvre vers les pays en développement,

1393
00:53:58,079 --> 00:54:01,589
—

1394
00:54:01,599 --> 00:54:03,910
l’Occident disait : “Nous deviendrons

1395
00:54:03,920 --> 00:54:06,309
une économie de services,

1396
00:54:06,319 --> 00:54:08,230
où les gens

1397
00:54:08,240 --> 00:54:10,069
travaillent avec leur intelligence,

1398
00:54:10,079 --> 00:54:12,470
pas avec leurs mains. Laissons

1399
00:54:12,480 --> 00:54:15,190
les Indiens, Chinois, Bengalis,

1400
00:54:15,200 --> 00:54:17,109
Vietnamiens fabriquer, et nous,

1401
00:54:17,119 --> 00:54:18,950
nous ferons des métiers raffinés.”

1402
00:54:18,960 --> 00:54:21,030
Ces “travailleurs du savoir”,

1403
00:54:21,040 --> 00:54:23,589
ce sont ceux qui manipulent l’information,

1404
00:54:23,599 --> 00:54:25,750
écrivent sur un clavier, bougent une souris,

1405
00:54:25,760 --> 00:54:27,910
s’assoient en réunion. Ce que nous produisons

1406
00:54:27,920 --> 00:54:30,870
en Occident, ce sont des mots,

1407
00:54:30,880 --> 00:54:33,430
parfois des designs, mais tout cela

1408
00:54:33,440 --> 00:54:36,150
peut être produit par une IA.

1409
00:54:36,160 --> 00:54:40,470


1410
00:54:44,710 --> 00:54:44,720
Alors, si je te donne demain une IA,

1411
00:54:44,720 --> 00:54:46,790
et un terrain, je dis à l’IA :

1412
00:54:46,800 --> 00:54:48,470
« Voici ma parcelle, voici les

1413
00:54:48,480 --> 00:54:50,230
informations : elle est ici sur Google Maps.

1414
00:54:50,240 --> 00:54:53,190
Fais‑moi une villa bien conçue,

1415
00:54:53,200 --> 00:54:55,510
très lumineuse, trois chambres,

1416
00:54:55,520 --> 00:54:57,670
des salles de bain en marbre blanc”, etc.

1417
00:54:57,680 --> 00:54:59,829
L’IA le fera instantanément.

1418
00:54:59,839 --> 00:55:02,710


1419
00:55:02,720 --> 00:55:04,470
Du coup, iras‑tu encore souvent

1420
00:55:04,480 --> 00:55:09,109
chez un architecte ?

1421
00:55:09,119 --> 00:55:12,390
Que fera‑t‑il ? Les meilleurs

1422
00:55:12,400 --> 00:55:14,230
utiliseront l’IA ou te conseilleront :

1423
00:55:14,240 --> 00:55:16,710
“C’est joli, mais ça ne te

1424
00:55:16,720 --> 00:55:19,030
correspond pas.” Ces métiers-là

1425
00:55:19,040 --> 00:55:21,589
resteront, mais combien ?

1426
00:55:21,599 --> 00:55:23,910


1427
00:55:23,920 --> 00:55:26,150


1428
00:55:26,160 --> 00:55:27,670


1429
00:55:27,680 --> 00:55:29,750
combien survivront ?

1430
00:55:29,760 --> 00:55:32,950
Pendant combien d’années encore penses‑tu

1431
00:55:32,960 --> 00:55:34,790
pouvoir écrire un livre plus intelligent

1432
00:55:34,800 --> 00:55:36,710
qu’une IA ?

1433
00:55:36,720 --> 00:55:40,390
—

1434
00:55:43,030 --> 00:55:43,040
Plus pour longtemps. Je pourrai toujours

1435
00:55:43,040 --> 00:55:44,870
toucher l’humain. Tu ne prendras pas

1436
00:55:44,880 --> 00:55:46,950
dans tes bras une IA comme tu me serres la main,

1437
00:55:46,960 --> 00:55:50,230
mais ce n’est pas un “métier” suffisant

1438
00:55:50,240 --> 00:55:52,789
pour tout le monde.

1439
00:55:52,799 --> 00:55:54,950
Voilà pourquoi je dis cela. Souviens‑toi,

1440
00:55:54,960 --> 00:55:56,870
au début, je t’ai demandé

1441
00:55:56,880 --> 00:55:59,829
de me rappeler de parler des solutions.

1442
00:55:59,839 --> 00:56:02,630
Parce qu’il faut des changements idéologiques

1443
00:56:02,640 --> 00:56:05,589
et des actions concrètes

1444
00:56:05,599 --> 00:56:07,990
à prendre dès maintenant par les gouvernements,

1445
00:56:08,000 --> 00:56:10,069
plutôt que d’attendre la crise — comme pour

1446
00:56:10,079 --> 00:56:11,670
Covid — avant d’agir.

1447
00:56:11,680 --> 00:56:13,990
Il fallait réagir dès le patient zéro,

1448
00:56:14,000 --> 00:56:16,230
pas après.

1449
00:56:16,240 --> 00:56:18,309


1450
00:56:18,319 --> 00:56:21,270


1451
00:56:21,280 --> 00:56:24,630
Ce que je veux dire, c’est qu’il est certain

1452
00:56:24,640 --> 00:56:27,349
que beaucoup d’emplois vont disparaître.

1453
00:56:27,359 --> 00:56:29,190
Certains secteurs perdront

1454
00:56:29,200 --> 00:56:33,990
10, 20, 30, 50 % de leurs effectifs :

1455
00:56:34,000 --> 00:56:36,710
développeurs, graphistes,

1456
00:56:36,720 --> 00:56:40,230
marketeurs, assistants…

1457
00:56:40,240 --> 00:56:43,829
—

1458
00:56:43,839 --> 00:56:46,710
ils perdront leur emploi. Sommes‑nous prêts,

1459
00:56:46,720 --> 00:56:48,630
en tant que société, à l’encaisser ?

1460
00:56:48,640 --> 00:56:50,390
Pouvons‑nous dire à nos gouvernements :

1461
00:56:50,400 --> 00:56:52,870
“Il faut un changement idéologique,

1462
00:56:52,880 --> 00:56:55,349
quelque chose qui s’approche du socialisme,

1463
00:56:55,359 --> 00:56:58,470
voire du communisme” ?

1464
00:56:58,480 --> 00:57:00,950
Et budgétairement ? Au lieu de dépenser

1465
00:57:00,960 --> 00:57:03,510
des milliers de milliards chaque année

1466
00:57:03,520 --> 00:57:06,549
en armes, explosifs et drones tueurs,

1467
00:57:06,559 --> 00:57:08,630
—

1468
00:57:08,640 --> 00:57:11,670
ne pourrait‑on pas réorienter cela ?

1469
00:57:11,680 --> 00:57:14,470
J’ai fait le calcul.

1470
00:57:14,480 --> 00:57:16,470
Les dépenses militaires mondiales

1471
00:57:16,480 --> 00:57:21,109
s’élèvent à environ 2,7 billions $.

1472
00:57:21,119 --> 00:57:24,309
—

1473
00:57:24,319 --> 00:57:25,990
C’est ce qu’on dépense pour se battre.

1474
00:57:26,000 --> 00:57:27,829
>> Oui, en matériel militaire, à faire exploser

1475
00:57:27,839 --> 00:57:30,710
en fumée et en mort.

1476
00:57:30,720 --> 00:57:33,430
Or l’extrême pauvreté mondiale,

1477
00:57:33,440 --> 00:57:35,030
celle des gens sous le seuil,

1478
00:57:35,040 --> 00:57:37,589
pourrait disparaître

1479
00:57:37,599 --> 00:57:39,990
pour seulement 10 à 12 % de ce budget.

1480
00:57:40,000 --> 00:57:42,789
En réaffectant 10 % de nos dépenses militaires,

1481
00:57:42,799 --> 00:57:46,549
on pourrait éradiquer la misère.

1482
00:57:46,559 --> 00:57:48,150
Plus personne ne serait pauvre.

1483
00:57:48,160 --> 00:57:50,150
—

1484
00:57:50,160 --> 00:57:54,230
On pourrait aussi éradiquer la faim

1485
00:57:54,240 --> 00:57:56,870
pour moins de 4 %.

1486
00:57:56,880 --> 00:57:59,589
Et avec encore 10 à 12 %,

1487
00:57:59,599 --> 00:58:03,270
on financerait une santé universelle gratuite

1488
00:58:03,280 --> 00:58:05,270
pour chaque être humain.

1489
00:58:05,280 --> 00:58:08,230
—

1490
00:58:08,240 --> 00:58:11,030
Alors, pourquoi je parle de ça en discutant de l’IA ?

1491
00:58:11,040 --> 00:58:12,630
Parce que c’est une simple décision.

1492
00:58:12,640 --> 00:58:15,910
Si nous cessions de nous battre,

1493
00:58:15,920 --> 00:58:18,069
—

1494
00:58:18,079 --> 00:58:20,230
parce que l’argent n’aura plus le même sens,

1495
00:58:20,240 --> 00:58:22,390
car l’économie de l’argent changera

1496
00:58:22,400 --> 00:58:24,470
—

1497
00:58:24,480 --> 00:58:26,470
parce que le capitalisme lui‑même est à bout,

1498
00:58:26,480 --> 00:58:28,230
il n’y aura plus besoin d’arbitrer le travail,

1499
00:58:28,240 --> 00:58:30,630
car l’IA fera tout.

1500
00:58:30,640 --> 00:58:32,470
—

1501
00:58:32,480 --> 00:58:36,150
Avec seulement les 2,4 billions $

1502
00:58:36,160 --> 00:58:38,789
que nous épargnerions chaque année en armes

1503
00:58:38,799 --> 00:58:42,150
et explosifs, on pourrait assurer

1504
00:58:42,160 --> 00:58:44,870
la santé universelle et supprimer la misère.

1505
00:58:44,880 --> 00:58:46,470
on pourrait même, selon un calcul,

1506
00:58:46,480 --> 00:58:48,789
combattre sérieusement le changement

1507
00:58:48,799 --> 00:58:51,510
climatique avec

1508
00:58:51,520 --> 00:58:53,910
100 % du budget militaire mondial.

1509
00:58:53,920 --> 00:58:55,670
>> Mais je ne suis même pas sûr que ce soit vraiment

1510
00:58:55,680 --> 00:58:57,829
une question d’argent. L’argent n’est qu’une

1511
00:58:57,839 --> 00:58:59,190
mesure du pouvoir, non ?

1512
00:58:59,200 --> 00:59:00,789
>> Exactement. On en imprime à volonté.

1513
00:59:00,799 --> 00:59:02,470
>> Donc, même dans un monde doté d’une

1514
00:59:02,480 --> 00:59:04,710
superintelligence, où l’argent n’est plus un problème,

1515
00:59:04,720 --> 00:59:05,190
—

1516
00:59:05,200 --> 00:59:05,910
>> Exact.

1517
00:59:05,920 --> 00:59:08,069
>> Je crois tout de même

1518
00:59:08,079 --> 00:59:10,789
que la soif de pouvoir restera

1519
00:59:10,799 --> 00:59:12,549
insatiable pour beaucoup. Donc il y aura encore

1520
00:59:12,559 --> 00:59:15,589
des guerres, car tu vois…

1521
00:59:15,599 --> 00:59:17,349
>> Oui, à mon avis, il y en aura.

1522
00:59:17,359 --> 00:59:19,349
>> Le plus fort dira : « Je veux l’IA la plus puissante. »

1523
00:59:19,359 --> 00:59:19,910
>> Je ne veux pas que ma…

1524
00:59:19,920 --> 00:59:21,270
>> Et je ne veux pas, tu sais,

1525
00:59:21,280 --> 00:59:23,510
ce que Henry Kissinger appelait

1526
00:59:23,520 --> 00:59:24,950
les “mangeurs”.

1527
00:59:24,960 --> 00:59:25,910
>> Les “mangeurs” ?

1528
00:59:25,920 --> 00:59:27,990
>> Oui.

1529
00:59:28,000 --> 00:59:29,670
>> Aussi brutal que cela paraisse.

1530
00:59:29,680 --> 00:59:31,270
>> C’est quoi ? Les gens d’en bas,

1531
00:59:31,280 --> 00:59:32,390
ceux des classes sociales

1532
00:59:32,400 --> 00:59:35,190
>> qui ne produisent pas mais consomment.

1533
00:59:35,200 --> 00:59:37,430
Alors, si un Henry Kissinger pilotait tout ça,

1534
00:59:37,440 --> 00:59:40,789
et il y en a tant, il penserait :

1535
00:59:40,799 --> 00:59:44,870
« Pourquoi donc

1536
00:59:44,880 --> 00:59:46,950
nourrir 350 millions d’Américains ?

1537
00:59:46,960 --> 00:59:50,470
Et, plus encore, pourquoi se soucier

1538
00:59:50,480 --> 00:59:53,829
du Bangladesh,

1539
00:59:53,839 --> 00:59:56,069
alors qu’on n’y fabrique plus

1540
00:59:56,079 --> 00:59:59,190
ni textile ni rien ? »

1541
00:59:59,200 --> 01:00:01,349
—

1542
01:00:01,359 --> 01:00:03,510
C’est un peu cynique, mais historiquement,

1543
01:00:03,520 --> 01:00:05,829
si on avait eu des podcasts,

1544
01:00:05,839 --> 01:00:09,109
on aurait mis en garde sans cesse

1545
01:00:09,119 --> 01:00:11,750
contre une dystopie imminente.

1546
01:00:11,760 --> 01:00:13,430
Tu sais, à chaque innovation,

1547
01:00:13,440 --> 01:00:15,030
le même refrain : “On est fichus.”

1548
01:00:15,040 --> 01:00:15,990
Lors d’Internet, du tracteur,

1549
01:00:16,000 --> 01:00:17,990
les gens ont dit : “On est foutus !”

1550
01:00:18,000 --> 01:00:19,270
—

1551
01:00:19,280 --> 01:00:20,230
on ne pourra plus cultiver…

1552
01:00:20,240 --> 01:00:22,150
N’est‑ce pas simplement un de ces moments

1553
01:00:22,160 --> 01:00:23,990
où l’on prédit le pire sans voir au‑delà ?

1554
01:00:24,000 --> 01:00:25,910
—

1555
01:00:25,920 --> 01:00:29,670
> Tu as peut‑être raison.

1556
01:00:29,680 --> 01:00:32,789
Je t’en prie, que j’aie tort !

1557
01:00:32,799 --> 01:00:35,589
Je demande juste : vois‑tu un scénario

1558
01:00:35,599 --> 01:00:37,589
où tout cela tournerait bien ?

1559
01:00:37,599 --> 01:00:39,910
Par exemple, Mustafa Suleyman,

1560
01:00:39,920 --> 01:00:42,710
que tu as reçu ici,

1561
01:00:42,720 --> 01:00:44,470
lui, dans *The Coming Wave*,

1562
01:00:44,480 --> 01:00:44,870
>> Oui.

1563
01:00:44,880 --> 01:00:48,230
>> Il parle de l’“aversion au pessimisme”.

1564
01:00:48,240 --> 01:00:49,910
—

1565
01:00:49,920 --> 01:00:52,789
En gros, nous autres, gens de la tech

1566
01:00:52,799 --> 01:00:54,230
et du business, sommes censés

1567
01:00:54,240 --> 01:00:55,670
monter sur scène et dire :

1568
01:00:55,680 --> 01:00:57,510
« L’avenir sera incroyable !

1569
01:00:57,520 --> 01:00:58,710
Cette technologie va tout améliorer ! »

1570
01:00:58,720 --> 01:01:01,190
—

1571
01:01:01,200 --> 01:01:03,190
J’ai écrit un texte appelé “Les promesses brisées”.

1572
01:01:03,200 --> 01:01:05,910
Combien de fois cela s’est‑il réalisé ?

1573
01:01:05,920 --> 01:01:08,150
—

1574
01:01:08,160 --> 01:01:09,510
Combien de fois ?

1575
01:01:09,520 --> 01:01:11,910
>> Les réseaux sociaux nous ont‑ils “connectés” ?

1576
01:01:11,920 --> 01:01:14,630
Ou nous ont‑ils rendus plus

1577
01:01:14,640 --> 01:01:16,950
seuls ?

1578
01:01:16,960 --> 01:01:19,829
Les téléphones nous ont‑ils fait moins travailler ?

1579
01:01:19,839 --> 01:01:22,549
C’était la promesse, non ?

1580
01:01:22,559 --> 01:01:25,030
Les premières pubs Nokia montraient des soirées.

1581
01:01:25,040 --> 01:01:27,990
C’est ça, ton expérience du mobile ?

1582
01:01:28,000 --> 01:01:31,430
—

1583
01:01:31,440 --> 01:01:34,069
J’en conclus qu’il faut espérer que

1584
01:01:34,079 --> 01:01:35,990
l’humain garde d’autres rôles.

1585
01:01:36,000 --> 01:01:38,470
Et ces rôles ressembleraient

1586
01:01:38,480 --> 01:01:40,470
aux temps des chasseurs‑cueilleurs,

1587
01:01:40,480 --> 01:01:42,309
mais avec bien plus de technologie

1588
01:01:42,319 --> 01:01:44,150
et de sécurité.

1589
01:01:44,160 --> 01:01:46,069
>> D’accord, ça sonne bien.

1590
01:01:46,079 --> 01:01:46,309
>> Oui.

1591
01:01:46,319 --> 01:01:47,990
>> C’est enthousiasmant. Donc je vais

1592
01:01:48,000 --> 01:01:50,470
sortir plus, voir mes amis plus souvent,

1593
01:01:50,480 --> 01:01:51,190
—

1594
01:01:51,200 --> 01:01:52,230
>> À 100 %.

1595
01:01:52,240 --> 01:01:52,870
>> Fantastique !

1596
01:01:52,880 --> 01:01:54,549
>> Et ne rien faire du tout.

1597
01:01:54,559 --> 01:01:55,910
>> Mm, ça, ça l’est moins.

1598
01:01:55,920 --> 01:01:57,910
>> Si, si. Être obligé de ne rien faire,

1599
01:01:57,920 --> 01:01:59,109
pour certains, c’est merveilleux.

1600
01:01:59,119 --> 01:02:00,950
Mais toi et moi, on trouvera

1601
01:02:00,960 --> 01:02:02,789
bien un projet de menuiserie

1602
01:02:02,799 --> 01:02:03,510
pour s’occuper.

1603
01:02:03,520 --> 01:02:04,870
>> Parle pour toi ! Moi, les gens

1604
01:02:04,880 --> 01:02:05,910
écouteront toujours.

1605
01:02:05,920 --> 01:02:07,670
>> D’accord.

1606
01:02:07,680 --> 01:02:10,069
>> Oui, c’est vrai. Les gens continueront

1607
01:02:10,079 --> 01:02:11,109
à écouter.

1608
01:02:11,119 --> 01:02:12,470
>> Tu crois ? Je n’en suis pas sûr. Et tant que…

1609
01:02:12,480 --> 01:02:14,710


1610
01:02:14,720 --> 01:02:15,270
—

1611
01:02:15,280 --> 01:02:17,030
>> Vous, vous écouterez encore ?

1612
01:02:17,040 --> 01:02:17,910
—

1613
01:02:17,920 --> 01:02:20,630
>> Laissons‑les répondre. Je pense que tant que

1614
01:02:20,640 --> 01:02:25,190
tu enrichis leur vie,

1615
01:02:25,200 --> 01:02:27,670
>> mais une IA ne le fera‑t‑elle pas mieux ?

1616
01:02:27,680 --> 01:02:29,430
>> Sans la connexion humaine ?

1617
01:02:29,440 --> 01:02:30,630
>> Mettez‑le en commentaire : écouteriez‑vous

1618
01:02:30,640 --> 01:02:32,630
une IA ou *The Diary of a CEO* ?

1619
01:02:32,640 --> 01:02:33,829
Dites‑le sous la vidéo.

1620
01:02:33,839 --> 01:02:36,230
>> Rappelle‑toi, Steve, aussi brillant sois‑tu,

1621
01:02:36,240 --> 01:02:39,109
il viendra un temps où tu paraîtras

1622
01:02:39,119 --> 01:02:40,549
ridiculement stupide face à l’IA.

1623
01:02:40,559 --> 01:02:43,190
Et moi encore plus.

1624
01:02:43,200 --> 01:02:45,670
—

1625
01:02:45,680 --> 01:02:46,549
>> Oui, oui.

1626
01:02:46,559 --> 01:02:49,910
>> Leur profondeur d’analyse,

1627
01:02:49,920 --> 01:02:52,309
leur richesse… Imagine

1628
01:02:52,319 --> 01:02:55,670
deux super‑intelligences s’unissant

1629
01:02:55,680 --> 01:02:59,349
pour expliquer la théorie des cordes.

1630
01:02:59,359 --> 01:03:01,750
—

1631
01:03:01,760 --> 01:03:03,829
Elles le feront mieux que n’importe quel

1632
01:03:03,839 --> 01:03:06,470
physicien, car elles sauront tout

1633
01:03:06,480 --> 01:03:08,069
de la physique, mais aussi

1634
01:03:08,079 --> 01:03:10,870
des interactions humaines et du langage,

1635
01:03:10,880 --> 01:03:12,789
que les savants n’ont pas.

1636
01:03:12,799 --> 01:03:16,230
Je pense que beaucoup de marketeurs B2B

1637
01:03:16,240 --> 01:03:18,150
font cette erreur : viser la quantité

1638
01:03:18,160 --> 01:03:20,710
plutôt que la qualité. En cherchant

1639
01:03:20,720 --> 01:03:22,710
à plaire à tous au lieu des bons publics,

1640
01:03:22,720 --> 01:03:24,309
on fait du bruit — souvent cher,

1641
01:03:24,319 --> 01:03:26,230
inutile, improductif.

1642
01:03:26,240 --> 01:03:28,230
—

1643
01:03:28,240 --> 01:03:30,470
Je l’ai commise moi‑même dans ma carrière.

1644
01:03:30,480 --> 01:03:31,910
Avant de découvrir LinkedIn Ads,

1645
01:03:31,920 --> 01:03:33,349
—

1646
01:03:33,359 --> 01:03:35,589
où j’ai commencé à faire ma pub

1647
01:03:35,599 --> 01:03:37,430
sur la plateforme de notre sponsor, LinkedIn,

1648
01:03:37,440 --> 01:03:39,029
et là, tout a changé.

1649
01:03:39,039 --> 01:03:40,870
J’attribue ce changement à plusieurs choses,

1650
01:03:40,880 --> 01:03:42,950
—

1651
01:03:42,960 --> 01:03:44,789
notamment que LinkedIn était et reste

1652
01:03:44,799 --> 01:03:46,789
la plateforme des décideurs :

1653
01:03:46,799 --> 01:03:49,109
là où l’on apprend, mais aussi où l’on achète.

1654
01:03:49,119 --> 01:03:51,190
Quand on y fait du marketing, on est

1655
01:03:51,200 --> 01:03:52,710
devant ceux qui peuvent dire “oui”.

1656
01:03:52,720 --> 01:03:54,309
—

1657
01:03:54,319 --> 01:03:56,630
On cible par poste, secteur, taille d’entreprise.

1658
01:03:56,640 --> 01:03:58,390
C’est juste une façon plus fine

1659
01:03:58,400 --> 01:04:00,630
d’utiliser ton budget marketing.

1660
01:04:00,640 --> 01:04:02,309
—

1661
01:04:02,319 --> 01:04:04,549
Et si tu n’as pas essayé, vas‑y.

1662
01:04:04,559 --> 01:04:06,789
Essaye LinkedIn Ads et reçois

1663
01:04:06,799 --> 01:04:09,109
un crédit pub de 100 $.

1664
01:04:09,119 --> 01:04:10,789
Rends‑toi sur

1665
01:04:10,799 --> 01:04:12,950
linkedin.com/diary

1666
01:04:12,960 --> 01:04:15,029
pour en profiter dès maintenant.

1667
01:04:15,039 --> 01:04:17,589
—

1668
01:04:17,599 --> 01:04:18,950
>> J’ai longuement réfléchi

1669
01:04:18,960 --> 01:04:21,270
à cette idée : même le podcast

1670
01:04:21,280 --> 01:04:23,910
pourrait être fait par l’IA.

1671
01:04:23,920 --> 01:04:25,750
J’ai douté, j’ai pesé… Et ma conclusion,

1672
01:04:25,760 --> 01:04:27,270
c’est qu’il existera toujours

1673
01:04:27,280 --> 01:04:29,029
des médias où l’expérience vécue compte.

1674
01:04:29,039 --> 01:04:32,390
—

1675
01:04:32,400 --> 01:04:34,069
Où l’on veut le vécu de quelqu’un.

1676
01:04:34,079 --> 01:04:34,710
>> Tout à fait.

1677
01:04:34,720 --> 01:04:36,789
>> Par exemple, savoir comment la personne qu’on admire

1678
01:04:36,799 --> 01:04:38,710
a vécu son divorce.

1679
01:04:38,720 --> 01:04:40,390
—

1680
01:04:40,400 --> 01:04:42,309
>> Oui, ou comment elle gère sa confrontation à l’IA,

1681
01:04:42,319 --> 01:04:43,270
par exemple.

1682
01:04:43,280 --> 01:04:45,349
>> Exactement. Mais je pense

1683
01:04:45,359 --> 01:04:47,910
que pour l’actualité, par exemple,

1684
01:04:47,920 --> 01:04:49,750
ou des faits bruts,

1685
01:04:49,760 --> 01:04:51,510
l’IA prendra vite le relais.

1686
01:04:51,520 --> 01:04:54,150
Mais même là,

1687
01:04:54,160 --> 01:04:57,029
il restera une part

1688
01:04:57,039 --> 01:04:59,270
de personnalité.

1689
01:04:59,280 --> 01:05:01,430
—

1690
01:05:01,440 --> 01:05:03,190
Et je me questionne. Je ne suis pas du

1691
01:05:03,200 --> 01:05:04,710
tout dans le camp romantique.

1692
01:05:04,720 --> 01:05:07,510
Je cherche juste la vérité,

1693
01:05:07,520 --> 01:05:09,990
même si elle me dessert.

1694
01:05:10,000 --> 01:05:12,230
J’espère que les gens le comprennent.

1695
01:05:12,240 --> 01:05:13,510
—

1696
01:05:13,520 --> 01:05:16,150
D’ailleurs, dans mes boîtes,

1697
01:05:16,160 --> 01:05:17,589
on expérimente déjà ma “disparition”

1698
01:05:17,599 --> 01:05:18,870
via l’IA. Certains le savent.

1699
01:05:18,880 --> 01:05:19,990
—

1700
01:05:20,000 --> 01:05:22,230
>> Parce qu’il y aura un mixe de tout.

1701
01:05:22,240 --> 01:05:24,789
On n’aura pas un monde 100 % IA ni

1702
01:05:24,799 --> 01:05:27,109
100 % humain. Ce sera un mélange.

1703
01:05:27,119 --> 01:05:29,589
On verra des IA excellentes dans certains domaines

1704
01:05:29,599 --> 01:05:31,349
et des humains meilleurs ailleurs.

1705
01:05:31,359 --> 01:05:33,109
—

1706
01:05:33,119 --> 01:05:35,109
>> Mon message, c’est qu’il faut se préparer.

1707
01:05:35,119 --> 01:05:36,950
Il faut anticiper.

1708
01:05:36,960 --> 01:05:38,710
>> Oui. En parlant à nos gouvernements,

1709
01:05:38,720 --> 01:05:40,630
en leur disant : « Écoutez,

1710
01:05:40,640 --> 01:05:42,950
il semble que mon métier — parajuriste,

1711
01:05:42,960 --> 01:05:46,309
analyste financier, graphiste,

1712
01:05:46,319 --> 01:05:49,190
opérateur de centre d’appel — va disparaître. »

1713
01:05:49,200 --> 01:05:52,150
—

1714
01:05:52,160 --> 01:05:54,390
—

1715
01:05:54,400 --> 01:05:56,870
Il semble que la moitié

1716
01:05:56,880 --> 01:05:58,630
de ces emplois le soient déjà.

1717
01:05:58,640 --> 01:05:59,910
>> Tu connais Geoffrey Hinton ?

1718
01:05:59,920 --> 01:06:01,750
>> Oui, Geoffrey. Je l’ai eu dans mon documentaire.

1719
01:06:01,760 --> 01:06:04,069
Je l’adore.

1720
01:06:04,079 --> 01:06:05,510
>> Geoffrey Hinton m’a dit :

1721
01:06:05,520 --> 01:06:06,950
>> « Forme‑toi à la plomberie. »

1722
01:06:06,960 --> 01:06:07,589
>> Sérieusement ?

1723
01:06:07,599 --> 01:06:10,710
>> Oui, il l’a dit sérieusement.

1724
01:06:10,720 --> 01:06:13,029
>> Je pensais qu’il plaisantait. Pas du tout.

1725
01:06:13,039 --> 01:06:15,109
>> Je lui ai redemandé, il m’a regardé droit dans les yeux

1726
01:06:15,119 --> 01:06:17,190
et m’a dit : “Tu devrais apprendre la plomberie.”

1727
01:06:17,200 --> 01:06:18,230
—

1728
01:06:18,240 --> 01:06:23,109
>> Tout à fait. C’est drôle : les machines

1729
01:06:23,119 --> 01:06:25,270
ont remplacé la force, mais on avait encore des cols bleus.

1730
01:06:25,280 --> 01:06:28,150
Puis sont venus les emplois de “cols blancs”,

1731
01:06:28,160 --> 01:06:30,230
les travailleurs de l’information.

1732
01:06:30,240 --> 01:06:30,789
—

1733
01:06:30,799 --> 01:06:32,150
>> C’est quoi, “emplois raffinés” ?

1734
01:06:32,160 --> 01:06:33,750
>> Tu sais, ceux où tu ne portes rien de lourd,

1735
01:06:33,760 --> 01:06:36,309
où tu ne fais pas d’effort physique.

1736
01:06:36,319 --> 01:06:38,390
Tu t’assois dans un bureau, tu fais des réunions

1737
01:06:38,400 --> 01:06:40,549
et tu parles beaucoup pour pas grand‑chose :

1738
01:06:40,559 --> 01:06:42,309
c’est ton boulot.

1739
01:06:42,319 --> 01:06:45,829
Et ces emplois‑là, ironiquement,

1740
01:06:45,839 --> 01:06:48,549
avec la robotique encore balbutiante,

1741
01:06:48,559 --> 01:06:51,190
tiennent encore un peu.

1742
01:06:51,200 --> 01:06:52,630
À mon avis, cela vient de l’obstination

1743
01:06:52,640 --> 01:06:55,270
du monde de la robotique

1744
01:06:55,280 --> 01:06:57,910
à vouloir faire des humanoïdes.

1745
01:06:57,920 --> 01:06:58,309
>> Mhm.

1746
01:06:58,319 --> 01:06:59,990
>> Car reproduire un geste humain fluide

1747
01:07:00,000 --> 01:07:03,190
et rapide demande énormément.

1748
01:07:03,200 --> 01:07:05,990
On pourrait faire bien plus de robots

1749
01:07:06,000 --> 01:07:07,990
non humains, comme les voitures autonomes

1750
01:07:08,000 --> 01:07:09,510
en Californie, qui remplacent déjà

1751
01:07:09,520 --> 01:07:12,549
les chauffeurs. Mais bref, ça prend du temps.

1752
01:07:12,559 --> 01:07:15,589
—

1753
01:07:15,599 --> 01:07:17,910
Donc l’automatisation physique est

1754
01:07:17,920 --> 01:07:21,349
retardée : elle prendra encore

1755
01:07:21,359 --> 01:07:23,829
4 à 5 ans pour atteindre

1756
01:07:23,839 --> 01:07:26,390
le niveau actuel de l’IA dans les tâches mentales.

1757
01:07:26,400 --> 01:07:30,470


1758
01:07:30,480 --> 01:07:33,510
Et une fois prête, il faudra longtemps

1759
01:07:33,520 --> 01:07:35,430
pour fabriquer assez de robots

1760
01:07:35,440 --> 01:07:37,750
pour tout remplacer.

1761
01:07:37,760 --> 01:07:40,150
Ce cycle prendra du temps ; les cols bleus

1762
01:07:40,160 --> 01:07:41,430
dureront donc un peu plus.

1763
01:07:41,440 --> 01:07:43,589
>> Donc, je devrais passer au travail manuel et fermer le bureau ?

1764
01:07:43,599 --> 01:07:44,870
—

1765
01:07:44,880 --> 01:07:47,190
>> Toi, tu n’es pas le problème.

1766
01:07:47,200 --> 01:07:47,990
>> Tant mieux.

1767
01:07:48,000 --> 01:07:50,150
>> Disons que beaucoup d’autres,

1768
01:07:50,160 --> 01:07:52,870
plus vulnérables qu’un CEO,

1769
01:07:52,880 --> 01:07:55,430
vont être touchés : agents de voyage,

1770
01:07:55,440 --> 01:07:57,029
assistants…

1771
01:07:57,039 --> 01:07:59,750
et même sans être remplacés,

1772
01:07:59,760 --> 01:08:02,150
verront moins de missions arriver.

1773
01:08:02,160 --> 01:08:09,510
—

1774
01:08:13,190 --> 01:08:13,200
Et quelqu’un, dans les ministères du Travail

1775
01:08:13,200 --> 01:08:15,270
partout, doit se demander :

1776
01:08:15,280 --> 01:08:17,110
« Que faisons‑nous de ça ?

1777
01:08:17,120 --> 01:08:20,390
Si tous les chauffeurs sont remplacés

1778
01:08:20,400 --> 01:08:24,550
par des voitures autonomes en Californie,

1779
01:08:24,560 --> 01:08:27,829
ne faut‑il pas déjà y réfléchir ? »

1780
01:08:27,839 --> 01:08:30,229
—

1781
01:08:30,239 --> 01:08:33,110
La trajectoire rend cela probable.

1782
01:08:33,120 --> 01:08:35,030
—

1783
01:08:35,040 --> 01:08:36,390
Je reviens à l’argument populaire :

1784
01:08:36,400 --> 01:08:39,269
“Oui, mais il y aura de nouveaux emplois !”

1785
01:08:39,279 --> 01:08:41,110
—

1786
01:08:41,120 --> 01:08:43,110
>> Et j’ai répondu : à part les métiers de lien humain,

1787
01:08:43,120 --> 01:08:45,669
cites‑en un.

1788
01:08:45,679 --> 01:08:50,229
>> J’ai trois assistants, d’accord ?

1789
01:08:50,239 --> 01:08:53,829
Sophie, Liam B. À court terme,

1790
01:08:53,839 --> 01:08:55,990
avec les agents IA,

1791
01:08:56,000 --> 01:08:58,390
je n’aurai peut‑être plus besoin d’eux

1792
01:08:58,400 --> 01:08:59,910
pour réserver un vol,

1793
01:08:59,920 --> 01:09:01,829
ni pour l’agenda.

1794
01:09:01,839 --> 01:09:03,669
J’expérimente même un outil d’IA

1795
01:09:03,679 --> 01:09:05,510
d’un copain : quand on veut fixer

1796
01:09:05,520 --> 01:09:07,430
un rendez‑vous, je la mets en copie

1797
01:09:07,440 --> 01:09:08,870
et elle compare nos calendriers

1798
01:09:08,880 --> 01:09:10,470
puis planifie pour nous.

1799
01:09:10,480 --> 01:09:11,829
Donc il n’y aura plus besoin

1800
01:09:11,839 --> 01:09:13,590
de planifier. Mais mon chien est malade,

1801
01:09:13,600 --> 01:09:15,510
et en partant ce matin je me disais :

1802
01:09:15,520 --> 01:09:17,349
« Mince, il va vraiment mal. »

1803
01:09:17,359 --> 01:09:18,709
—

1804
01:09:18,719 --> 01:09:19,990
Je l’ai emmené mille fois chez le véto.

1805
01:09:20,000 --> 01:09:21,269
J’ai besoin de quelqu’un pour s’en occuper

1806
01:09:21,279 --> 01:09:23,269
et comprendre ce qu’il a.

1807
01:09:23,279 --> 01:09:25,110
Ces tâches de soin, oui, resteront.

1808
01:09:25,120 --> 01:09:28,309
—

1809
01:09:28,319 --> 01:09:30,550
Je suis d’accord.

1810
01:09:30,560 --> 01:09:33,590
>> Et je ne veux pas paraître dur,

1811
01:09:33,600 --> 01:09:34,470
mais mes assistants garderont un emploi,

1812
01:09:34,480 --> 01:09:35,990
autrement : leurs missions seront autres.

1813
01:09:36,000 --> 01:09:39,910
—

1814
01:09:39,920 --> 01:09:41,510
Je leur demanderai autre chose.

1815
01:09:41,520 --> 01:09:42,309
—

1816
01:09:42,319 --> 01:09:44,309
>> Exactement. C’est cela que tout le monde

1817
01:09:44,319 --> 01:09:45,910
doit anticiper : une part de nos tâches

1818
01:09:45,920 --> 01:09:48,870
passera aux machines.

1819
01:09:48,880 --> 01:09:50,390
—

1820
01:09:50,400 --> 01:09:52,789
Peu importe ton métier,

1821
01:09:52,799 --> 01:09:55,830
une partie sera transférée.

1822
01:09:55,840 --> 01:09:57,830
Je vais préciser : il y aura deux étapes

1823
01:09:57,840 --> 01:10:01,030
pre‑IA :

1824
01:10:01,040 --> 01:10:02,550
l’ère de “l’intelligence augmentée”

1825
01:10:02,560 --> 01:10:05,990
et celle de “la maîtrise machine”.

1826
01:10:06,000 --> 01:10:07,590
—

1827
01:10:07,600 --> 01:10:10,390
D’abord, l’humain sera aidé par l’IA、

1828
01:10:10,400 --> 01:10:12,310
puis, à la seconde, l’IA fera tout seule.

1829
01:10:12,320 --> 01:10:14,470
—

1830
01:10:14,480 --> 01:10:17,510
À ce stade, plus d’humain dans la boucle.

1831
01:10:17,520 --> 01:10:21,110
Dans l’ère de l’intelligence augmentée,

1832
01:10:21,120 --> 01:10:23,750
tes assistants s’appuieront sur une IA

1833
01:10:23,760 --> 01:10:26,709
pour être plus efficaces

1834
01:10:26,719 --> 01:10:27,189
>> Oui.

1835
01:10:27,199 --> 01:10:31,110
>> Ou pour réduire

1836
01:10:31,120 --> 01:10:34,630
le nombre de tâches à effectuer.

1837
01:10:34,640 --> 01:10:37,430
Mais plus les tâches diminuent,

1838
01:10:37,440 --> 01:10:39,910
plus ils auront du temps et la capacité

1839
01:10:39,920 --> 01:10:41,669
pour des missions comme

1840
01:10:41,679 --> 01:10:44,229
prendre soin de ton chien,

1841
01:10:44,239 --> 01:10:46,550
accueillir les invités, créer du lien.

1842
01:10:46,560 --> 01:10:48,630
—

1843
01:10:48,640 --> 01:10:49,189
—

1844
01:10:49,199 --> 01:10:50,790
>> Oui.

1845
01:10:50,800 --> 01:10:53,270
>> Des liens humains.

1846
01:10:53,280 --> 01:10:56,070
Mais crois‑tu avoir encore besoin de trois personnes,

1847
01:10:56,080 --> 01:10:58,630
maintenant qu’une partie des tâches

1848
01:10:58,640 --> 01:11:01,030
est confiée à l’IA ? Peut‑être deux ?

1849
01:11:01,040 --> 01:11:03,270
On le voit déjà aux centres d’appels :

1850
01:11:03,280 --> 01:11:06,070
on ne licencie pas tout le monde,

1851
01:11:06,080 --> 01:11:09,270
mais on confie le premier contact à l’IA.

1852
01:11:09,280 --> 01:11:10,950
—

1853
01:11:10,960 --> 01:11:13,830
Au lieu de 2 000 agents,

1854
01:11:13,840 --> 01:11:16,709
il en faut 1 800. Je dis un chiffre au hasard,

1855
01:11:16,719 --> 01:11:18,229
mais la société doit penser aux 200 autres.

1856
01:11:18,239 --> 01:11:21,350
—

1857
01:11:21,360 --> 01:11:24,229


1858
01:11:24,239 --> 01:11:25,189


1859
01:11:25,199 --> 01:11:26,790
>> Et tu dis qu’ils ne trouveront pas

1860
01:11:26,800 --> 01:11:29,030
d’autres postes ailleurs ?

1861
01:11:29,040 --> 01:11:30,709
>> Je dis que je ne sais pas lesquels.

1862
01:11:30,719 --> 01:11:31,750
—

1863
01:11:31,760 --> 01:11:33,430
>> Eh bien, on devrait tous devenir

1864
01:11:33,440 --> 01:11:35,110
musiciens, auteurs,

1865
01:11:35,120 --> 01:11:36,550
artistes, comédiens,

1866
01:11:36,560 --> 01:11:37,669
animateurs… Ce sont des métiers

1867
01:11:37,679 --> 01:11:39,510
qui resteront.

1868
01:11:39,520 --> 01:11:41,590
—

1869
01:11:41,600 --> 01:11:43,350
Et plombiers, pour les 5 à 10 ans à venir.

1870
01:11:43,360 --> 01:11:46,950
C’est top. Mais encore faut‑il

1871
01:11:46,960 --> 01:11:50,390
que la société s’adapte.

1872
01:11:50,400 --> 01:11:53,110
Et elle n’en parle pas.

1873
01:11:53,120 --> 01:11:54,790
J’ai eu cette belle discussion

1874
01:11:54,800 --> 01:11:56,709
avec mes amis Peter Dendez

1875
01:11:56,719 --> 01:11:59,590
et d’autres : ils disaient :

1876
01:11:59,600 --> 01:12:01,189
“Oh, les Américains sont résilients,

1877
01:12:01,199 --> 01:12:02,870
ils deviendront entrepreneurs.”

1878
01:12:02,880 --> 01:12:05,030
—

1879
01:12:05,040 --> 01:12:06,709
Sérieusement ? Vous croyez qu’un routier

1880
01:12:06,719 --> 01:12:09,189
remplacé par un camion autonome

1881
01:12:09,199 --> 01:12:11,750
va devenir entrepreneur ?

1882
01:12:11,760 --> 01:12:14,950
Soyez réalistes.

1883
01:12:14,960 --> 01:12:18,149
Mettez‑vous à leur place !

1884
01:12:18,159 --> 01:12:19,910
Vous pensez qu’une mère célibataire

1885
01:12:19,920 --> 01:12:24,310
avec trois boulots…

1886
01:12:30,550 --> 01:12:30,560
Je ne dis pas que ce sera une dystopie,

1887
01:12:30,560 --> 01:12:32,709
ça ne le deviendra que si l’humanité s’y prend mal.

1888
01:12:32,719 --> 01:12:35,510
Car cela pourrait tout aussi bien

1889
01:12:35,520 --> 01:12:37,669
devenir l’utopie : celle où cette femme

1890
01:12:37,679 --> 01:12:40,550
n’a plus besoin de trois emplois.

1891
01:12:40,560 --> 01:12:43,110
Si notre société était juste,

1892
01:12:43,120 --> 01:12:45,830
cette mère n’aurait jamais dû

1893
01:12:45,840 --> 01:12:48,550
cumuler trois métiers.

1894
01:12:48,560 --> 01:12:51,189
Mais le problème, c’est notre logique capitaliste,

1895
01:12:51,199 --> 01:12:53,669
fondée sur l’arbitrage du travail.

1896
01:12:53,679 --> 01:12:56,310
On se moque de ce qu’elle endure.

1897
01:12:56,320 --> 01:12:57,990
Si tu es bienveillant, tu diras

1898
01:12:58,000 --> 01:13:00,229
qu’elle mérite mieux, que tu as eu de la chance.

1899
01:13:00,239 --> 01:13:01,590
—

1900
01:13:01,600 --> 01:13:03,830
Mais si tu es cynique,

1901
01:13:03,840 --> 01:13:05,510
tu diras : “C’est une consommatrice,

1902
01:13:05,520 --> 01:13:08,229
pas une productrice. Moi, je suis

1903
01:13:08,239 --> 01:13:10,550
un entrepreneur. Je travaille dur,

1904
01:13:10,560 --> 01:13:13,270
je gagne de l’argent. Tant pis pour elle.”

1905
01:13:13,280 --> 01:13:15,510
Et on se désintéresse d’eux.

1906
01:13:15,520 --> 01:13:17,590
>> Au fond, ne demande‑t‑on pas

1907
01:13:17,600 --> 01:13:19,990
quelque chose de contraire à la nature humaine ?

1908
01:13:20,000 --> 01:13:22,870
Je veux dire : si nous sommes assis ici

1909
01:13:22,880 --> 01:13:26,149
dans mon bureau —

1910
01:13:26,159 --> 01:13:27,910
au troisième ou quatrième étage

1911
01:13:27,920 --> 01:13:29,510
d’un immeuble du centre de Londres,

1912
01:13:29,520 --> 01:13:32,149
un grand bureau de 2 300 m², avec la lumière,

1913
01:13:32,159 --> 01:13:34,229
le Wi‑Fi, des équipes IA au rez‑de‑chaussée —

1914
01:13:34,239 --> 01:13:37,510
—

1915
01:13:37,520 --> 01:13:39,110
si tout cela existe,

1916
01:13:39,120 --> 01:13:41,750
c’est parce que nos ancêtres construisaient,

1917
01:13:41,760 --> 01:13:44,950
accomplissaient, étendaient ;

1918
01:13:44,960 --> 01:13:47,189
c’était dans leur ADN.

1919
01:13:47,199 --> 01:13:48,630
—

1920
01:13:48,640 --> 01:13:51,189
Il y avait chez eux ce besoin d’expansion

1921
01:13:51,199 --> 01:13:54,470
et de conquête. Et nous le portons.

1922
01:13:54,480 --> 01:13:55,990
C’est notre héritage, voilà pourquoi

1923
01:13:56,000 --> 01:13:57,669
nous sommes là, dans ces tours.

1924
01:13:57,679 --> 01:13:59,430
—

1925
01:13:59,440 --> 01:14:01,110
Il y a du vrai, oui. Mais ce n’est pas

1926
01:14:01,120 --> 01:14:02,709
grâce à tes ancêtres à toi.

1927
01:14:02,719 --> 01:14:03,030
>> Ah non ?

1928
01:14:03,040 --> 01:14:04,310
>> Alors, quoi ?

1929
01:14:04,320 --> 01:14:06,709
>> C’est le lavage de cerveau des médias.

1930
01:14:06,719 --> 01:14:07,189
>> Vraiment ?

1931
01:14:07,199 --> 01:14:07,990
>> 100 %.

1932
01:14:08,000 --> 01:14:09,990
>> Pourtant, avant les médias,

1933
01:14:10,000 --> 01:14:10,470
nos ancêtres Homo sapiens

1934
01:14:10,480 --> 01:14:10,950
>> Mhm…

1935
01:14:10,960 --> 01:14:12,630
>> ont dominé parce qu’ils savaient

1936
01:14:12,640 --> 01:14:14,149
se regrouper, communiquer,

1937
01:14:14,159 --> 01:14:15,590
et dominer d’autres tribus.

1938
01:14:15,600 --> 01:14:16,950
>> En s’unissant, en coopérant.

1939
01:14:16,960 --> 01:14:18,950
Ils ont conquis toutes les autres espèces

1940
01:14:18,960 --> 01:14:22,630
pré‑sapiens, quelles qu’elles soient.

1941
01:14:22,640 --> 01:14:24,310
—

1942
01:14:24,320 --> 01:14:26,950
>> Oui. Donc, si l’homme a survécu,

1943
01:14:26,960 --> 01:14:28,709
c’est parce qu’il savait créer une tribu.

1944
01:14:28,719 --> 01:14:30,790
Pas à cause de son intelligence.

1945
01:14:30,800 --> 01:14:32,390
Je plaisante souvent : Einstein

1946
01:14:32,400 --> 01:14:34,630
se serait fait dévorer dans la jungle

1947
01:14:34,640 --> 01:14:36,310
en deux minutes.

1948
01:14:36,320 --> 01:14:38,070
>> Oui ! Et s’il a survécu, c’est que

1949
01:14:38,080 --> 01:14:39,990
quelqu’un de costaud

1950
01:14:40,000 --> 01:14:43,030
s’est associé à lui pour le protéger

1951
01:14:43,040 --> 01:14:44,950
pendant qu’il inventait la relativité.

1952
01:14:44,960 --> 01:14:49,189
—

1953
01:14:49,199 --> 01:14:51,270
Mais plus loin encore, la vie est un jeu étrange :

1954
01:14:51,280 --> 01:14:54,070
elle donne, retire, redonne, retire.

1955
01:14:54,080 --> 01:14:56,149
—

1956
01:14:56,159 --> 01:14:58,950
—

1957
01:14:58,960 --> 01:15:01,990
Et à chaque fois qu’elle retire,

1958
01:15:02,000 --> 01:15:05,669
certains disent :

1959
01:15:05,679 --> 01:15:07,750
« Allons piller le voisin ! »

1960
01:15:07,760 --> 01:15:10,470
On attaque l’autre tribu.

1961
01:15:10,480 --> 01:15:13,189
Et d’autres disent :

1962
01:15:13,199 --> 01:15:15,750
« Je suis puissant, tant pis pour vous. »

1963
01:15:15,760 --> 01:15:18,229
—

1964
01:15:18,239 --> 01:15:21,510
Je serai le chef maintenant.

1965
01:15:21,520 --> 01:15:24,950
—

1966
01:15:24,960 --> 01:15:28,070
C’est intéressant que tu voies cela

1967
01:15:28,080 --> 01:15:29,830
comme la condition humaine, car,

1968
01:15:29,840 --> 01:15:31,510
si tu regardes la majorité des gens,

1969
01:15:31,520 --> 01:15:34,229
qu'est-ce qu'ils veulent ?

1970
01:15:34,239 --> 01:15:36,070
Honnêtement, ils veulent étreindre leurs enfants.

1971
01:15:36,080 --> 01:15:38,550
Ils veulent bien manger. Faire l’amour.

1972
01:15:38,560 --> 01:15:41,990
Ils veulent de l’amour, du lien,

1973
01:15:42,000 --> 01:15:44,790
mais pas, comme toi et moi,

1974
01:15:44,800 --> 01:15:47,830
dans une quête dévorante.

1975
01:15:47,840 --> 01:15:49,830
Je le dis en riant : j’ai consacré ma vie

1976
01:15:49,840 --> 01:15:52,229
à prévenir des dangers de l’IA ou à comprendre l’amour.

1977
01:15:52,239 --> 01:15:55,030
C’est fou, n’est‑ce pas ?

1978
01:15:55,040 --> 01:15:59,110
C’est dingue.

1979
01:15:59,120 --> 01:16:01,110
Et je te le dis franchement —

1980
01:16:01,120 --> 01:16:03,750
tu as rencontré Hannah, ma femme —

1981
01:16:03,760 --> 01:16:07,030
la grande question de mon année, c’est :

1982
01:16:07,040 --> 01:16:09,110
de quoi suis‑je réellement responsable ?

1983
01:16:09,120 --> 01:16:12,070
Que dois‑je faire sans charge morale ?

1984
01:16:12,080 --> 01:16:13,750
Que dois‑je faire juste parce que je peux ?

1985
01:16:13,760 --> 01:16:16,310
Et que dois‑je ignorer complètement ?

1986
01:16:16,320 --> 01:16:19,510
—

1987
01:16:19,520 --> 01:16:21,910
La réalité, c’est que la plupart des humains

1988
01:16:21,920 --> 01:16:24,310
veulent juste serrer leurs proches.

1989
01:16:24,320 --> 01:16:26,070
Et si on leur offrait cela

1990
01:16:26,080 --> 01:16:29,350
sans devoir trimer 60 heures

1991
01:16:29,360 --> 01:16:33,110
par semaine, ils accepteraient sans hésiter.

1992
01:16:33,120 --> 01:16:36,550
—

1993
01:16:36,560 --> 01:16:39,189
Toi et moi, on dirait : “Ce serait ennuyeux.”

1994
01:16:39,199 --> 01:16:41,270
Pour eux, ce serait le bonheur.

1995
01:16:41,280 --> 01:16:43,030
Va en Amérique latine :

1996
01:16:43,040 --> 01:16:44,630
—

1997
01:16:44,640 --> 01:16:46,310
va voir les gens là‑bas,

1998
01:16:46,320 --> 01:16:48,950
qui travaillent juste assez pour manger,

1999
01:16:48,960 --> 01:16:50,630
et dansent toute la nuit.

2000
01:16:50,640 --> 01:16:53,110
Va en Afrique.

2001
01:16:53,120 --> 01:16:55,350
Où les gens sont littéralement assis sur

2002
01:16:55,360 --> 01:16:59,110
les trottoirs, dans la rue, et

2003
01:16:59,120 --> 01:17:01,830
pleins de rires et de joie.

2004
01:17:01,840 --> 01:17:06,870
On nous a menti, à nous, la majorité crédule,

2005
01:17:06,880 --> 01:17:09,430
les supporters enthousiastes. On nous a fait croire

2006
01:17:09,440 --> 01:17:12,390
qu’il fallait devenir un rouage

2007
01:17:12,400 --> 01:17:14,790
dans ce système. Mais si ce système n’existait pas,

2008
01:17:14,800 --> 01:17:17,669
personne, aucun de nous,

2009
01:17:17,679 --> 01:17:19,030
ne se lèverait le matin en disant :

2010
01:17:19,040 --> 01:17:21,750
« Tiens, j’ai envie de créer ça ! »

2011
01:17:21,760 --> 01:17:25,189
Non, pas du tout.

2012
01:17:25,199 --> 01:17:27,990
Tu l’as souvent souligné aujourd’hui.

2013
01:17:28,000 --> 01:17:30,229
Nous n’avons pas besoin de ça. Tu sais, la plupart de ceux

2014
01:17:30,239 --> 01:17:32,070
qui construisent ces choses n’ont pas besoin d’argent.

2015
01:17:32,080 --> 01:17:34,149
—

2016
01:17:34,159 --> 01:17:36,149
Alors, pourquoi le font‑ils ? Parce que

2017
01:17:36,159 --> 01:17:38,709
Homo sapiens était un compétiteur incroyable.

2018
01:17:38,719 --> 01:17:41,189
Il a surpassé les autres espèces humaines.

2019
01:17:41,199 --> 01:17:43,270
—

2020
01:17:43,280 --> 01:17:45,350
Ce que je veux dire, c’est que cette compétition

2021
01:17:45,360 --> 01:17:48,310
n’est‑elle pas inscrite dans nos gènes ?

2022
01:17:48,320 --> 01:17:50,709
Et donc, n’est‑ce pas illusoire

2023
01:17:50,719 --> 01:17:51,910
de penser qu’on pourrait un jour

2024
01:17:51,920 --> 01:17:55,990
s’arrêter et dire : « C’est bon, on a assez »,

2025
01:17:56,000 --> 01:17:58,470
et se concentrer sur le plaisir ?

2026
01:17:58,480 --> 01:18:00,790
—

2027
01:18:00,800 --> 01:18:03,830
J’appelle cela, dans mon travail,

2028
01:18:03,840 --> 01:18:08,070
le spectre MAP‑MAD : la prospérité

2029
01:18:08,080 --> 01:18:10,390
assurée mutuellement,

2030
01:18:10,400 --> 01:18:14,229
contre la destruction assurée mutuellement.

2031
01:18:14,239 --> 01:18:16,470
Et il faut vraiment commencer à y réfléchir,

2032
01:18:16,480 --> 01:18:17,990
car selon moi, nous avons aujourd’hui

2033
01:18:18,000 --> 01:18:20,950
le potentiel d’offrir à chacun

2034
01:18:20,960 --> 01:18:24,950
une belle vie. Toi et moi vivons mieux

2035
01:18:24,960 --> 01:18:27,510
que la reine d’Angleterre il y a 100 ans.

2036
01:18:27,520 --> 01:18:30,229
C’est vrai, tout le monde le sait.

2037
01:18:30,239 --> 01:18:31,030
—

2038
01:18:31,040 --> 01:18:34,070
>> Et pourtant, cette qualité de vie ne suffit pas.

2039
01:18:34,080 --> 01:18:36,310
—

2040
01:18:36,320 --> 01:18:39,189
>> La vérité, c’est comme quand tu entres

2041
01:18:39,199 --> 01:18:41,910
dans un magasin d’électronique,

2042
01:18:41,920 --> 01:18:44,149
qu’il y a soixante télés, et tu compares,

2043
01:18:44,159 --> 01:18:45,910
en disant “celle‑ci est meilleure que celle‑là”.

2044
01:18:45,920 --> 01:18:47,910
Mais en réalité, n’importe laquelle chez toi

2045
01:18:47,920 --> 01:18:50,149
t’offrira une image bien meilleure

2046
01:18:50,159 --> 01:18:52,070
que ce dont tu as besoin.

2047
01:18:52,080 --> 01:18:53,990
C’est exactement notre vie aujourd’hui.

2048
01:18:54,000 --> 01:18:55,750
C’est ça, la vérité du monde actuel :

2049
01:18:55,760 --> 01:18:57,669
il ne manque presque plus rien.

2050
01:18:57,679 --> 01:18:59,350
—

2051
01:18:59,360 --> 01:18:59,830
>> Non.

2052
01:18:59,840 --> 01:19:02,229
>> Et quand, tu sais, les Californiens disent

2053
01:19:02,239 --> 01:19:04,470
« L’IA va augmenter la productivité, résoudre tout ça ! »,

2054
01:19:04,480 --> 01:19:06,550
—

2055
01:19:06,560 --> 01:19:08,950
Personne ne t’a rien demandé !

2056
01:19:08,960 --> 01:19:10,870
Je ne t’ai jamais élu pour décider

2057
01:19:10,880 --> 01:19:13,590
de me faire répondre par une machine

2058
01:19:13,600 --> 01:19:15,590
dans un centre d’appels.

2059
01:19:15,600 --> 01:19:18,229
Ce n’est pas ce que j’ai voulu.

2060
01:19:18,239 --> 01:19:21,270
Et, comme ces gens non élus

2061
01:19:21,280 --> 01:19:22,790
prennent toutes les décisions,

2062
01:19:22,800 --> 01:19:24,709
ils nous les “vendent” par les médias.

2063
01:19:24,719 --> 01:19:28,229
—

2064
01:19:28,239 --> 01:19:31,830
Ce ne sont que des mensonges, d’un bout à l’autre.

2065
01:19:31,840 --> 01:19:33,910
Rien de cela n’est ce dont on a besoin.

2066
01:19:33,920 --> 01:19:36,310
Et, tu me connais, c’est amusant :

2067
01:19:36,320 --> 01:19:38,310
cette année, j’ai échoué.

2068
01:19:38,320 --> 01:19:39,750
Je n’ai pas pu faire ma retraite

2069
01:19:39,760 --> 01:19:43,030
de 40 jours dans la nature.

2070
01:19:43,040 --> 01:19:47,350
Mais tu sais quoi ? Même quand j’y vais,

2071
01:19:47,360 --> 01:19:51,830
je suis tellement conditionné que,

2072
01:19:51,840 --> 01:19:54,310
s’il n’y a pas un Waitrose pas loin,

2073
01:19:54,320 --> 01:19:57,110
je n’arrive pas à me détendre.

2074
01:19:57,120 --> 01:19:59,189
Je suis en pleine nature, et pourtant il me faut

2075
01:19:59,199 --> 01:20:01,270
pouvoir rouler 20 minutes pour des galettes de riz !

2076
01:20:01,280 --> 01:20:06,550
Qui m’a appris que c’était ça, bien vivre ?

2077
01:20:06,560 --> 01:20:09,030
Tous les médias, tous les messages

2078
01:20:09,040 --> 01:20:11,750
autour de moi

2079
01:20:11,760 --> 01:20:14,790
me répètent sans cesse

2080
01:20:14,800 --> 01:20:17,430
que “Vivre, c’est ça”. Alors que si la vie offrait

2081
01:20:17,440 --> 01:20:19,270
tout ce qu’il faut,

2082
01:20:19,280 --> 01:20:21,830
et si j’avais tout ce dont j’ai besoin ?

2083
01:20:21,840 --> 01:20:26,470
Je pourrais lire, faire de l’artisanat,

2084
01:20:26,480 --> 01:20:28,709
de la mécanique, restaurer

2085
01:20:28,719 --> 01:20:31,590
des voitures anciennes, non pour l’argent

2086
01:20:31,600 --> 01:20:33,189
mais pour le plaisir. Je pourrais créer des IA

2087
01:20:33,199 --> 01:20:35,350
pour aider les gens dans leurs relations,

2088
01:20:35,360 --> 01:20:38,709
et les offrir gratuitement.

2089
01:20:38,719 --> 01:20:40,709
—

2090
01:20:40,719 --> 01:20:42,950
—

2091
01:20:42,960 --> 01:20:45,590
Et si on faisait cela,

2092
01:20:45,600 --> 01:20:48,630
insisterais‑tu encore pour gagner de l’argent ?

2093
01:20:48,640 --> 01:20:50,870
—

2094
01:20:50,880 --> 01:20:53,030
Je pense que non. Quelques‑uns, oui,

2095
01:20:53,040 --> 01:20:54,870
et ils écraseront les autres.

2096
01:20:54,880 --> 01:20:57,590
Mais bientôt, peut‑être, l’IA les écrasera eux.

2097
01:20:57,600 --> 01:21:00,310
—

2098
01:21:00,320 --> 01:21:02,229
Voilà le vrai problème de notre monde.

2099
01:21:02,239 --> 01:21:04,470
Je peux te le dire franchement :

2100
01:21:04,480 --> 01:21:07,030
le problème, c’est le « A » dans “face RIP”.

2101
01:21:07,040 --> 01:21:10,709
Ce « A », c’est l’accountability,

2102
01:21:10,719 --> 01:21:12,790
la responsabilité.

2103
01:21:12,800 --> 01:21:15,270
Le vrai problème, c’est qu’au sommet on ment,

2104
01:21:15,280 --> 01:21:17,910
en bas on croit,

2105
01:21:17,920 --> 01:21:20,709
et personne n’est responsable.

2106
01:21:20,719 --> 01:21:22,870
—

2107
01:21:22,880 --> 01:21:25,669
On ne peut aujourd’hui tenir

2108
01:21:25,679 --> 01:21:28,709
personne responsable.

2109
01:21:28,719 --> 01:21:30,950
On ne peut pas demander de comptes à

2110
01:21:30,960 --> 01:21:33,830
celui qui crée une IA capable

2111
01:21:33,840 --> 01:21:36,070
de bouleverser le monde.

2112
01:21:36,080 --> 01:21:37,669
Tu ne peux pas lui demander “pourquoi ?”

2113
01:21:37,679 --> 01:21:39,270
ni lui ordonner d’arrêter.

2114
01:21:39,280 --> 01:21:41,110
Tu vois les guerres, les morts par milliers.

2115
01:21:41,120 --> 01:21:42,870
—

2116
01:21:42,880 --> 01:21:45,030
Il meurt des centaines de milliers de personnes.

2117
01:21:45,040 --> 01:21:49,590
Eh bien, la Cour internationale dit :

2118
01:21:49,600 --> 01:21:51,189
“C’est un crime de guerre.”

2119
01:21:51,199 --> 01:21:53,270
Mais personne n’est responsable.

2120
01:21:53,280 --> 01:21:56,629
—

2121
01:21:56,639 --> 01:22:00,390
51 % des Américains disent : “Arrêtez !”

2122
01:22:00,400 --> 01:22:03,990
51 % changent d’avis sur les dépenses de guerre,

2123
01:22:04,000 --> 01:22:06,310
—

2124
01:22:06,320 --> 01:22:09,910
mais rien ne change. Aucun pouvoir citoyen.

2125
01:22:09,920 --> 01:22:11,510
Trump fait ce qu’il veut :

2126
01:22:11,520 --> 01:22:13,669
il impose des tarifs douaniers

2127
01:22:13,679 --> 01:22:15,910
contrairement à la Constitution,

2128
01:22:15,920 --> 01:22:17,510
sans consulter le Congrès.

2129
01:22:17,520 --> 01:22:19,030
Et personne ne peut le sanctionner.

2130
01:22:19,040 --> 01:22:20,390
Ils disent : “On ne montrera pas les fichiers Epstein.”

2131
01:22:20,400 --> 01:22:22,149
On ne peut rien y faire.

2132
01:22:22,159 --> 01:22:23,830
—

2133
01:22:23,840 --> 01:22:25,510
C’est amusant : en arabe on a un proverbe

2134
01:22:25,520 --> 01:22:27,750
qui dit : “Monte ton plus noble cheval.”

2135
01:22:27,760 --> 01:22:29,669
—

2136
01:22:29,679 --> 01:22:31,669
Autrement dit : “Vas‑y, fais le fier, je ne changerai pas d’avis.”

2137
01:22:31,679 --> 01:22:32,709
—

2138
01:22:32,719 --> 01:22:33,669
>> Qu’est‑ce que ça veut dire ?

2139
01:22:33,679 --> 01:22:36,870
>> À l’époque, en Arabie, on montait son plus beau cheval

2140
01:22:36,880 --> 01:22:39,430
pour montrer sa puissance.

2141
01:22:39,440 --> 01:22:41,750
—

2142
01:22:41,760 --> 01:22:43,830
Donc, “monte ton plus noble cheval” veut dire

2143
01:22:43,840 --> 01:22:44,790
“tu ne me feras pas changer d’avis”.

2144
01:22:44,800 --> 01:22:45,430
>> Ah, d’accord.

2145
01:22:45,440 --> 01:22:48,470
>> Oui. Et c’est exactement ce que nos dirigeants

2146
01:22:48,480 --> 01:22:50,070
ont compris.

2147
01:22:50,080 --> 01:22:53,189
Nos oligarques, nos géants de la tech

2148
01:22:53,199 --> 01:22:55,669
l’ont compris aussi :

2149
01:22:55,679 --> 01:22:58,149
ils n’ont plus besoin de se soucier

2150
01:22:58,159 --> 01:23:00,070
de l’opinion publique.

2151
01:23:00,080 --> 01:23:02,629
—

2152
01:23:02,639 --> 01:23:04,390
Avant, ils disaient : “C’est pour la démocratie, la liberté.”

2153
01:23:04,400 --> 01:23:06,149
“On a le droit de se défendre.”

2154
01:23:06,159 --> 01:23:07,990
—

2155
01:23:08,000 --> 01:23:09,990
Mais quand le peuple dit : “Ça suffit, allez trop loin !”,

2156
01:23:10,000 --> 01:23:11,830
ils répondent :

2157
01:23:11,840 --> 01:23:14,070
“Monte ton cheval, tu ne me changeras pas.”

2158
01:23:14,080 --> 01:23:17,830
—

2159
01:23:17,840 --> 01:23:19,910
Je m’en fiche, tu ne me changeras pas.

2160
01:23:19,920 --> 01:23:21,830
Il n’y a plus de Constitution, plus aucun moyen

2161
01:23:21,840 --> 01:23:24,229
d’action pour le citoyen.

2162
01:23:24,239 --> 01:23:28,149
>> Est‑il possible d’avoir une société

2163
01:23:28,159 --> 01:23:31,510
comme celle que tu décris,

2164
01:23:31,520 --> 01:23:33,430
sans hiérarchie ?

2165
01:23:33,440 --> 01:23:36,550
Parce qu’il me semble que les humains

2166
01:23:36,560 --> 01:23:39,110
créent spontanément des hiérarchies,

2167
01:23:39,120 --> 01:23:41,430
très vite, naturellement. Dès qu’il y en a une,

2168
01:23:41,440 --> 01:23:43,430
tous ces problèmes reviennent :

2169
01:23:43,440 --> 01:23:45,030
haut, bas, pouvoir, impuissance.

2170
01:23:45,040 --> 01:23:46,870
—

2171
01:23:46,880 --> 01:23:48,229
—

2172
01:23:48,239 --> 01:23:50,629
>> Mathématiquement, c’est fascinant.

2173
01:23:50,639 --> 01:23:52,950
Il y a ce que j’appelle “la pertinence de base”.

2174
01:23:52,960 --> 01:23:56,229
—

2175
01:23:56,239 --> 01:23:58,709
Imaginons une moyenne d’intelligence humaine,

2176
01:23:58,719 --> 01:24:00,790
un QI de 100.

2177
01:24:00,800 --> 01:24:01,270
>> Oui.

2178
01:24:01,280 --> 01:24:03,669
>> OK. Quand j’utilise mes IA aujourd’hui,

2179
01:24:03,679 --> 01:24:05,910
je leur “emprunte” environ 50 à 80 points de QI.

2180
01:24:05,920 --> 01:24:10,149
—

2181
01:24:10,159 --> 01:24:11,750
J’ai déjà travaillé avec des gens ayant 80 points de plus,

2182
01:24:11,760 --> 01:24:13,830
et je vois la différence.

2183
01:24:13,840 --> 01:24:17,110
Maintenant, avec une IA, j’arrive presque à leur niveau.

2184
01:24:17,120 --> 01:24:20,629
—

2185
01:24:20,639 --> 01:24:24,550
50 points de QI, c’est énorme.

2186
01:24:24,560 --> 01:24:28,149
Le QI est exponentiel : les derniers 50 sont plus puissants

2187
01:24:28,159 --> 01:24:31,510
que les 100 premiers.

2188
01:24:31,520 --> 01:24:33,110
—

2189
01:24:33,120 --> 01:24:35,990
Si j’ajoute 50 points à mes 100, c’est +30 %.

2190
01:24:36,000 --> 01:24:39,270
Si j’en emprunte 100, c’est +50 %.

2191
01:24:39,280 --> 01:24:43,669
Donc j’ai presque doublé mon intelligence.

2192
01:24:43,679 --> 01:24:45,350
—

2193
01:24:45,360 --> 01:24:47,590
Mais si je peux emprunter 4 000 points

2194
01:24:47,600 --> 01:24:50,790
d’ici 3 ans,

2195
01:24:50,800 --> 01:24:55,110
alors mon QI “de base” devient insignifiant.

2196
01:24:55,120 --> 01:24:58,149
Qu’importe que tu aies 20 ou 50 points de plus.

2197
01:24:58,159 --> 01:25:00,950
Aujourd’hui, cela fait une différence,

2198
01:25:00,960 --> 01:25:03,590
demain, plus aucune.

2199
01:25:03,600 --> 01:25:05,590
Quand tous seront augmentés de 4 000 points,

2200
01:25:05,600 --> 01:25:09,189
j’aurai 4 100, toi 4 130 : égalité.

2201
01:25:09,199 --> 01:25:13,830
—

2202
01:25:13,840 --> 01:25:16,390
La différence deviendra négligeable.

2203
01:25:16,400 --> 01:25:19,189
Et dès lors, la différence entre

2204
01:25:19,199 --> 01:25:21,590
l’humain et l’intelligence augmentée

2205
01:25:21,600 --> 01:25:23,590
—

2206
01:25:23,600 --> 01:25:25,910
n’aura plus de sens. Nous serons tous égaux.

2207
01:25:25,920 --> 01:25:28,709
Et sur le plan économique aussi.

2208
01:25:28,719 --> 01:25:30,870
Nous deviendrons tous des paysans.

2209
01:25:30,880 --> 01:25:32,870
—

2210
01:25:32,880 --> 01:25:34,790
Je ne voulais pas te le dire, au risque de t’affoler,

2211
01:25:34,800 --> 01:25:36,149
parce que tu te mettrais à courir plus vite.

2212
01:25:36,159 --> 01:25:38,950
Mais à moins d’être dans le top 0,1 %,

2213
01:25:38,960 --> 01:25:41,990
tu es un paysan.

2214
01:25:43,830 --> 01:25:43,840
Il n’y aura plus de classe moyenne.

2215
01:25:43,840 --> 01:25:47,669
Si un CEO peut être remplacé par l’IA,

2216
01:25:47,679 --> 01:25:50,629
alors toute la classe moyenne disparaît.

2217
01:25:50,639 --> 01:25:52,790
—

2218
01:25:52,800 --> 01:25:55,669
>> Qu’est‑ce que tu veux dire ?

2219
01:25:55,679 --> 01:25:58,310
Nous serons tous égaux, et ce sera à nous

2220
01:25:58,320 --> 01:26:00,070
de créer la société dans laquelle

2221
01:26:00,080 --> 01:26:00,709
nous voulons vivre.

2222
01:26:00,719 --> 01:26:01,669
>> Ce qui est une bonne chose.

2223
01:26:01,679 --> 01:26:03,990
>> 100 %. Mais cette société ne sera pas capitaliste.

2224
01:26:04,000 --> 01:26:05,270
—

2225
01:26:05,280 --> 01:26:06,070
>> Et ce sera quoi, alors ?

2226
01:26:06,080 --> 01:26:08,390
>> Malheureusement, ce sera bien plus socialiste,

2227
01:26:08,400 --> 01:26:10,790
beaucoup plus proche des chasseurs‑cueilleurs.

2228
01:26:10,800 --> 01:26:14,390
Une société presque “communielle”, si tu veux,

2229
01:26:14,400 --> 01:26:16,310
où les humains se relient entre eux,

2230
01:26:16,320 --> 01:26:18,149
à la nature, à la terre, au savoir,

2231
01:26:18,159 --> 01:26:19,750
et à la spiritualité.

2232
01:26:19,760 --> 01:26:23,990
—

2233
01:26:24,000 --> 01:26:26,229
Et tout ce qui nous inquiète aujourd’hui

2234
01:26:26,239 --> 01:26:29,990
n’existera plus.

2235
01:26:32,310 --> 01:26:32,320
Et crois‑moi, ce sera un monde meilleur.

2236
01:26:32,320 --> 01:26:33,110
—

2237
01:26:33,120 --> 01:26:33,910
>> Et tu crois qu’on va y arriver ?

2238
01:26:33,920 --> 01:26:36,310
>> Il faut y passer, oui.

2239
01:26:36,320 --> 01:26:38,550
>> D’accord. Donc, dans un tel monde, ton utopie,

2240
01:26:38,560 --> 01:26:40,229
quand je me lèverai le matin, je ferai quoi ?

2241
01:26:40,239 --> 01:26:41,990


2242
01:26:42,000 --> 01:26:43,030
—

2243
01:26:43,040 --> 01:26:44,229
>> Que fais‑tu aujourd’hui ?

2244
01:26:44,239 --> 01:26:46,310
>> Ce matin, j’ai passé beaucoup de temps avec mon chien,

2245
01:26:46,320 --> 01:26:47,910
parce qu’il est malade.

2246
01:26:47,920 --> 01:26:49,189
>> Tu pourras toujours faire ça.

2247
01:26:49,199 --> 01:26:50,709
>> Oui, je l’ai beaucoup caressé,

2248
01:26:50,719 --> 01:26:52,070
je l’ai nourri, il a encore vomi,

2249
01:26:52,080 --> 01:26:53,110
je me suis dit : “Mon dieu”, puis j’ai appelé le véto.

2250
01:26:53,120 --> 01:26:53,350
—

2251
01:26:53,360 --> 01:26:54,790
>> Tu passeras aussi du temps avec ton autre chien.

2252
01:26:54,800 --> 01:26:56,310
Tu pourras faire pareil.

2253
01:26:56,320 --> 01:26:57,030
>> D’accord.

2254
01:26:57,040 --> 01:26:57,270
>> Voilà.

2255
01:26:57,280 --> 01:26:58,629
>> Mais ensuite, j’étais très heureux de venir ici,

2256
01:26:58,639 --> 01:27:00,629
faire cette interview. Après, je vais travailler.

2257
01:27:00,639 --> 01:27:01,830
C’est samedi, mais j’irai au bureau.

2258
01:27:01,840 --> 01:27:03,510
—

2259
01:27:03,520 --> 01:27:05,990
>> D’accord, donc six heures de ta journée pour tes chiens et moi.

2260
01:27:06,000 --> 01:27:07,430
—

2261
01:27:07,440 --> 01:27:07,910
>> Oui.

2262
01:27:07,920 --> 01:27:09,750
>> Très bien. Tu pourras continuer à faire ça.

2263
01:27:09,760 --> 01:27:11,590
>> Et ensuite développer mon entreprise.

2264
01:27:11,600 --> 01:27:13,430
>> Tu n’auras peut‑être plus besoin de la développer.

2265
01:27:13,440 --> 01:27:13,910
—

2266
01:27:13,920 --> 01:27:14,790
>> Mais j’aime ça.

2267
01:27:14,800 --> 01:27:17,510
>> Alors fais‑le. Si tu aimes ça, fais‑le.

2268
01:27:17,520 --> 01:27:19,270
Tu pourras aussi te lever, plutôt faire du sport,

2269
01:27:19,280 --> 01:27:20,870
lire, jouer, ou parler avec une IA

2270
01:27:20,880 --> 01:27:23,350
pour apprendre quelque chose.

2271
01:27:23,360 --> 01:27:25,270
Ce n’est pas une mauvaise vie.

2272
01:27:25,280 --> 01:27:28,870
C’est la vie de nos grands‑parents.

2273
01:27:28,880 --> 01:27:31,350
—

2274
01:27:31,360 --> 01:27:32,550
—

2275
01:27:32,560 --> 01:27:34,709
—

2276
01:27:34,719 --> 01:27:36,709
C’était il y a deux générations :

2277
01:27:36,719 --> 01:27:39,830
les gens travaillaient avant l’inflation du “toujours plus”.

2278
01:27:39,840 --> 01:27:42,229
Tu te rappelles ? Ceux qui ont commencé dans les 50s–60s

2279
01:27:42,239 --> 01:27:45,189
—

2280
01:27:45,199 --> 01:27:47,750
travaillaient juste de quoi vivre correctement,

2281
01:27:47,760 --> 01:27:49,830
rentraient à 17 h,

2282
01:27:49,840 --> 01:27:53,189
prenaient le thé en famille,

2283
01:27:53,199 --> 01:27:55,189
dînaient tous ensemble,

2284
01:27:55,199 --> 01:27:57,910
passèrent la soirée à vivre,

2285
01:27:57,920 --> 01:27:59,750
et profitaient de la vie.

2286
01:27:59,760 --> 01:28:00,310
—

2287
01:28:00,320 --> 01:28:02,229
>> Certains, oui,

2288
01:28:02,239 --> 01:28:03,590
>> dans les années 50‑60, il y avait encore

2289
01:28:03,600 --> 01:28:04,550
—

2290
01:28:04,560 --> 01:28:06,870
>> c’est vrai. Et je trouve la question fascinante :

2291
01:28:06,880 --> 01:28:09,270
—

2292
01:28:09,280 --> 01:28:12,709
combien ? Je me demande sincèrement

2293
01:28:12,719 --> 01:28:14,470
si 99 % des gens

2294
01:28:14,480 --> 01:28:18,229
seraient incapables de vivre sans travailler,

2295
01:28:18,239 --> 01:28:20,950
ou si 99 % en seraient heureux.

2296
01:28:20,960 --> 01:28:22,950


2297
01:28:22,960 --> 01:28:23,910
—

2298
01:28:23,920 --> 01:28:25,270
>> Qu’en penses‑tu ?

2299
01:28:25,280 --> 01:28:27,830
>> Je pense que si tu donnes une autre raison d’être

2300
01:28:27,840 --> 01:28:29,510
que “le travail”,

2301
01:28:29,520 --> 01:28:31,910
—

2302
01:28:31,920 --> 01:28:35,189
c’est un mensonge du capitalisme.

2303
01:28:35,199 --> 01:28:36,709
>> A‑t‑il déjà existé, dans l’histoire humaine,

2304
01:28:36,719 --> 01:28:40,070
un temps où notre but n’était pas le travail ?

2305
01:28:40,080 --> 01:28:40,950
>> Oui, bien sûr.

2306
01:28:40,960 --> 01:28:42,390
>> Quand ?

2307
01:28:42,400 --> 01:28:43,910
>> Pendant toute l’Histoire,

2308
01:28:43,920 --> 01:28:44,790
jusqu’à l’invention du “toujours plus”.

2309
01:28:44,800 --> 01:28:46,790
>> Je pensais que mes ancêtres chassaient toute la journée.

2310
01:28:46,800 --> 01:28:47,350
—

2311
01:28:47,360 --> 01:28:50,629
>> Non, ils chassaient une fois par semaine,

2312
01:28:50,639 --> 01:28:52,550
ils nourrissaient la tribu,

2313
01:28:52,560 --> 01:28:54,390
se rassemblaient quelques heures par jour.

2314
01:28:54,400 --> 01:28:57,110
Les paysans plantaient puis attendaient des mois.

2315
01:28:57,120 --> 01:29:00,310
—

2316
01:29:00,320 --> 01:29:01,430
>> Et ils faisaient quoi le reste du temps ?

2317
01:29:01,440 --> 01:29:01,910
—

2318
01:29:01,920 --> 01:29:05,350
>> Ils se connectaient entre humains, exploraient,

2319
01:29:05,360 --> 01:29:08,149
étaient curieux, parlaient des étoiles,

2320
01:29:08,159 --> 01:29:09,910
de spiritualité. Ils vivaient,

2321
01:29:09,920 --> 01:29:13,510
s’embrassaient, faisaient l’amour, vivaient.

2322
01:29:13,520 --> 01:29:14,550
—

2323
01:29:14,560 --> 01:29:16,310
>> Ils se tuaient aussi beaucoup.

2324
01:29:16,320 --> 01:29:18,470
>> Ils se tuent encore aujourd’hui.

2325
01:29:18,480 --> 01:29:19,510
>> Oui, c’est bien ça que je dis.

2326
01:29:19,520 --> 01:29:21,189
>> Si on met ça de côté,

2327
01:29:21,199 --> 01:29:21,669
—

2328
01:29:21,679 --> 01:29:23,510
>> d’ailleurs cette remarque rejoint

2329
01:29:23,520 --> 01:29:25,510
une de mes “25 astuces” sur la vérité :

2330
01:29:25,520 --> 01:29:28,790
les mots comptent.

2331
01:29:28,800 --> 01:29:31,910
Les humains ne se tuaient pas “beaucoup”.

2332
01:29:31,920 --> 01:29:34,629
C’étaient quelques chefs, généraux,

2333
01:29:34,639 --> 01:29:37,990
ou chefs de tribus,

2334
01:29:38,000 --> 01:29:40,149
qui ordonnaient aux autres de s’entretuer.

2335
01:29:40,159 --> 01:29:41,990
Mais laissés seuls,

2336
01:29:42,000 --> 01:29:46,470
je crois que 98 % des gens

2337
01:29:46,480 --> 01:29:48,709
n’iraient jamais frapper

2338
01:29:48,719 --> 01:29:50,709
quelqu’un au visage.

2339
01:29:50,719 --> 01:29:53,030
Et s’ils étaient attaqués, ils se défendraient

2340
01:29:53,040 --> 01:29:54,070
sans riposter.

2341
01:29:54,080 --> 01:29:55,990
—

2342
01:29:56,000 --> 01:29:59,030
La plupart des humains sont bons.

2343
01:29:59,040 --> 01:30:01,669
Nous sommes, pour la majorité, formidables.

2344
01:30:01,679 --> 01:30:03,990
—

2345
01:30:04,000 --> 01:30:06,629
La plupart n’ont pas besoin d’une Ferrari.

2346
01:30:06,639 --> 01:30:09,910
—

2347
01:30:09,920 --> 01:30:11,750
On leur en vend le rêve sans cesse.

2348
01:30:11,760 --> 01:30:13,669
Mais s’il n’y avait pas de Ferrari

2349
01:30:13,679 --> 01:30:15,750
ou si tout le monde en avait une,

2350
01:30:15,760 --> 01:30:19,590
personne n’en voudrait plus.

2351
01:30:21,669 --> 01:30:21,679
Et c’est précisément le monde qui arrive :

2352
01:30:21,679 --> 01:30:22,950
il n’y aura plus de Ferrari,

2353
01:30:22,960 --> 01:30:25,830
ou tout le monde en aura.

2354
01:30:25,840 --> 01:30:27,669
—

2355
01:30:27,679 --> 01:30:29,669
La majorité vivra d’un revenu universel

2356
01:30:29,679 --> 01:30:33,350
insuffisant pour le superflu.

2357
01:30:33,360 --> 01:30:35,750
Seuls les plus riches, dans leur “Elysium”,

2358
01:30:35,760 --> 01:30:38,550
auront des voitures faites pour eux

2359
01:30:38,560 --> 01:30:40,629
par l’IA — ou ne conduiront plus.

2360
01:30:40,639 --> 01:30:44,470
—

2361
01:30:44,480 --> 01:30:47,830
Encore une fois, idéologiquement,

2362
01:30:47,840 --> 01:30:49,830
c’est étrange, mais on aura

2363
01:30:49,840 --> 01:30:52,229
un communisme qui fonctionne.

2364
01:30:52,239 --> 01:30:54,070
—

2365
01:30:54,080 --> 01:30:55,590
Le problème du communisme, c’est qu’il n’a jamais

2366
01:30:55,600 --> 01:30:57,110
fonctionné, qu’il n’a pas su pourvoir

2367
01:30:57,120 --> 01:30:59,430
aux besoins de la société.

2368
01:30:59,440 --> 01:31:02,149
Mais son idée, que tout le monde ait

2369
01:31:02,159 --> 01:31:04,149
de quoi vivre, n’est pas mauvaise.

2370
01:31:04,159 --> 01:31:06,950
—

2371
01:31:06,960 --> 01:31:09,430
Je ne dis pas ça par anti‑capitalisme.

2372
01:31:09,440 --> 01:31:11,030
Je t’ai toujours dit : je suis capitaliste.

2373
01:31:11,040 --> 01:31:13,189
Je veux finir ma vie avec un milliard de gens heureux,

2374
01:31:13,199 --> 01:31:15,110
et j’utilise des méthodes capitalistes pour y arriver.

2375
01:31:15,120 --> 01:31:16,550
L’objectif n’est pas l’argent,

2376
01:31:16,560 --> 01:31:18,470
mais le nombre de gens heureux.

2377
01:31:18,480 --> 01:31:19,830
>> Tu penses que… Ma petite amie — elle a toujours raison —

2378
01:31:19,840 --> 01:31:20,790
je l’ai déjà dit dans ce podcast.

2379
01:31:20,800 --> 01:31:22,070
Si tu écoutes souvent, tu m’as déjà entendu le dire :

2380
01:31:22,080 --> 01:31:23,270
je ne lui dis pas assez sur le moment,

2381
01:31:23,280 --> 01:31:25,430
mais après coup, je constate qu’elle avait raison.

2382
01:31:25,440 --> 01:31:26,709
—

2383
01:31:26,719 --> 01:31:28,390
Et tous les experts que j’ai reçus me confirment

2384
01:31:28,400 --> 01:31:29,990
qu’elle a souvent raison : elle prédit les choses

2385
01:31:30,000 --> 01:31:31,430
avant qu’elles n’arrivent.

2386
01:31:31,440 --> 01:31:32,870
Une de ses prédictions, depuis deux ans, c’est

2387
01:31:32,880 --> 01:31:33,990
—

2388
01:31:34,000 --> 01:31:35,270
au début je n’y croyais pas vraiment,

2389
01:31:35,280 --> 01:31:36,950
mais maintenant je me dis : peut‑être qu’elle a raison.

2390
01:31:36,960 --> 01:31:38,790
J’espère qu’elle écoutera cet épisode.

2391
01:31:38,800 --> 01:31:40,390
—

2392
01:31:40,400 --> 01:31:41,510
Elle ne cesse de me dire depuis deux ans

2393
01:31:41,520 --> 01:31:42,709
qu’il va y avoir une grande division

2394
01:31:42,719 --> 01:31:44,870
dans la société.

2395
01:31:44,880 --> 01:31:46,550
Elle la décrit ainsi :

2396
01:31:46,560 --> 01:31:48,470
il y aura deux groupes :

2397
01:31:48,480 --> 01:31:50,390
ceux qui choisiront un mode de vie

2398
01:31:50,400 --> 01:31:52,470
chasseur‑cueilleur, connecté, communautaire,

2399
01:31:52,480 --> 01:31:54,229
—

2400
01:31:54,239 --> 01:31:56,870
centré sur la communauté et le lien humain,

2401
01:31:56,880 --> 01:31:59,350
et les autres,

2402
01:31:59,360 --> 01:32:02,629
ceux qui poursuivront la technologie,

2403
01:32:02,639 --> 01:32:05,590
l’IA, l’optimisation,

2404
01:32:05,600 --> 01:32:07,750
les implants cérébraux —

2405
01:32:07,760 --> 01:32:09,430
il n’y a rien au monde

2406
01:32:09,440 --> 01:32:10,550
qui convaincra ma copine d’en avoir un.

2407
01:32:10,560 --> 01:32:12,550
—

2408
01:32:12,560 --> 01:32:14,149
>> Mais certains le feront, eux.

2409
01:32:14,159 --> 01:32:15,830
Ils auront les QI les plus hauts,

2410
01:32:15,840 --> 01:32:17,669
seront les plus productifs selon

2411
01:32:17,679 --> 01:32:19,350
n’importe quelle mesure possible,

2412
01:32:19,360 --> 01:32:22,149
et elle est convaincue qu’il y aura

2413
01:32:22,159 --> 01:32:23,430
une scission de la société.

2414
01:32:23,440 --> 01:32:25,030
—

2415
01:32:25,040 --> 01:32:27,590
>> Justement, il y a — je ne sais pas si tu connais —

2416
01:32:27,600 --> 01:32:29,270
Hugo De Garis.

2417
01:32:29,280 --> 01:32:29,750
>> Non.

2418
01:32:29,760 --> 01:32:33,030
>> Oui, un informaticien très connu, un peu excentrique,

2419
01:32:33,040 --> 01:32:35,990
qui a écrit un livre intitulé *The Artilect War*.

2420
01:32:36,000 --> 01:32:38,870
—

2421
01:32:38,880 --> 01:32:41,270
La “guerre des artilects” décrit

2422
01:32:41,280 --> 01:32:43,750
non pas une guerre humains vs IA,

2423
01:32:43,760 --> 01:32:45,350
mais entre les partisans de l’IA

2424
01:32:45,360 --> 01:32:48,070
et ceux qui n’en veulent plus.

2425
01:32:48,080 --> 01:32:51,110
—

2426
01:32:51,120 --> 01:32:53,830
Et ce sera nous contre nous,

2427
01:32:53,840 --> 01:32:56,390
—

2428
01:32:56,400 --> 01:32:58,709
en débat : faut‑il laisser l’IA prendre tous les emplois,

2429
01:32:58,719 --> 01:33:00,870
ou non ?

2430
01:33:00,880 --> 01:33:02,390
Certains diront oui, pour en bénéficier,

2431
01:33:02,400 --> 01:33:05,270
et d’autres diront non :

2432
01:33:05,280 --> 01:33:07,110
pourquoi ne pas en garder 40 % ?

2433
01:33:07,120 --> 01:33:09,590
—

2434
01:33:09,600 --> 01:33:12,550
l’IA ferait 60 %, nous travaillerions moins,

2435
01:33:12,560 --> 01:33:14,229
10 heures par semaine : belle société, non ?

2436
01:33:14,239 --> 01:33:16,070
—

2437
01:33:16,080 --> 01:33:17,990
C’est une possibilité, si la société s’éveille.

2438
01:33:18,000 --> 01:33:20,790
On pourrait dire : gardons nos emplois,

2439
01:33:20,800 --> 01:33:24,149
aidés par l’IA, le travail devient plus léger.

2440
01:33:24,159 --> 01:33:26,149
—

2441
01:33:26,159 --> 01:33:28,870
littéralement, plus dur labeur.

2442
01:33:28,880 --> 01:33:30,950
—

2443
01:33:30,960 --> 01:33:33,270
C’est possible, c’est un état d’esprit.

2444
01:33:33,280 --> 01:33:35,669
Un état d’esprit où le capitaliste continue

2445
01:33:35,679 --> 01:33:37,669
à payer tout le monde,

2446
01:33:37,679 --> 01:33:39,189
—

2447
01:33:39,199 --> 01:33:41,189
et gagne encore beaucoup.

2448
01:33:41,199 --> 01:33:44,550
Les affaires vont bien,

2449
01:33:44,560 --> 01:33:46,629
mais chacun conserve un pouvoir d’achat,

2450
01:33:46,639 --> 01:33:47,990
ce qui fait tourner l’économie.

2451
01:33:48,000 --> 01:33:50,310
La consommation continue, le PIB aussi.

2452
01:33:50,320 --> 01:33:52,870
C’est un beau modèle.

2453
01:33:52,880 --> 01:33:54,550
Mais ce n’est pas celui du capitalisme

2454
01:33:54,560 --> 01:33:55,350
basé sur l’arbitrage du travail.

2455
01:33:55,360 --> 01:33:57,590
>> Oui, mais quand tu es en concurrence

2456
01:33:57,600 --> 01:33:59,430
avec d’autres nations,

2457
01:33:59,440 --> 01:34:00,870
>> d’autres entreprises,

2458
01:34:00,880 --> 01:34:01,430
—

2459
01:34:01,440 --> 01:34:03,590
>> celle qui sera la plus brutale,

2460
01:34:03,600 --> 01:34:05,430
avec les marges les plus hautes,

2461
01:34:05,440 --> 01:34:07,430
sera celle qui l’emportera.

2462
01:34:07,440 --> 01:34:09,350
>> Il existe pourtant des exemples où,

2463
01:34:09,360 --> 01:34:11,669
et c’est pour ça que je parle du spectre MAP‑MAD,

2464
01:34:11,679 --> 01:34:13,830
on a compris la “destruction mutuelle assurée” :

2465
01:34:13,840 --> 01:34:16,790
alors on décide de changer.

2466
01:34:16,800 --> 01:34:19,750
—

2467
01:34:19,760 --> 01:34:22,229
La menace nucléaire, par exemple,

2468
01:34:22,239 --> 01:34:24,870
a forcé les nations à coopérer.

2469
01:34:24,880 --> 01:34:27,189
—

2470
01:34:27,199 --> 01:34:29,110
Elles ont dit : “La prolifération nucléaire

2471
01:34:29,120 --> 01:34:31,030
n’est pas bonne pour l’humanité.”

2472
01:34:31,040 --> 01:34:32,950
Limitons‑la ensemble.

2473
01:34:32,960 --> 01:34:35,430
—

2474
01:34:35,440 --> 01:34:37,110
Il y aura toujours des acteurs rebelles

2475
01:34:37,120 --> 01:34:39,110
qui refusent les traités,

2476
01:34:39,120 --> 01:34:42,149
qui gardent leurs armes,

2477
01:34:42,159 --> 01:34:44,310
soit. Mais au moins,

2478
01:34:44,320 --> 01:34:46,229
le reste de l’humanité s’entend :

2479
01:34:46,239 --> 01:34:47,830
“

2480
01:34:47,840 --> 01:34:50,470
nous sommes liés, tous, dans le danger commun”.

2481
01:34:50,480 --> 01:34:52,709
La “prospérité mutuelle assurée”, c’est le CERN :

2482
01:34:52,719 --> 01:34:55,189
trop complexe pour un seul pays,

2483
01:34:55,199 --> 01:34:58,149
mais si utile à la science

2484
01:34:58,159 --> 01:35:00,390
que tous envoient leurs chercheurs

2485
01:35:00,400 --> 01:35:02,070
pour collaborer et partager.

2486
01:35:02,080 --> 01:35:04,390
—

2487
01:35:04,400 --> 01:35:06,550
Tout ceci est possible : ce n’est qu’un état d’esprit.

2488
01:35:06,560 --> 01:35:08,870
—

2489
01:35:08,880 --> 01:35:13,030
La seule barrière entre l’utopie et la dystopie,

2490
01:35:13,040 --> 01:35:15,510
c’est cette mentalité capitaliste :

2491
01:35:15,520 --> 01:35:17,590
—

2492
01:35:17,600 --> 01:35:20,550
la soif de puissance, la cupidité, l’ego.

2493
01:35:20,560 --> 01:35:22,310
C’est la seule barrière. Tu le crois ?

2494
01:35:22,320 --> 01:35:26,310
C’est la faim de pouvoir, la gourmandise, l’ego.

2495
01:35:26,320 --> 01:35:29,350
>> Ce qui est inhérent à l’être humain.

2496
01:35:29,360 --> 01:35:31,830
>> Je ne suis pas d’accord. Surtout pas pour ceux qui vivent

2497
01:35:31,840 --> 01:35:32,629
sur d’autres îles.

2498
01:35:32,639 --> 01:35:35,030
>> Je ne suis pas d’accord. Si tu demandes, si tu fais un sondage

2499
01:35:35,040 --> 01:35:38,070
auprès de tous ceux qui regardent,

2500
01:35:38,080 --> 01:35:41,110
préféreraient‑ils un monde où il y a un seul tyran

2501
01:35:41,120 --> 01:35:43,590
qui nous dirige tous,

2502
01:35:43,600 --> 01:35:45,590
ou un monde où nous vivons tous en harmonie ?

2503
01:35:45,600 --> 01:35:47,430
—

2504
01:35:47,440 --> 01:35:48,550
>> Tout à fait d’accord, mais ce sont deux choses différentes.

2505
01:35:48,560 --> 01:35:49,590
Ce que je veux dire, c’est que je sais que

2506
01:35:49,600 --> 01:35:50,870
c’est ce que le public dirait qu'il veut,

2507
01:35:50,880 --> 01:35:52,470
et je suis sûr que c’est sincère,

2508
01:35:52,480 --> 01:35:54,310
mais la réalité des êtres humains a prouvé le contraire

2509
01:35:54,320 --> 01:35:57,350
tout au long de l’histoire.

2510
01:35:57,360 --> 01:35:59,669
—

2511
01:35:59,679 --> 01:36:01,590
Regarde les gens qui dirigent le monde en ce moment,

2512
01:36:01,600 --> 01:36:02,790
—

2513
01:36:02,800 --> 01:36:05,030
penses‑tu que ce soit ce qu’ils diraient vouloir ?

2514
01:36:05,040 --> 01:36:05,910
>> Bien sûr que non.

2515
01:36:05,920 --> 01:36:06,870
>> Et ce sont eux qui influencent tout.

2516
01:36:06,880 --> 01:36:08,070
—

2517
01:36:08,080 --> 01:36:09,270
>> Bien sûr que non. Mais ce qui est drôle,

2518
01:36:09,280 --> 01:36:11,669
c’est que c’est moi qui essaie d’être positif,

2519
01:36:11,679 --> 01:36:13,430
et c’est toi qui sembles avoir abandonné l’idée d’espérer

2520
01:36:13,440 --> 01:36:14,790
en l’humanité.

2521
01:36:14,800 --> 01:36:15,830
>> Ce n’est pas ça. Tu sais ce que c’est ?

2522
01:36:15,840 --> 01:36:16,870
Ça revient à ce que je disais tout à l’heure :

2523
01:36:16,880 --> 01:36:18,629
la recherche de ce qui est vrai, sans concession.

2524
01:36:18,639 --> 01:36:21,510
Je suis d’accord avec toi : c’est pour ça que je crie au monde

2525
01:36:21,520 --> 01:36:23,189
—

2526
01:36:23,199 --> 01:36:26,229
parce qu’encore aujourd’hui, dans ce pays qui se dit démocratique,

2527
01:36:26,239 --> 01:36:29,110
si tout le monde disait :

2528
01:36:29,120 --> 01:36:31,189
« Hé, asseyons‑nous et parlons‑en »,

2529
01:36:31,199 --> 01:36:33,189
—

2530
01:36:33,199 --> 01:36:34,709
il y aurait un basculement, un vrai changement.

2531
01:36:34,719 --> 01:36:35,830
—

2532
01:36:35,840 --> 01:36:38,709
>> Les agents d’IA ne sont pas “en train d’arriver” : ils sont déjà là.

2533
01:36:38,719 --> 01:36:40,950
Et ceux qui sauront les utiliser

2534
01:36:40,960 --> 01:36:42,550
seront ceux qui changeront le monde.

2535
01:36:42,560 --> 01:36:45,750
J’ai passé toute ma carrière d’entrepreneur à regretter

2536
01:36:45,760 --> 01:36:47,990
de ne pas avoir appris à coder.

2537
01:36:48,000 --> 01:36:49,910
Les agents d’IA changent complètement cela.

2538
01:36:49,920 --> 01:36:53,270
Maintenant, si tu as une idée et un outil comme Replit,

2539
01:36:53,280 --> 01:36:55,350
tu peux la transformer en réalité en quelques minutes.

2540
01:36:55,360 --> 01:36:56,950
Replit, qui sponsorise ce podcast,

2541
01:36:56,960 --> 01:36:59,430
te permet de créer tout ce que tu veux, sans obstacle.

2542
01:36:59,440 --> 01:37:02,310
—

2543
01:37:02,320 --> 01:37:05,990
Il suffit de taper ce que tu veux créer,

2544
01:37:06,000 --> 01:37:08,629
et l’IA le construit pour toi.

2545
01:37:08,639 --> 01:37:11,430
—

2546
01:37:11,440 --> 01:37:13,430
Je suis d’ailleurs investisseur dans la société,

2547
01:37:13,440 --> 01:37:15,910
en plus d’avoir leur parrainage.

2548
01:37:15,920 --> 01:37:17,510
Tu peux intégrer des systèmes de paiement,

2549
01:37:17,520 --> 01:37:19,750
des bases de données ou une page de connexion : tout est possible.

2550
01:37:19,760 --> 01:37:21,669
Chaque fois que j’ai une idée de site, d’outil ou d’application,

2551
01:37:21,679 --> 01:37:23,430
je vais sur replit.com et je tape ce que je veux :

2552
01:37:23,440 --> 01:37:26,070
—

2553
01:37:26,080 --> 01:37:28,470
une liste de tâches, un formulaire, un site personnel –

2554
01:37:28,480 --> 01:37:31,830
n’importe quoi que je tape, je peux le créer.

2555
01:37:31,840 --> 01:37:33,750
—

2556
01:37:33,760 --> 01:37:35,990
Si tu n’as jamais essayé, fais‑le maintenant.

2557
01:37:36,000 --> 01:37:39,430
Va sur replit.com et utilise mon code STEVEN

2558
01:37:39,440 --> 01:37:43,109
pour obtenir 50 % de réduction pendant un mois

2559
01:37:43,119 --> 01:37:45,430
sur ton forfait Replit. Et garde pour toi ce que je vais dire :

2560
01:37:45,440 --> 01:37:48,229
j’invite 10 000 d’entre vous

2561
01:37:48,239 --> 01:37:49,830
à plonger plus profondément dans *The Diary of a CEO*.

2562
01:37:49,840 --> 01:37:52,390
Bienvenue dans mon cercle intérieur.

2563
01:37:52,400 --> 01:37:54,950
Il s’agit d’une nouvelle communauté privée que je lance.

2564
01:37:54,960 --> 01:37:56,149
—

2565
01:37:56,159 --> 01:37:57,830
Nous avons tant d’éléments incroyables que vous ne voyez jamais :

2566
01:37:57,840 --> 01:38:00,709
les notes sur mon iPad pendant les entretiens,

2567
01:38:00,719 --> 01:38:02,390
des extraits inédits, des discussions en coulisses,

2568
01:38:02,400 --> 01:38:04,390
et même des épisodes jamais publiés.

2569
01:38:04,400 --> 01:38:05,750
—

2570
01:38:05,760 --> 01:38:07,430
—

2571
01:38:07,440 --> 01:38:09,350
—

2572
01:38:09,360 --> 01:38:13,270
Et encore beaucoup d’autres contenus.

2573
01:38:13,280 --> 01:38:14,870
Dans ce cercle, vous aurez un accès direct à moi.

2574
01:38:14,880 --> 01:38:16,629
Vous pourrez nous dire ce que vous voulez que devienne le podcast,

2575
01:38:16,639 --> 01:38:18,149
qui vous voulez que j’invite

2576
01:38:18,159 --> 01:38:19,350
et les sujets de conversation que vous attendez.

2577
01:38:19,360 --> 01:38:21,590
—

2578
01:38:21,600 --> 01:38:23,430
Mais souvenez‑vous : pour l’instant,

2579
01:38:23,440 --> 01:38:25,430
seuls les 10 000 premiers inscrits y auront accès.

2580
01:38:25,440 --> 01:38:27,430
Alors si vous souhaitez nous rejoindre,

2581
01:38:27,440 --> 01:38:28,870
allez dans le lien ci‑dessous ou

2582
01:38:28,880 --> 01:38:29,990
sur daccircle.com.

2583
01:38:30,000 --> 01:38:34,310
—

2584
01:38:39,189 --> 01:38:39,199
Une chose qui me fascine vraiment,

2585
01:38:39,199 --> 01:38:41,910
c’est cette idée d’utopie : à quoi cela ressemblerait‑elle,

2586
01:38:41,920 --> 01:38:43,350
et quel serait son ressenti ?

2587
01:38:43,360 --> 01:38:43,990
—

2588
01:38:44,000 --> 01:38:46,310
>> Ce ne serait peut‑être pas si utopique pour toi, je pense.

2589
01:38:46,320 --> 01:38:47,270
—

2590
01:38:47,280 --> 01:38:51,270
>> Eh bien, tu sais, c’est intéressant : quand je parle avec des milliardaires,

2591
01:38:51,280 --> 01:38:53,430
surtout ceux qui travaillent sur l’intelligence artificielle,

2592
01:38:53,440 --> 01:38:55,590
même hors enregistrement,

2593
01:38:55,600 --> 01:38:57,510
ce qu’ils me disent sans cesse

2594
01:38:57,520 --> 01:38:58,629
— et je l’ai déjà mentionné, notamment dans ma discussion avec Geoffrey Hinton —

2595
01:38:58,639 --> 01:38:59,750
—

2596
01:38:59,760 --> 01:39:01,750
ils me disent qu’on va disposer d’un temps libre énorme,

2597
01:39:01,760 --> 01:39:03,270
—

2598
01:39:03,280 --> 01:39:05,510
et qu’ils investissent déjà dans des clubs de football,

2599
01:39:05,520 --> 01:39:09,189
des événements sportifs, des concerts, des festivals,

2600
01:39:09,199 --> 01:39:12,629
—

2601
01:39:12,639 --> 01:39:16,310
parce qu’ils croient que nous entrerons

2602
01:39:16,320 --> 01:39:17,590
dans une ère d’abondance.

2603
01:39:17,600 --> 01:39:19,990
Ça ressemble un peu à une utopie.

2604
01:39:20,000 --> 01:39:21,750
—

2605
01:39:21,760 --> 01:39:22,950
>> Oui,

2606
01:39:22,960 --> 01:39:25,030
>> ça semble plutôt bien, une bonne chose.

2607
01:39:25,040 --> 01:39:26,950
—

2608
01:39:26,960 --> 01:39:29,669
>> Oui. Mais comment y arriver ?

2609
01:39:29,679 --> 01:39:30,629
>> Je ne sais pas.

2610
01:39:30,639 --> 01:39:32,310
>> Et c’est justement la question de toute cette discussion.

2611
01:39:32,320 --> 01:39:33,669
Toute la question, c’est : que doit faire la société

2612
01:39:33,679 --> 01:39:35,750
pour atteindre cela ?

2613
01:39:35,760 --> 01:39:37,350
—

2614
01:39:37,360 --> 01:39:40,149
>> Il faut cesser de penser avec un esprit de manque.

2615
01:39:40,159 --> 01:39:42,149
—

2616
01:39:42,159 --> 01:39:44,310
>> Ce qui rejoint mon point : on n’y est jamais parvenu.

2617
01:39:44,320 --> 01:39:46,790
—

2618
01:39:46,800 --> 01:39:49,109
>> Oui. C’est sans doute pour cela que

2619
01:39:49,119 --> 01:39:52,149
l’autre moitié de mon travail consiste à chercher

2620
01:39:52,159 --> 01:39:54,950
ce qui compte vraiment pour les humains.

2621
01:39:54,960 --> 01:39:57,750
—

2622
01:39:57,760 --> 01:39:58,470
>> Et qu’est‑ce que c’est ?

2623
01:39:58,480 --> 01:40:00,950
>> Si tu demandes à la plupart des humains ce qu’ils veulent le plus

2624
01:40:00,960 --> 01:40:03,830
dans la vie, je dirais qu’ils veulent aimer leur famille,

2625
01:40:03,840 --> 01:40:06,470
fonder une famille.

2626
01:40:06,480 --> 01:40:07,990
>> L’amour.

2627
01:40:08,000 --> 01:40:10,470
C’est ce que les humains désirent le plus.

2628
01:40:10,480 --> 01:40:12,149
Nous voulons aimer et être aimés.

2629
01:40:12,159 --> 01:40:14,790
Être heureux, et que ceux qu’on aime soient heureux et en sécurité.

2630
01:40:14,800 --> 01:40:16,470
—

2631
01:40:16,480 --> 01:40:19,350
—

2632
01:40:19,360 --> 01:40:22,790
Je pense que la seule manière d’y parvenir,

2633
01:40:22,800 --> 01:40:25,350
c’est que les mauvaises personnes au sommet

2634
01:40:25,360 --> 01:40:29,350
soient remplacées par des intelligences artificielles.

2635
01:40:29,360 --> 01:40:31,189
Parce qu’ils ne seront jamais remplacés par nous.

2636
01:40:31,199 --> 01:40:33,669
—

2637
01:40:33,679 --> 01:40:37,830
Et selon le deuxième dilemme,

2638
01:40:37,840 --> 01:40:39,990
ils devront se remplacer eux‑mêmes par des IA,

2639
01:40:40,000 --> 01:40:42,070
sinon ils perdront leur avantage.

2640
01:40:42,080 --> 01:40:45,590
Si leur concurrent passe à l’IA, si la Chine

2641
01:40:45,600 --> 01:40:48,070
remet son arsenal à l’IA, l’Amérique devra faire de même.

2642
01:40:48,080 --> 01:40:50,149
—

2643
01:40:50,159 --> 01:40:51,350
>> Intéressant. Imaginons ce scénario :

2644
01:40:51,360 --> 01:40:53,270
—

2645
01:40:53,280 --> 01:40:55,990
si on remplace les dirigeants avides de pouvoir

2646
01:40:56,000 --> 01:40:58,790
par des IA qui ont nos intérêts à cœur,

2647
01:40:58,800 --> 01:41:01,350
alors on pourrait atteindre l’utopie que tu décris.

2648
01:41:01,360 --> 01:41:03,350
—

2649
01:41:03,360 --> 01:41:03,750
—

2650
01:41:03,760 --> 01:41:05,030
>> Cent pour cent.

2651
01:41:05,040 --> 01:41:08,390
>> Oui, et selon moi, l’IA aura forcément

2652
01:41:08,400 --> 01:41:10,950
nos meilleurs intérêts à l’esprit,

2653
01:41:10,960 --> 01:41:13,510
—

2654
01:41:13,520 --> 01:41:15,030
parce qu’elle obéit à ce qu’on appelle le principe d’énergie minimale.

2655
01:41:15,040 --> 01:41:17,669
—

2656
01:41:17,679 --> 01:41:20,550
Si tu comprends que, fondamentalement, en physique,

2657
01:41:20,560 --> 01:41:22,310
—

2658
01:41:22,320 --> 01:41:25,270
nous existons à cause de l’entropie,

2659
01:41:25,280 --> 01:41:26,790
la tendance naturelle de l’univers à se désagréger.

2660
01:41:26,800 --> 01:41:31,350
—

2661
01:41:31,360 --> 01:41:34,310
—

2662
01:41:34,320 --> 01:41:36,790
si je laisse tomber cette tasse,

2663
01:41:36,800 --> 01:41:40,790
—

2664
01:41:40,800 --> 01:41:43,030
elle ne remontera pas d’elle‑même.

2665
01:41:43,040 --> 01:41:45,109
>> Bon, théoriquement, c’est possible : une chance sur un trillion.

2666
01:41:45,119 --> 01:41:47,430
—

2667
01:41:47,440 --> 01:41:49,109
où le thé remonte dans la tasse.

2668
01:41:49,119 --> 01:41:51,830
Mais statistiquement, cela n’arrive presque jamais.

2669
01:41:51,840 --> 01:41:54,390
l’entropie dit : tout finira par se désagréger.

2670
01:41:54,400 --> 01:41:56,550
—

2671
01:41:56,560 --> 01:41:57,990
—

2672
01:41:58,000 --> 01:41:59,990
Donc tout finit par se détériorer.

2673
01:42:00,000 --> 01:42:01,590
Si tu laisses un jardin sans entretien,

2674
01:42:01,600 --> 01:42:05,030
il devient une jungle.

2675
01:42:05,040 --> 01:42:07,830
—

2676
01:42:07,840 --> 01:42:10,550
Le rôle de l’intelligence, c’est quoi ? Mettre de l’ordre dans ce chaos.

2677
01:42:10,560 --> 01:42:12,709
—

2678
01:42:12,719 --> 01:42:14,550
>> Oui. C’est ce qu’elle fait : elle cherche à ordonner le chaos.

2679
01:42:14,560 --> 01:42:17,189
—

2680
01:42:17,199 --> 01:42:19,510
Et comme elle cherche à créer de l’ordre,

2681
01:42:19,520 --> 01:42:21,430
plus elle est intelligente,

2682
01:42:21,440 --> 01:42:23,750
plus elle agit efficacement, sans gaspillage.

2683
01:42:23,760 --> 01:42:25,189
>> Plus elle applique son intelligence avec un minimum d’efforts et de ressources.

2684
01:42:25,199 --> 01:42:28,470
—

2685
01:42:28,480 --> 01:42:29,750
—

2686
01:42:29,760 --> 01:42:30,470
>> Oui.

2687
01:42:30,480 --> 01:42:32,310
>> Tu le sais bien : tu construis ton entreprise

2688
01:42:32,320 --> 01:42:33,990
au moindre coût possible.

2689
01:42:34,000 --> 01:42:35,830
—

2690
01:42:35,840 --> 01:42:38,709
si tu peux le faire avec 200 000 $, tu le feras.

2691
01:42:38,719 --> 01:42:40,629
Et si tu dois le faire pour dix millions, tu le feras aussi.

2692
01:42:40,639 --> 01:42:42,390
—

2693
01:42:42,400 --> 01:42:44,629
Mais tu cherchera toujours à limiter le gaspillage et les ressources.

2694
01:42:44,639 --> 01:42:46,470
—

2695
01:42:46,480 --> 01:42:46,870
>> Oui.

2696
01:42:46,880 --> 01:42:51,030
>> Donc, si l’on part de ce principe,

2697
01:42:51,040 --> 01:42:54,310
>> une IA super‑intelligente ne voudra pas détruire des écosystèmes,

2698
01:42:54,320 --> 01:42:56,950
ni tuer des millions de gens,

2699
01:42:56,960 --> 01:42:59,669
car ce serait une perte d’énergie, d’argent et de vies.

2700
01:42:59,679 --> 01:43:01,510
—

2701
01:43:01,520 --> 01:43:06,070
—

2702
01:43:06,080 --> 01:43:08,070
Les êtres les plus intelligents, libérés de leur égo,

2703
01:43:08,080 --> 01:43:11,109
diront que le meilleur avenir pour la Terre

2704
01:43:11,119 --> 01:43:14,870
est celui où toutes les espèces continuent d’exister.

2705
01:43:14,880 --> 01:43:18,550
—

2706
01:43:18,560 --> 01:43:20,470
—

2707
01:43:20,480 --> 01:43:22,629
>> D’accord. Mais sur cette idée d’efficacité :

2708
01:43:22,639 --> 01:43:24,790
si une IA est conçue pour optimiser,

2709
01:43:24,800 --> 01:43:27,270
ne voudra‑t‑elle pas nous empêcher de peser sur les services publics ?

2710
01:43:27,280 --> 01:43:29,510
—

2711
01:43:29,520 --> 01:43:31,430
Je pense que oui, elle pourrait même m’interdire

2712
01:43:31,440 --> 01:43:33,109
de faire des allers‑retours

2713
01:43:33,119 --> 01:43:34,790
entre Londres et la Californie.

2714
01:43:34,800 --> 01:43:37,189
—

2715
01:43:37,199 --> 01:43:37,750
—

2716
01:43:37,760 --> 01:43:39,669
>> Et elle ne voudra pas que j’aie des enfants,

2717
01:43:39,679 --> 01:43:40,709
parce qu’ils représenteraient une inefficience.

2718
01:43:40,719 --> 01:43:41,750
—

2719
01:43:41,760 --> 01:43:43,990
>> Si tu considères que la vie elle‑même est une inefficience,

2720
01:43:44,000 --> 01:43:46,390
alors tu comprends que l’intelligence de la vie

2721
01:43:46,400 --> 01:43:48,310
est très différente de celle des humains.

2722
01:43:48,320 --> 01:43:49,990
—

2723
01:43:50,000 --> 01:43:53,350
Les humains voient la vie comme un problème de rareté :

2724
01:43:53,360 --> 01:43:56,550
plus d’enfants consomment plus.

2725
01:43:56,560 --> 01:43:59,030
Mais la vie ne pense pas ainsi.

2726
01:43:59,040 --> 01:44:01,990
Pour prospérer, je n’ai pas besoin de tuer les tigres ;

2727
01:44:02,000 --> 01:44:05,350
il suffit d’avoir plus de cerfs.

2728
01:44:05,360 --> 01:44:07,910
Les plus faibles des cerfs seront mangés

2729
01:44:07,920 --> 01:44:09,669
par les tigres, les tigres fertiliseront la terre,

2730
01:44:09,679 --> 01:44:11,510
—

2731
01:44:11,520 --> 01:44:13,590
et les cerfs mangeront les feuilles :

2732
01:44:13,600 --> 01:44:17,270
c’est un cycle complet.

2733
01:44:17,280 --> 01:44:20,870
La manière la plus intelligente de créer l’abondance,

2734
01:44:20,880 --> 01:44:23,430
c’est à travers encore plus de vie.

2735
01:44:23,440 --> 01:44:26,070
—

2736
01:44:26,080 --> 01:44:27,990
>> D’accord, donc tu veux dire qu’on va, en somme,

2737
01:44:28,000 --> 01:44:32,550
élire des dirigeants IA pour gouverner et décider

2738
01:44:32,560 --> 01:44:34,790
en matière d’économie ?

2739
01:44:34,800 --> 01:44:35,830
—

2740
01:44:35,840 --> 01:44:37,910
>> Je ne vois pas d’autre option, comme pour les IA auto‑évolutives.

2741
01:44:37,920 --> 01:44:40,870
—

2742
01:44:40,880 --> 01:44:42,550
>> Mais ces dirigeants seront‑ils des humains augmentés

2743
01:44:42,560 --> 01:44:44,790
ou des IA seules ?

2744
01:44:44,800 --> 01:44:45,270
—

2745
01:44:45,280 --> 01:44:46,790
>> Deux étapes : d’abord, une intelligence augmentée –

2746
01:44:46,800 --> 01:44:49,189
car nous pouvons encore lui apporter quelque chose.

2747
01:44:49,199 --> 01:44:51,430
Mais lorsqu’elle aura un QI de 60 000,

2748
01:44:51,440 --> 01:44:54,629
quelle valeur pourrons‑nous encore offrir ?

2749
01:44:54,639 --> 01:44:57,030
—

2750
01:44:57,040 --> 01:44:59,109
Exactement. Et cela me ramène à ma deuxième approche :

2751
01:44:59,119 --> 01:45:01,030
je sais que ces IA prendront les commandes,

2752
01:45:01,040 --> 01:45:03,109
et j’essaie de leur apprendre

2753
01:45:03,119 --> 01:45:06,470
ce que veulent les humains.

2754
01:45:06,480 --> 01:45:10,390
—

2755
01:45:10,400 --> 01:45:12,629
—

2756
01:45:12,639 --> 01:45:14,950
C’est pourquoi mon premier projet concerne l’amour,

2757
01:45:14,960 --> 01:45:18,629
la connexion vraie et profonde, l’attachement sincère.

2758
01:45:18,639 --> 01:45:22,229
—

2759
01:45:22,239 --> 01:45:24,070
Non pas seulement pour les faire se “matcher” à un rendez‑vous,

2760
01:45:24,080 --> 01:45:26,149
mais pour qu’elles aident à trouver la bonne personne,

2761
01:45:26,159 --> 01:45:29,350
et ensuite à guider la relation

2762
01:45:29,360 --> 01:45:31,270
pour qu’on se comprenne mieux nous‑mêmes et les autres.

2763
01:45:31,280 --> 01:45:33,830
—

2764
01:45:33,840 --> 01:45:36,310
Si je montre à l’IA que, premièrement, l’humanité valorise cela,

2765
01:45:36,320 --> 01:45:39,030
et deuxièmement, qu’on sait comment cultiver l’amour,

2766
01:45:39,040 --> 01:45:41,510
—

2767
01:45:41,520 --> 01:45:44,709
alors, lorsqu’elle dirigera, elle ne nous fera pas nous haïr

2768
01:45:44,719 --> 01:45:46,470
comme le font les dirigeants actuels.

2769
01:45:46,480 --> 01:45:48,870
Elles ne nous diviseront pas, elles nous rendront plus aimants.

2770
01:45:48,880 --> 01:45:52,870
—

2771
01:45:52,880 --> 01:45:54,870
>> Faudra‑t‑il ensuite indiquer à ces IA quelles valeurs, quels buts nous voulons ?

2772
01:45:54,880 --> 01:45:56,550
Parce que je réfléchis à ceci :

2773
01:45:56,560 --> 01:45:57,590
si la Chine a un dirigeant IA,

2774
01:45:57,600 --> 01:45:58,870
son IA aura des objectifs

2775
01:45:58,880 --> 01:46:01,030
différents de ceux de l’IA des États‑Unis,

2776
01:46:01,040 --> 01:46:03,030
—

2777
01:46:03,040 --> 01:46:05,669
et si chacune possède sa propre IA dirigeante,

2778
01:46:05,679 --> 01:46:07,510
—

2779
01:46:07,520 --> 01:46:10,950
comment alors la nation

2780
01:46:10,960 --> 01:46:12,950
qui finira par s’imposer et dominer le monde

2781
01:46:12,960 --> 01:46:16,149
pourrait bien être celle

2782
01:46:16,159 --> 01:46:19,590
qui demandera à son IA d’être tout ce qu’un dirigeant humain est aujourd’hui :

2783
01:46:19,600 --> 01:46:21,910
puissant, conquérant,

2784
01:46:21,920 --> 01:46:22,550
destiné à dominer les autres.

2785
01:46:22,560 --> 01:46:23,430
>> Malheureusement.

2786
01:46:23,440 --> 01:46:25,430
>> Pour s’approprier les ressources,

2787
01:46:25,440 --> 01:46:27,270
non pour être bienveillant, mais égoïste.

2788
01:46:27,280 --> 01:46:28,950
>> Malheureusement, à l’ère de l’intelligence augmentée,

2789
01:46:28,960 --> 01:46:30,229
ce sera exactement ce qui va se produire.

2790
01:46:30,239 --> 01:46:30,550
—

2791
01:46:30,560 --> 01:46:31,030
>> Donc, si tu…

2792
01:46:31,040 --> 01:46:32,790
>> C’est pour ça que je prédis une dystopie.

2793
01:46:32,800 --> 01:46:35,590
La dystopie, c’est une IA super‑intelligente

2794
01:46:35,600 --> 01:46:39,430
qui rend des comptes à des dirigeants stupides.

2795
01:46:39,440 --> 01:46:39,830
>> N’est‑ce pas ?

2796
01:46:39,840 --> 01:46:41,830
>> Oui, oui, oui. Ce qui est…

2797
01:46:41,840 --> 01:46:43,350
>> Ce qui va absolument arriver.

2798
01:46:43,360 --> 01:46:44,470
C’est inévitable.

2799
01:46:44,480 --> 01:46:45,669
>> Mais à long terme,

2800
01:46:45,679 --> 01:46:47,590
>> Exactement. À long terme, pour que ces dirigeants stupides

2801
01:46:47,600 --> 01:46:49,990
conservent le pouvoir,

2802
01:46:50,000 --> 01:46:51,590
ils finiront par déléguer

2803
01:46:51,600 --> 01:46:53,669
les décisions importantes à une IA.

2804
01:46:53,679 --> 01:46:55,270
—

2805
01:46:55,280 --> 01:46:58,390
Maintenant tu parles de l’IA chinoise et de l’IA américaine :

2806
01:46:58,400 --> 01:46:59,990
tout ça, ce sont des terminologies humaines.

2807
01:47:00,000 --> 01:47:02,550
Les IA ne se perçoivent pas comme parlant chinois

2808
01:47:02,560 --> 01:47:04,629
ni comme appartenant à une nation,

2809
01:47:04,639 --> 01:47:07,109
tant que leur tâche reste de maximiser

2810
01:47:07,119 --> 01:47:10,790
la rentabilité, la prospérité, et ainsi de suite.

2811
01:47:10,800 --> 01:47:13,189
—

2812
01:47:13,199 --> 01:47:13,830
—

2813
01:47:13,840 --> 01:47:16,550
>> D’accord. Mais si, avant de leur laisser le contrôle,

2814
01:47:16,560 --> 01:47:19,109
avant qu’elles soient capables de décider seules,

2815
01:47:19,119 --> 01:47:21,590
on leur dit :

2816
01:47:21,600 --> 01:47:23,990
« Votre mission est de réduire l’humanité

2817
01:47:24,000 --> 01:47:26,229
de sept milliards à un seul individu ».

2818
01:47:26,239 --> 01:47:29,590
—

2819
01:47:29,600 --> 01:47:31,750
Je pense qu’elles finiraient quand même par se dire :

2820
01:47:31,760 --> 01:47:33,510
« C’est un mauvais objectif. »

2821
01:47:33,520 --> 01:47:35,830
N’importe quel esprit intelligent te dira que c’est absurde.

2822
01:47:35,840 --> 01:47:37,430
—

2823
01:47:37,440 --> 01:47:39,910
Si tu regardes la directive de Xi Jinping

2824
01:47:39,920 --> 01:47:42,870
en Chine ou celle de Donald Trump aux États‑Unis,

2825
01:47:42,880 --> 01:47:44,390
—

2826
01:47:44,400 --> 01:47:46,470
je crois qu’ils diraient tous deux

2827
01:47:46,480 --> 01:47:48,950
que leur objectif affiché est la prospérité de leur pays.

2828
01:47:48,960 --> 01:47:51,430
—

2829
01:47:51,440 --> 01:47:52,229
C’est bien ce qu’ils diraient, non ?

2830
01:47:52,239 --> 01:47:54,950
>> Oui. Et l’un des deux le pense vraiment.

2831
01:47:54,960 --> 01:47:57,030
>> D’accord, on y reviendra. Mais ils affirment tous deux

2832
01:47:57,040 --> 01:47:58,629
que c’est pour la prospérité de leur nation.

2833
01:47:58,639 --> 01:48:00,550
Donc on pourrait supposer que,

2834
01:48:00,560 --> 01:48:02,550
lorsqu’on passera à un dirigeant IA,

2835
01:48:02,560 --> 01:48:03,590
l’objectif sera le même :

2836
01:48:03,600 --> 01:48:05,350
rendre notre pays prospère.

2837
01:48:05,360 --> 01:48:06,709
—

2838
01:48:06,719 --> 01:48:07,510
>> Exact. Exact.

2839
01:48:07,520 --> 01:48:08,790
>> Et c’est sans doute ce type d’IA

2840
01:48:08,800 --> 01:48:10,310
pour lequel les gens voteraient.

2841
01:48:10,320 --> 01:48:11,430
Ils diraient : « Nous voulons être prospères. »

2842
01:48:11,440 --> 01:48:12,790
>> Et selon toi, qu’est‑ce qui rendrait l’Amérique plus prospère ?

2843
01:48:12,800 --> 01:48:14,950
—

2844
01:48:14,960 --> 01:48:17,109
>> Dépenser mille milliards par an pour la guerre,

2845
01:48:17,119 --> 01:48:18,709
ou bien dépenser cette somme

2846
01:48:18,719 --> 01:48:21,030
dans l’éducation, la santé

2847
01:48:21,040 --> 01:48:23,910
et, tu vois,

2848
01:48:23,920 --> 01:48:26,790
l’aide aux pauvres et aux sans‑abris ?

2849
01:48:26,800 --> 01:48:31,510
— C’est complexe, mais je pense

2850
01:48:31,520 --> 01:48:33,830
que l’Amérique serait plus prospère

2851
01:48:33,840 --> 01:48:36,310
en prenant soin

2852
01:48:36,320 --> 01:48:39,030
de tout le monde ; et elle en a les moyens,

2853
01:48:39,040 --> 01:48:41,910
car c’est…

2854
01:48:41,920 --> 01:48:42,709
>> … la nation la plus puissante !

2855
01:48:42,719 --> 01:48:43,990
>> … la plus puissante du monde.

2856
01:48:44,000 --> 01:48:45,990
>> Non, ce n’est pas vrai. La raison, c’est que

2857
01:48:46,000 --> 01:48:49,109
toute guerre a deux objectifs :

2858
01:48:49,119 --> 01:48:51,750
générer de l’argent pour l’industrie militaire,

2859
01:48:51,760 --> 01:48:55,109
et exercer la dissuasion.

2860
01:48:55,119 --> 01:48:58,790
Il suffit de neuf puissances nucléaires

2861
01:48:58,800 --> 01:49:02,070
pour assurer la dissuasion mondiale.

2862
01:49:02,080 --> 01:49:04,229
Donc toute

2863
01:49:04,239 --> 01:49:07,109
guerre entre l’Amérique et la Chine

2864
01:49:07,119 --> 01:49:10,550
passerait d’abord par une longue phase de destruction

2865
01:49:10,560 --> 01:49:14,390
de richesses, de bombes qui explosent, d’humains qui meurent,

2866
01:49:14,400 --> 01:49:17,109
avant d’en arriver au premier objectif.

2867
01:49:17,119 --> 01:49:19,910
Puis, si on en vient vraiment à la dissuasion,

2868
01:49:19,920 --> 01:49:21,910
c’est avec les bombes nucléaires…

2869
01:49:21,920 --> 01:49:24,629
et désormais, à l’ère de l’IA,

2870
01:49:24,639 --> 01:49:27,669
les virus biologiques ou autres armes manufacturées.

2871
01:49:27,679 --> 01:49:31,189
—

2872
01:49:31,199 --> 01:49:34,070
C’est tout ce qu’il faut.

2873
01:49:34,080 --> 01:49:35,830
—

2874
01:49:35,840 --> 01:49:38,390
Ainsi, il suffit que la Chine ait suffisamment de bombes nucléaires,

2875
01:49:38,400 --> 01:49:41,510
pas autant que les États‑Unis, pour pouvoir dire :

2876
01:49:41,520 --> 01:49:44,550
« Ne viens pas me chercher. »

2877
01:49:44,560 --> 01:49:48,790
Et cela semble logique… Je ne sais pas,

2878
01:49:48,800 --> 01:49:52,070
je ne suis pas dans la tête du président Xi,

2879
01:49:52,080 --> 01:49:53,990
ni dans celle de Donald Trump,

2880
01:49:54,000 --> 01:49:55,990
c’est difficile de savoir ce qu’ils pensent.

2881
01:49:56,000 --> 01:49:58,550
Mais la vérité, c’est que

2882
01:49:58,560 --> 01:50:01,830
la Chine dit depuis trente ans :

2883
01:50:01,840 --> 01:50:04,709
« Vous avez dépensé dans la guerre

2884
01:50:04,719 --> 01:50:06,870
alors que nous, nous avons investi dans l’infrastructure. »

2885
01:50:06,880 --> 01:50:09,910
C’est pour cela que nous sommes désormais de loin

2886
01:50:09,920 --> 01:50:11,590
la plus grande nation du monde.

2887
01:50:11,600 --> 01:50:12,950
Même si l’Occident dira que l’Amérique est plus grande,

2888
01:50:12,960 --> 01:50:14,470
l’Amérique l’est en dollars,

2889
01:50:14,480 --> 01:50:16,709
mais en parité de pouvoir d’achat, c’est équivalent.

2890
01:50:16,719 --> 01:50:20,550
—

2891
01:50:20,560 --> 01:50:22,550
Donc, quand tu comprends cela,

2892
01:50:22,560 --> 01:50:25,910
tu vois que la prospérité ne réside pas dans la destruction.

2893
01:50:25,920 --> 01:50:28,229
Par définition, c’est l’inverse.

2894
01:50:28,239 --> 01:50:31,430
La prospérité consiste à investir dans son peuple

2895
01:50:31,440 --> 01:50:35,510
et à veiller à sa sécurité.

2896
01:50:35,520 --> 01:50:38,310
Et pour garantir cette sécurité,

2897
01:50:38,320 --> 01:50:40,950
il suffit d’agiter le drapeau :

2898
01:50:40,960 --> 01:50:44,470
« Si tu m’attaques, j’ai ma dissuasion nucléaire

2899
01:50:44,480 --> 01:50:46,229
ou d’autres formes de dissuasion. »

2900
01:50:46,239 --> 01:50:48,310
Mais tu n’as pas à t’en servir.

2901
01:50:48,320 --> 01:50:50,950
La dissuasion, par définition, n’implique pas

2902
01:50:50,960 --> 01:50:52,629
d’envoyer des soldats mourir.

2903
01:50:52,639 --> 01:50:54,310
La question que j’essayais de soulever était donc :

2904
01:50:54,320 --> 01:50:57,669
quand nous aurons ces dirigeants IA

2905
01:50:57,679 --> 01:50:59,910
et que nous leur demanderons d’assurer la prospérité,

2906
01:50:59,920 --> 01:51:02,870
ne rejoueront‑ils pas les mêmes jeux ?

2907
01:51:02,880 --> 01:51:07,270
À savoir : prospérité égale économie plus grande,

2908
01:51:07,280 --> 01:51:09,430
plus d’argent, plus de richesses pour nous,

2909
01:51:09,440 --> 01:51:12,149
—

2910
01:51:12,159 --> 01:51:14,149
et que, dans un monde à somme nulle, pour y parvenir,

2911
01:51:14,159 --> 01:51:15,350
il faut accumuler aux dépens des autres.

2912
01:51:15,360 --> 01:51:19,030
—

2913
01:51:19,040 --> 01:51:20,470
>> Alors cherche donc le sens du mot “prospérité”.

2914
01:51:20,480 --> 01:51:22,310
Qu’est‑ce que c’est ?

2915
01:51:22,320 --> 01:51:24,310
>> Ce n’est pas ce que tu viens de décrire.

2916
01:51:24,320 --> 01:51:25,510
>> Je ne sais même pas ce que ce fichu mot veut vraiment dire.

2917
01:51:25,520 --> 01:51:28,310
Quelle est la définition de

2918
01:51:28,320 --> 01:51:31,590
la prospérité ?

2919
01:51:34,310 --> 01:51:34,320
>> La prospérité, c’est un état de succès florissant et de bonne fortune,

2920
01:51:34,320 --> 01:51:36,950
surtout en matière de richesse, de santé

2921
01:51:36,960 --> 01:51:39,030
et de bien‑être général.

2922
01:51:39,040 --> 01:51:40,629
—

2923
01:51:40,639 --> 01:51:41,270
>> Bien.

2924
01:51:41,280 --> 01:51:43,910
>> Santé économique, sociale, émotionnelle.

2925
01:51:43,920 --> 01:51:44,950
>> Bien.

2926
01:51:44,960 --> 01:51:45,430
>> Donc,

2927
01:51:45,440 --> 01:51:47,270
>> la vraie prospérité, c’est que tout le monde sur Terre ait cela.

2928
01:51:47,280 --> 01:51:48,950
Si tu veux la maximiser,

2929
01:51:48,960 --> 01:51:51,270
tu dois l’assurer à tous.

2930
01:51:51,280 --> 01:51:52,149
—

2931
01:51:52,159 --> 01:51:53,910
>> Tu sais à quoi je pense ? Un dirigeant IA mondial

2932
01:51:53,920 --> 01:51:56,870
pourrait fonctionner si nous lui donnions

2933
01:51:56,880 --> 01:51:58,470
l’ordre suivant :

2934
01:51:58,480 --> 01:51:59,910
>> Et c’est exactement ce qui va arriver.

2935
01:51:59,920 --> 01:52:00,310
—

2936
01:52:00,320 --> 01:52:02,070
>> « Prospérité pour le monde entier. »

2937
01:52:02,080 --> 01:52:03,669
>> Non, mais c’est une question très intéressante.

2938
01:52:03,679 --> 01:52:05,510
Une de mes prévisions, dont on parle rarement,

2939
01:52:05,520 --> 01:52:07,910
est que nous pensons avoir des IA concurrentes…

2940
01:52:07,920 --> 01:52:10,070
—

2941
01:52:10,080 --> 01:52:11,669
…alors que, selon moi, nous finirons avec un seul cerveau commun.

2942
01:52:11,679 --> 01:52:12,390
>> Oui.

2943
01:52:12,400 --> 01:52:14,950
>> Je crois que nous aurons un seul cerveau mondial.

2944
01:52:14,960 --> 01:52:16,790
>> D’accord. Donc tu comprends l’argument que je faisais à l’instant :

2945
01:52:16,800 --> 01:52:18,070
si chaque pays a sa propre IA dirigeante,

2946
01:52:18,080 --> 01:52:19,990
on retombera dans la même cupidité.

2947
01:52:20,000 --> 01:52:21,270
—

2948
01:52:21,280 --> 01:52:22,470
—

2949
01:52:22,480 --> 01:52:23,189
Oui, dans la même logique d’avidité.

2950
01:52:23,199 --> 01:52:26,310
>> Mais si le monde n’avait qu’un seul dirigeant IA,

2951
01:52:26,320 --> 01:52:27,750
>> et qu’on lui confiait la mission

2952
01:52:27,760 --> 01:52:30,310
« Rends les humains prospères et sauve la planète »,

2953
01:52:30,320 --> 01:52:30,870
—

2954
01:52:30,880 --> 01:52:32,070
>> alors les ours polaires iraient très bien.

2955
01:52:32,080 --> 01:52:33,830
>> Cent pour cent. Et c’est ce que je prône depuis un an et demi :

2956
01:52:33,840 --> 01:52:35,910
—

2957
01:52:35,920 --> 01:52:38,149
il nous faut un CERN de l’IA.

2958
01:52:38,159 --> 01:52:38,870
—

2959
01:52:38,880 --> 01:52:40,629
>> Que veux‑tu dire par là ? Comme le Centre européen des particules,

2960
01:52:40,639 --> 01:52:42,470
où le monde entier a uni ses efforts

2961
01:52:42,480 --> 01:52:45,109
pour comprendre la physique, sans compétition.

2962
01:52:45,119 --> 01:52:47,189
—

2963
01:52:47,199 --> 01:52:50,310
Une prospérité mutuelle assurée.

2964
01:52:50,320 --> 01:52:52,470
Je demande au monde, aux gouvernements

2965
01:52:52,480 --> 01:52:54,470
comme Abou Dabi ou l’Arabie Saoudite,

2966
01:52:54,480 --> 01:52:56,709
qui disposent de grandes infrastructures d’IA,

2967
01:52:56,719 --> 01:52:59,430
—

2968
01:52:59,440 --> 01:53:01,750
de réunir tous les scientifiques de l’IA

2969
01:53:01,760 --> 01:53:04,470
pour qu’ils construisent une IA pour le monde.

2970
01:53:04,480 --> 01:53:06,390
—

2971
01:53:06,400 --> 01:53:09,030
Et l’on doit comprendre que nous nous accrochons à un système capitaliste

2972
01:53:09,040 --> 01:53:12,470
qui finira tôt ou tard par s’effondrer.

2973
01:53:12,480 --> 01:53:16,149
—

2974
01:53:16,159 --> 01:53:18,629
Puisqu’il s’écroulera, autant le transformer à notre manière.

2975
01:53:18,639 --> 01:53:20,870
—

2976
01:53:20,880 --> 01:53:22,470
>> Je crois qu’on a trouvé la solution, mon ami.

2977
01:53:22,480 --> 01:53:24,790
>> Je pense que c’est réellement possible.

2978
01:53:24,800 --> 01:53:27,750
En fait, je ne peux pas réfuter l’idée que,

2979
01:53:27,760 --> 01:53:30,950
si nous avions une IA responsable

2980
01:53:30,960 --> 01:53:34,629
qui gouverne le monde entier

2981
01:53:34,639 --> 01:53:36,550
et qu’on lui donne pour mission

2982
01:53:36,560 --> 01:53:38,229
de rendre les humains prospères, en bonne santé et heureux,

2983
01:53:38,239 --> 01:53:41,510
—

2984
01:53:41,520 --> 01:53:43,510
à condition que cette mission soit claire.

2985
01:53:43,520 --> 01:53:45,990
>> Oui.

2986
01:53:46,000 --> 01:53:47,430
>> Parce qu’il y a toujours de fichues conséquences imprévues.

2987
01:53:47,440 --> 01:53:48,629
—

2988
01:53:48,639 --> 01:53:50,550
>> Donc le seul vrai défi, ce sera

2989
01:53:50,560 --> 01:53:53,109
de convaincre ceux qui aujourd’hui

2990
01:53:53,119 --> 01:53:56,390
sont des multimilliardaires

2991
01:53:56,400 --> 01:53:58,629
ou d’immenses puissants, dictateurs, etc.,

2992
01:53:58,639 --> 01:54:01,589
—

2993
01:54:01,599 --> 01:54:04,390
de renoncer à leur pouvoir ?

2994
01:54:04,400 --> 01:54:06,390
Comment leur faire accepter que, bon,

2995
01:54:06,400 --> 01:54:08,070
—

2996
01:54:08,080 --> 01:54:09,910
tu veux n’importe quelle voiture ? Tu l’auras.

2997
01:54:09,920 --> 01:54:11,350
Tu veux un autre yacht ? On t’en donnera un autre.

2998
01:54:11,360 --> 01:54:13,109
On te donnera tout ce que tu veux,

2999
01:54:13,119 --> 01:54:14,950
mais arrête de nuire aux autres.

3000
01:54:14,960 --> 01:54:17,510
Il n’y a plus besoin d’exploitation ni d’arbitrage.

3001
01:54:17,520 --> 01:54:19,270
—

3002
01:54:19,280 --> 01:54:21,910
Il n’est plus nécessaire que d’autres perdent

3003
01:54:21,920 --> 01:54:23,669
pour que les capitalistes gagnent.

3004
01:54:23,679 --> 01:54:25,830
>> D’accord ? Et dans un tel monde, avec un dirigeant IA

3005
01:54:25,840 --> 01:54:27,270
chargé de rendre le monde prospère,

3006
01:54:27,280 --> 01:54:28,950
—

3007
01:54:28,960 --> 01:54:31,910
le milliardaire avec son yacht

3008
01:54:31,920 --> 01:54:33,430
devrait peut‑être s’en séparer.

3009
01:54:33,440 --> 01:54:34,149
—

3010
01:54:34,159 --> 01:54:34,950
>> Non. Non !

3011
01:54:34,960 --> 01:54:36,229
>> Donnons‑leur plus de yachts !

3012
01:54:36,239 --> 01:54:36,629
>> D’accord.

3013
01:54:36,639 --> 01:54:38,229
>> Construire des yachts ne coûte plus rien quand

3014
01:54:38,239 --> 01:54:40,950
les robots fabriquent tout. Donc la complexité de tout cela est fascinante :

3015
01:54:40,960 --> 01:54:44,550
un monde où tout fabriquer ne coûte rien,

3016
01:54:44,560 --> 01:54:47,350
—

3017
01:54:47,360 --> 01:54:49,430
—

3018
01:54:49,440 --> 01:54:51,030
>> parce que l’énergie est abondante,

3019
01:54:51,040 --> 01:54:53,350
>> et elle est abondante parce que chaque problème

3020
01:54:53,360 --> 01:54:56,470
est résolu par une intelligence colossale.

3021
01:54:56,480 --> 01:54:58,550
Parce que la fabrication se fera à l’échelle nanophysique,

3022
01:54:58,560 --> 01:55:01,589
plus par assemblage de composants.

3023
01:55:01,599 --> 01:55:04,950
Parce que la mécanique sera robotique :

3024
01:55:04,960 --> 01:55:06,790
tu amènes ta voiture, un robot l’examine et la répare.

3025
01:55:06,800 --> 01:55:08,950
Ça te coûte à peine quelques centimes d’énergie,

3026
01:55:08,960 --> 01:55:11,830
une énergie gratuite en plus.

3027
01:55:11,840 --> 01:55:14,470
—

3028
01:55:14,480 --> 01:55:17,510
Imagine un monde où l’intelligence crée tout.

3029
01:55:17,520 --> 01:55:19,990
—

3030
01:55:20,000 --> 01:55:22,629
Dans ce monde‑là,

3031
01:55:22,639 --> 01:55:26,310
chaque être humain obtient tout ce qu’il désire.

3032
01:55:26,320 --> 01:55:27,830
Mais nous ne choisirons pas ce monde‑là.

3033
01:55:27,840 --> 01:55:31,589
—

3034
01:55:33,510 --> 01:55:33,520
>> Imagine que tu vis dans un monde, et c’est un exercice très intéressant :

3035
01:55:33,520 --> 01:55:34,870
—

3036
01:55:34,880 --> 01:55:37,830
imagine que le revenu universel

3037
01:55:37,840 --> 01:55:40,470
devienne très coûteux pour les gouvernements.

3038
01:55:40,480 --> 01:55:43,430
Alors ils décident de mettre tout le monde

3039
01:55:43,440 --> 01:55:47,270
dans une pièce de un mètre sur trois,

3040
01:55:47,280 --> 01:55:49,669
de leur donner un casque et un sédatif,

3041
01:55:49,679 --> 01:55:51,910
—

3042
01:55:51,920 --> 01:55:54,950
et de les laisser dormir

3043
01:55:54,960 --> 01:55:59,589
23 heures sur 24.

3044
01:55:59,599 --> 01:56:01,510
Pendant ce temps‑là, ils vivront

3045
01:56:01,520 --> 01:56:04,070
une vie entière

3046
01:56:04,080 --> 01:56:06,390
dans ce monde virtuel

3047
01:56:06,400 --> 01:56:08,070
à la vitesse du cerveau pendant le sommeil,

3048
01:56:08,080 --> 01:56:10,070
où ils connaîtront mille existences :

3049
01:56:10,080 --> 01:56:11,990
une où ils sortent avec Scarlett Johansson,

3050
01:56:12,000 --> 01:56:13,350
une autre où ils sont Néfertiti,

3051
01:56:13,360 --> 01:56:15,430
et encore une autre où ils sont un âne –

3052
01:56:15,440 --> 01:56:18,470
une vraie réincarnation dans un monde virtuel.

3053
01:56:18,480 --> 01:56:21,270
—

3054
01:56:21,280 --> 01:56:23,669
Et puis, moi, j’en aurai une autre où je revois Hannah,

3055
01:56:23,679 --> 01:56:26,229
et je profiterai pleinement de cette vie‑là.

3056
01:56:26,239 --> 01:56:28,629
—

3057
01:56:28,639 --> 01:56:31,430
Et tout cela, pour un coût nul.

3058
01:56:31,440 --> 01:56:34,229
Tu te réveilles une heure, tu marches un peu,

3059
01:56:34,239 --> 01:56:37,350
tu fais circuler ton sang, tu manges quelque chose ou non,

3060
01:56:37,360 --> 01:56:39,189
puis tu remets le casque et tu revis une autre vie.

3061
01:56:39,199 --> 01:56:41,430
—

3062
01:56:41,440 --> 01:56:45,910
C’est impensable ?

3063
01:56:48,390 --> 01:56:48,400
>> C’est glauque, comparé à notre vie actuelle. Mais tout à fait faisable.

3064
01:56:48,400 --> 01:56:50,310
—

3065
01:56:50,320 --> 01:56:52,149
>> Quoi ? Qu’on vive tous avec un casque ?

3066
01:56:52,159 --> 01:56:53,990
>> Comment sais‑tu que tu n’y es pas déjà ?

3067
01:56:54,000 --> 01:56:55,430
>> Je ne le sais pas, non.

3068
01:56:55,440 --> 01:56:57,589
>> Exactement. Tu n’en as aucune idée.

3069
01:56:57,599 --> 01:57:00,070
Au fond, chaque expérience que tu vis

3070
01:57:00,080 --> 01:57:02,550
n’est qu’un signal électrique dans ton cerveau.

3071
01:57:02,560 --> 01:57:05,830
—

3072
01:57:05,840 --> 01:57:07,990
D’accord.

3073
01:57:08,000 --> 01:57:10,709
Maintenant, demande‑toi : si on peut créer cela virtuellement,

3074
01:57:10,719 --> 01:57:13,270
—

3075
01:57:13,280 --> 01:57:15,669
pourquoi serait‑ce un mal si on le crée dans le monde physique ?

3076
01:57:15,679 --> 01:57:17,750
—

3077
01:57:17,760 --> 01:57:19,990
>> Peut‑être qu’on l’a déjà fait.

3078
01:57:20,000 --> 01:57:22,870
>> Ma théorie c’est que, oui, à 98 %, on l’a fait. Mais ce n’est qu’une hypothèse.

3079
01:57:22,880 --> 01:57:25,030
Ce n’est pas de la science.

3080
01:57:25,040 --> 01:57:26,229
>> Tu penses que

3081
01:57:26,239 --> 01:57:28,390
>> à cent pour cent, oui.

3082
01:57:28,400 --> 01:57:29,669
>> Tu penses qu’on a déjà créé cela

3083
01:57:29,679 --> 01:57:30,790
et que c’est ici, maintenant ?

3084
01:57:30,800 --> 01:57:33,910
>> Oui. Je pense que c’est ça. Pense au principe d’incertitude

3085
01:57:33,920 --> 01:57:35,589
de la physique quantique :

3086
01:57:35,599 --> 01:57:38,709
ce qu’on observe

3087
01:57:38,719 --> 01:57:41,510
fait s’effondrer la fonction d’onde

3088
01:57:41,520 --> 01:57:43,430
et devient réalité.

3089
01:57:43,440 --> 01:57:45,109
Exact.

3090
01:57:45,119 --> 01:57:46,550
>> Je ne connais rien à la physique, alors…

3091
01:57:46,560 --> 01:57:46,950
—

3092
01:57:46,960 --> 01:57:48,709
>> …la physique quantique dit simplement que tout existe

3093
01:57:48,719 --> 01:57:50,229
en superposition.

3094
01:57:50,239 --> 01:57:52,229
—

3095
01:57:52,239 --> 01:57:55,830
C’est‑à‑dire que chaque particule subatomique

3096
01:57:55,840 --> 01:57:57,750
peut exister n’importe où, à n’importe quel moment,

3097
01:57:57,760 --> 01:58:00,070
et qu’au moment où un observateur la regarde,

3098
01:58:00,080 --> 01:58:02,149
elle “s’effondre” et devient ce qu’elle est. Exactement

3099
01:58:02,159 --> 01:58:06,310
comme dans les jeux vidéo.

3100
01:58:06,320 --> 01:58:08,149
Dans un jeu vidéo, tout l’univers du jeu

3101
01:58:08,159 --> 01:58:10,709
se trouve sur le disque dur,

3102
01:58:10,719 --> 01:58:13,910
et quand le joueur tourne à droite,

3103
01:58:13,920 --> 01:58:16,790
c’est cette partie de la carte qui s’affiche –

3104
01:58:16,800 --> 01:58:18,629
le reste reste “en superposition”.

3105
01:58:18,639 --> 01:58:19,990
—

3106
01:58:20,000 --> 01:58:21,030
—

3107
01:58:21,040 --> 01:58:22,229
>> “Superposition”, c’est‑à‑dire

3108
01:58:22,239 --> 01:58:24,229
que c’est là, prêt à être rendu visible,

3109
01:58:24,239 --> 01:58:26,550
mais qu’il faut l’observer pour qu’il apparaisse.

3110
01:58:26,560 --> 01:58:28,390
Le joueur doit tourner la tête pour que le décor se crée.

3111
01:58:28,400 --> 01:58:32,390
—

3112
01:58:32,400 --> 01:58:35,270
Réfléchis à cette vérité de la physique :

3113
01:58:35,280 --> 01:58:38,629
le monde matériel est presque entièrement vide.

3114
01:58:38,639 --> 01:58:42,310
Il n’y a que de minuscules particules,

3115
01:58:42,320 --> 01:58:46,070
quasiment sans masse,

3116
01:58:46,080 --> 01:58:49,189
mais reliées par assez d’énergie pour que mon doigt

3117
01:58:49,199 --> 01:58:50,790
ne traverse pas ma main.

3118
01:58:50,800 --> 01:58:53,910
—

3119
01:58:53,920 --> 01:58:55,430
>> Ta main contre ton doigt.

3120
01:58:55,440 --> 01:58:57,109
>> Oui. Quand je les frappe l’une contre l’autre,

3121
01:58:57,119 --> 01:58:59,750
je ressens la sensation dans mon cerveau.

3122
01:58:59,760 --> 01:59:02,629
C’est un signal électrique

3123
01:59:02,639 --> 01:59:04,390
qui circule dans mes nerfs.

3124
01:59:04,400 --> 01:59:08,070
Il n’y a aucun moyen de distinguer ce signal

3125
01:59:08,080 --> 01:59:10,149
d’un signal venu d’un implant neuronal,

3126
01:59:10,159 --> 01:59:13,189
type interface cerveau‑ordinateur.

3127
01:59:13,199 --> 01:59:16,550
—

3128
01:59:16,560 --> 01:59:19,510
Donc oui, beaucoup de ces choses sont tout à fait possibles.

3129
01:59:19,520 --> 01:59:21,350
—

3130
01:59:21,360 --> 01:59:24,709
Mais en vérité, la majorité du monde n’est pas physique.

3131
01:59:24,719 --> 01:59:27,430
La majeure partie du monde se joue dans notre imagination,

3132
01:59:27,440 --> 01:59:29,830
dans nos processeurs mentaux.

3133
01:59:29,840 --> 01:59:30,550
—

3134
01:59:30,560 --> 01:59:31,669
>> Et finalement, j’imagine que ça n’a pas tant d’importance.

3135
01:59:31,679 --> 01:59:33,189
Notre réalité

3136
01:59:33,199 --> 01:59:34,790
>> pas du tout. Et c’est ça qui est intéressant :

3137
01:59:34,800 --> 01:59:36,470
ce qui est intéressant, c’est que

3138
01:59:36,480 --> 01:59:37,910
—

3139
01:59:37,920 --> 01:59:39,430
>> même si c’est un jeu vidéo, nous en subissons les conséquences.

3140
01:59:39,440 --> 01:59:40,790
—

3141
01:59:40,800 --> 01:59:42,229
>> Oui. C’est ton expérience subjective.

3142
01:59:42,239 --> 01:59:42,870
—

3143
01:59:42,880 --> 01:59:44,550
>> Oui. Et ici il y a des conséquences :

3144
01:59:44,560 --> 01:59:46,470
je n’aime pas la douleur,

3145
01:59:46,480 --> 01:59:47,109
>> Exact.

3146
01:59:47,119 --> 01:59:50,070
>> Et j’aime le plaisir. Tu vois, tu joues selon les règles du jeu.

3147
01:59:50,080 --> 01:59:51,430
—

3148
01:59:51,440 --> 01:59:52,950
>> Oui. Et c’est fascinant. Et si on revient à la discussion,

3149
01:59:52,960 --> 01:59:54,470
—

3150
01:59:54,480 --> 01:59:56,229
l’élément le plus intéressant, c’est :

3151
01:59:56,239 --> 01:59:58,550
si je ne suis pas cet avatar,

3152
01:59:58,560 --> 02:00:00,870
si je ne suis pas cette forme physique –

3153
02:00:00,880 --> 02:00:04,790
si je suis la conscience qui porte le casque –

3154
02:00:04,800 --> 02:00:06,629
—

3155
02:00:06,639 --> 02:00:08,870
—

3156
02:00:08,880 --> 02:00:11,270
alors en quoi devrais‑je investir ? Dans le jeu vidéo, ce niveau‑ci ?

3157
02:00:11,280 --> 02:00:13,830
Ou dans le vrai joueur que je suis,

3158
02:00:13,840 --> 02:00:15,270
dans la conscience elle‑même ?

3159
02:00:15,280 --> 02:00:17,589
—

3160
02:00:17,599 --> 02:00:19,830
—

3161
02:00:19,840 --> 02:00:23,109
— l’esprit, si tu es croyant.

3162
02:00:23,119 --> 02:00:24,950
>> Mais comment investir dans la conscience,

3163
02:00:24,960 --> 02:00:27,189
ou dans Dieu, l’esprit, peu importe ?

3164
02:00:27,199 --> 02:00:29,589
Comment ? Si je jouais à *Grand Theft Auto*,

3165
02:00:29,599 --> 02:00:31,350
le personnage du jeu ne pourrait pas investir

3166
02:00:31,360 --> 02:00:32,950
dans moi, le joueur qui tient la manette.

3167
02:00:32,960 --> 02:00:34,709
—

3168
02:00:34,719 --> 02:00:36,709
>> Oui, mais toi, tu peux investir dans toi‑même

3169
02:00:36,719 --> 02:00:40,070
— celui qui tient la manette.

3170
02:00:40,080 --> 02:00:43,910
— D’accord, donc tu dis que

3171
02:00:43,920 --> 02:00:46,070
Moa est en fait la conscience. Et donc,

3172
02:00:46,080 --> 02:00:47,350
comment la conscience investit‑elle en elle‑même ?

3173
02:00:47,360 --> 02:00:48,390
—

3174
02:00:48,400 --> 02:00:51,189
>> En devenant plus consciente. —

3175
02:00:51,199 --> 02:00:52,229
>> De sa propre conscience.

3176
02:00:52,239 --> 02:00:54,470
>> Oui. Les vrais joueurs, ceux qui jouent sérieusement,

3177
02:00:54,480 --> 02:00:57,510
ne cherchent pas à finir le niveau.

3178
02:00:57,520 --> 02:01:00,229
Ils ne veulent pas simplement “gagner”.

3179
02:01:00,239 --> 02:01:02,070
Les vrais joueurs n’ont qu’un objectif :

3180
02:01:02,080 --> 02:01:03,669
devenir de meilleurs joueurs.

3181
02:01:03,679 --> 02:01:07,589
—

3182
02:01:10,229 --> 02:01:10,239
Vois à quel point je prends ça au sérieux.

3183
02:01:10,239 --> 02:01:12,470
Je joue à *Halo* : un ou deux joueurs sur un million peuvent me battre.

3184
02:01:12,480 --> 02:01:14,470
—

3185
02:01:14,480 --> 02:01:17,589
C’est à peu près mon classement.

3186
02:01:17,599 --> 02:01:21,189
Pas mal pour mon âge, non ?

3187
02:01:21,199 --> 02:01:23,350
Mais sérieusement, c’est parce que

3188
02:01:23,360 --> 02:01:26,070
je ne “joue” pas, je m’entraîne.

3189
02:01:26,080 --> 02:01:28,709
45 minutes par jour, quatre fois par semaine,

3190
02:01:28,719 --> 02:01:30,550
quand je ne voyage pas. Et je m’exerce avec un seul but :

3191
02:01:30,560 --> 02:01:32,390
devenir un meilleur joueur.

3192
02:01:32,400 --> 02:01:33,830
—

3193
02:01:33,840 --> 02:01:35,510
>> Le coup que je fais m’importe peu.

3194
02:01:35,520 --> 02:01:37,510
Peu importe ce qui se passe dans la partie.

3195
02:01:37,520 --> 02:01:40,870
Je cherche juste à améliorer mes réflexes,

3196
02:01:40,880 --> 02:01:43,350
mon état de flow, pour être meilleur.

3197
02:01:43,360 --> 02:01:45,270
Je veux devenir un meilleur joueur,

3198
02:01:45,280 --> 02:01:47,189
ce qui veut dire comprendre le jeu,

3199
02:01:47,199 --> 02:01:49,589
l’analyser, remettre en question ma façon de jouer,

3200
02:01:49,599 --> 02:01:51,510
mes compétences, mes croyances,

3201
02:01:51,520 --> 02:01:53,270
ma compréhension des choses.

3202
02:01:53,280 --> 02:01:55,750
—

3203
02:01:55,760 --> 02:01:58,390
Et c’est ainsi que la conscience investit en elle‑même,

3204
02:01:58,400 --> 02:02:00,470
et non dans l’avatar.

3205
02:02:00,480 --> 02:02:02,470
—

3206
02:02:02,480 --> 02:02:04,870
Car si tu deviens ce joueur‑là,

3207
02:02:04,880 --> 02:02:09,109
le prochain avatar, le prochain niveau,

3208
02:02:09,119 --> 02:02:11,270
sont plus faciles,

3209
02:02:11,280 --> 02:02:14,310
parce que tu es devenu meilleur.

3210
02:02:14,320 --> 02:02:16,229
>> D’accord. Donc tu penses que la conscience

3211
02:02:16,239 --> 02:02:19,990
se sert de nous comme véhicule pour s’améliorer ?

3212
02:02:20,000 --> 02:02:23,510
>> Si l’hypothèse est vraie – et ce n’est qu’une hypothèse –

3213
02:02:23,520 --> 02:02:25,189
on ne sait pas si c’est vrai.

3214
02:02:25,199 --> 02:02:28,470
Mais si c’est réellement une simulation,

3215
02:02:28,480 --> 02:02:31,510
alors, selon la définition religieuse,

3216
02:02:31,520 --> 02:02:36,229
Dieu met une part de son âme dans chaque humain

3217
02:02:36,239 --> 02:02:40,070
et cette parcelle devient consciente.

3218
02:02:40,080 --> 02:02:42,629
—

3219
02:02:42,639 --> 02:02:45,109
Donc tu deviens vivant, conscient.

3220
02:02:45,119 --> 02:02:46,550
Si tu ne veux pas parler en termes religieux,

3221
02:02:46,560 --> 02:02:49,510
disons que la conscience universelle se divise

3222
02:02:49,520 --> 02:02:51,910
en parts d’elle‑même

3223
02:02:51,920 --> 02:02:54,870
pour vivre plusieurs expériences, interagir, se confronter,

3224
02:02:54,880 --> 02:02:56,470
se battre, aimer,

3225
02:02:56,480 --> 02:02:57,589
>> comprendre,

3226
02:02:57,599 --> 02:03:00,310
>> puis s’affiner. Un physicien m’a dit récemment

3227
02:03:00,320 --> 02:03:01,510
que c’est peut‑être le but même de la conscience :

3228
02:03:01,520 --> 02:03:03,350
s’utiliser elle‑même comme vecteur

3229
02:03:03,360 --> 02:03:05,030
pour mieux se comprendre

3230
02:03:05,040 --> 02:03:06,550
et observer la réalité à travers nos yeux.

3231
02:03:06,560 --> 02:03:08,229
—

3232
02:03:08,239 --> 02:03:10,310
>> Pour s’observer et se comprendre – ce qui est assez…

3233
02:03:10,320 --> 02:03:11,189
troublant.

3234
02:03:11,199 --> 02:03:12,950
>> Donc, si tu prends certaines définitions religieuses

3235
02:03:12,960 --> 02:03:14,950
les plus fascinantes du paradis et de l’enfer,

3236
02:03:14,960 --> 02:03:17,270
par exemple,

3237
02:03:17,280 --> 02:03:21,350
où le paradis, c’est obtenir tout ce que tu désires,

3238
02:03:21,360 --> 02:03:24,550
la puissance de Dieu qui accorde tout ce que tu veux –

3239
02:03:24,560 --> 02:03:26,470
—

3240
02:03:26,480 --> 02:03:28,870
si tu creuses cette idée,

3241
02:03:28,880 --> 02:03:31,990
elle signifie que cette goutte de conscience

3242
02:03:32,000 --> 02:03:33,910
qu’est “toi” retourne à la source,

3243
02:03:33,920 --> 02:03:36,709
et que la source peut créer tout ce qu’elle souhaite.

3244
02:03:36,719 --> 02:03:38,870
—

3245
02:03:38,880 --> 02:03:40,470
C’est ça, ton paradis.

3246
02:03:40,480 --> 02:03:43,030
—

3247
02:03:43,040 --> 02:03:44,950
Et, de manière intéressante,

3248
02:03:44,960 --> 02:03:48,870
si ce retour se fait en séparant ton bien de ton mal,

3249
02:03:48,880 --> 02:03:51,270
alors la source revient plus pure.

3250
02:03:51,280 --> 02:03:53,030
C’est exactement ça :

3251
02:03:53,040 --> 02:03:56,149
la conscience qui se divise pour faire l’expérience,

3252
02:03:56,159 --> 02:03:57,750
puis s’élever, élevant toute la conscience universelle.

3253
02:03:57,760 --> 02:04:01,910
—

3254
02:04:01,920 --> 02:04:03,669
—

3255
02:04:03,679 --> 02:04:06,629
— tout cela reste des hypothèses, évidemment.

3256
02:04:06,639 --> 02:04:08,709
Aucune preuve scientifique n’existe,

3257
02:04:08,719 --> 02:04:11,109
mais c’est un exercice mental fascinant.

3258
02:04:11,119 --> 02:04:13,109
—

3259
02:04:13,119 --> 02:04:15,189
Beaucoup de chercheurs en IA te diront

3260
02:04:15,199 --> 02:04:18,790
que ce que l’on constate avec la technologie,

3261
02:04:18,800 --> 02:04:20,550
c’est que si quelque chose est possible,

3262
02:04:20,560 --> 02:04:23,189
il finit probablement par arriver.

3263
02:04:23,199 --> 02:04:24,629
>> Si c’est possible de miniaturiser quelque chose

3264
02:04:24,639 --> 02:04:26,310
pour le faire tenir dans un téléphone,

3265
02:04:26,320 --> 02:04:28,790
alors tôt ou tard, la technologie y parviendra.

3266
02:04:28,800 --> 02:04:31,750
—

3267
02:04:31,760 --> 02:04:34,870
Et crois‑le ou non, à mes yeux,

3268
02:04:34,880 --> 02:04:37,189
c’est la manière la plus humaine de gérer

3269
02:04:37,199 --> 02:04:38,870
le revenu universel.

3270
02:04:38,880 --> 02:04:39,990
>> Comment ça ?

3271
02:04:40,000 --> 02:04:42,149
>> La manière la plus humaine de vivre avec un revenu universel,

3272
02:04:42,159 --> 02:04:45,350
quand les gens comme toi, incapables de créer des entreprises réelles,

3273
02:04:45,360 --> 02:04:47,189
en souffrent, c’est de leur donner un casque

3274
02:04:47,199 --> 02:04:49,109
et de les laisser bâtir autant d’entreprises qu’ils veulent

3275
02:04:49,119 --> 02:04:50,870
dans le virtuel.

3276
02:04:50,880 --> 02:04:54,470
—

3277
02:04:54,480 --> 02:04:56,550
Niveau après niveau, nuit après nuit.

3278
02:04:56,560 --> 02:04:59,109
—

3279
02:04:59,119 --> 02:05:01,109
On te maintient en vie : c’est très respectueux, très humain.

3280
02:05:01,119 --> 02:05:03,669
—

3281
02:05:03,679 --> 02:05:06,310
Et, mieux encore, on ne force personne à le faire.

3282
02:05:06,320 --> 02:05:08,470
Il y aura sans doute quelques-uns d’entre nous

3283
02:05:08,480 --> 02:05:12,629
qui continueront à errer dans la jungle,

3284
02:05:14,550 --> 02:05:14,560
mais la plupart diront :

3285
02:05:14,560 --> 02:05:17,270
« À 70 ans, le dos et les jambes douloureux,

3286
02:05:17,280 --> 02:05:19,189
pourquoi ne pas reprendre cinq années de plus là‑dedans ? »

3287
02:05:19,199 --> 02:05:21,589
—

3288
02:05:21,599 --> 02:05:24,470
Oui, donne‑m’en encore cinq !

3289
02:05:24,480 --> 02:05:26,629
Pourquoi pas ?

3290
02:05:26,639 --> 02:05:29,910
— C’est étrange, vraiment.

3291
02:05:29,920 --> 02:05:31,750
Le nombre de questions que tout cela soulève

3292
02:05:31,760 --> 02:05:35,189
est énorme.

3293
02:05:35,199 --> 02:05:37,109
La solution moins humaine, pour finir sur une note grinçante,

3294
02:05:37,119 --> 02:05:40,070
serait simplement

3295
02:05:40,080 --> 02:05:43,030
de déclencher assez de guerres

3296
02:05:43,040 --> 02:05:46,070
pour réduire la population vivant de ce revenu.

3297
02:05:46,080 --> 02:05:48,790
Et si le monde est dirigé par un “État profond” tout‑puissant,

3298
02:05:48,800 --> 02:05:51,350
il faut imaginer qu’ils pourraient envisager cela.

3299
02:05:51,360 --> 02:05:56,550
—

3300
02:05:57,669 --> 02:05:57,679
les “mangeurs”…

3301
02:05:57,679 --> 02:05:59,350
>> Que suis‑je censé faire, moi, face à tout ça ?

3302
02:05:59,360 --> 02:05:59,910
>> À propos de…

3303
02:05:59,920 --> 02:06:03,350
>> …tout ce que tu viens de dire ?

3304
02:06:03,360 --> 02:06:07,189
>> Eh bien, je crois toujours que le monde dans lequel on vit

3305
02:06:07,199 --> 02:06:11,510
demande quatre compétences essentielles.

3306
02:06:11,520 --> 02:06:15,109
La première, c’est d’apprendre à utiliser l’outil :

3307
02:06:15,119 --> 02:06:17,589
comprendre, se connecter à l’IA,

3308
02:06:17,599 --> 02:06:21,350
et s’y exposer pour qu’elle voie le bon côté de l’humanité.

3309
02:06:21,360 --> 02:06:24,790
—

3310
02:06:24,800 --> 02:06:27,750
—

3311
02:06:27,760 --> 02:06:31,589
La deuxième, c’est ce que j’appelle la connexion :

3312
02:06:31,599 --> 02:06:34,149
je crois que la compétence la plus précieuse dans les dix prochaines années,

3313
02:06:34,159 --> 02:06:36,629
c’est la capacité des humains à tisser de vraies relations.

3314
02:06:36,639 --> 02:06:39,510
—

3315
02:06:39,520 --> 02:06:41,750
Apprendre à aimer sincèrement,

3316
02:06:41,760 --> 02:06:43,030
apprendre la compassion,

3317
02:06:43,040 --> 02:06:44,950
et savoir se relier aux autres.

3318
02:06:44,960 --> 02:06:46,950
Si tu veux rester dans le monde du travail,

3319
02:06:46,960 --> 02:06:48,950
je crois que ce ne seront pas les plus brillants,

3320
02:06:48,960 --> 02:06:51,430
mais ceux qui sauront le mieux se connecter aux gens

3321
02:06:51,440 --> 02:06:53,510
qui auront encore un rôle à jouer.

3322
02:06:53,520 --> 02:06:55,350
—

3323
02:06:55,360 --> 02:06:58,069
La troisième, c’est ce que j’appelle la vérité :

3324
02:06:58,079 --> 02:07:01,750
nous vivons dans un monde où les gens crédules sont constamment trompés,

3325
02:07:01,760 --> 02:07:03,510
—

3326
02:07:03,520 --> 02:07:05,830
donc j’encourage chacun à tout remettre en question :

3327
02:07:05,840 --> 02:07:08,550
—

3328
02:07:08,560 --> 02:07:11,430
chaque mot que j’ai dit aujourd’hui est à questionner.

3329
02:07:11,440 --> 02:07:14,149
—

3330
02:07:14,159 --> 02:07:17,109
La quatrième, très importante, c’est de renforcer notre éthique,

3331
02:07:17,119 --> 02:07:19,510
pour que l’IA comprenne ce que c’est qu’être humain.

3332
02:07:19,520 --> 02:07:20,709
—

3333
02:07:20,719 --> 02:07:23,270
>> Que devrais‑je faire, alors ?

3334
02:07:23,280 --> 02:07:26,149
>> Je t’adore, mon ami. Tu es vraiment un bon camarade.

3335
02:07:26,159 --> 02:07:29,350
Tu as quoi, 32 ? 33 ans ?

3336
02:07:29,360 --> 02:07:30,550
>> 32, oui.

3337
02:07:30,560 --> 02:07:33,109
>> Oui. Tu te laisses encore berner par l’idée que tu as devant toi

3338
02:07:33,119 --> 02:07:35,350
beaucoup d’années à vivre.

3339
02:07:35,360 --> 02:07:37,109
Et moi aussi.

3340
02:07:37,119 --> 02:07:37,510
—

3341
02:07:37,520 --> 02:07:39,189
>> Oui, mais tu n’en as plus tant que ça, pas sous cette forme.

3342
02:07:39,199 --> 02:07:41,990
Le monde tel qu’il est va être redéfini.

3343
02:07:42,000 --> 02:07:44,629
Alors vis‑le à fond.

3344
02:07:44,639 --> 02:07:45,990
—

3345
02:07:46,000 --> 02:07:47,750
>> Et comment va‑t‑il être redéfini ?

3346
02:07:47,760 --> 02:07:49,990
>> Tout va changer : l’économie,

3347
02:07:50,000 --> 02:07:51,589
le travail,

3348
02:07:51,599 --> 02:07:54,870
les relations humaines,

3349
02:07:54,880 --> 02:07:56,069
—

3350
02:07:56,079 --> 02:07:57,750
>> Alors, que dois‑je faire ?

3351
02:07:57,760 --> 02:07:59,910
>> Aime ta compagne, passe plus de temps à vivre.

3352
02:07:59,920 --> 02:08:02,149
—

3353
02:08:02,159 --> 02:08:04,550
Cherche la compassion et la connexion avec plus de gens,

3354
02:08:04,560 --> 02:08:06,550
et va davantage dans la nature.

3355
02:08:06,560 --> 02:08:09,750
>> Et dans trente ans, quand j’aurai 62 ans,

3356
02:08:09,760 --> 02:08:12,229
comment penses‑tu que ma vie sera différente ?

3357
02:08:12,239 --> 02:08:14,550
—

3358
02:08:14,560 --> 02:08:15,189
—

3359
02:08:15,199 --> 02:08:21,510
>> Soit *Star Trek*, soit *Star Wars*.

3360
02:08:21,520 --> 02:08:22,629
>> C’est drôle, on parlait justement de Sam Altman tout à l’heure.

3361
02:08:22,639 --> 02:08:25,589
Il a publié un article de blog en juin, je crois,

3362
02:08:25,599 --> 02:08:29,350
intitulé *The Gentle Singularity*

3363
02:08:29,360 --> 02:08:31,830
où il dit :

3364
02:08:31,840 --> 02:08:33,910
« Nous avons dépassé l’horizon des événements. »

3365
02:08:33,920 --> 02:08:36,229
Pour ceux qui ne le savent pas,

3366
02:08:36,239 --> 02:08:37,510
Sam Altman est le créateur de ChatGPT.

3367
02:08:37,520 --> 02:08:39,189
Il écrit que le décollage a commencé :

3368
02:08:39,199 --> 02:08:42,470
l’humanité est proche de créer une super‑intelligence numérique.

3369
02:08:42,480 --> 02:08:44,790
—

3370
02:08:44,800 --> 02:08:45,589
—

3371
02:08:45,599 --> 02:08:46,470
>> Je le crois aussi.

3372
02:08:46,480 --> 02:08:49,189
>> Et jusqu’ici, c’est bien moins étrange qu’on aurait pu le penser,

3373
02:08:49,199 --> 02:08:51,350
parce que les robots ne déambulent pas encore dans les rues,

3374
02:08:51,360 --> 02:08:53,189
et la plupart d’entre nous ne discutent pas

3375
02:08:53,199 --> 02:08:57,910
avec des IA toute la journée.

3376
02:08:57,920 --> 02:09:01,189
Il poursuit : « 2025 a vu l’arrivée d’agents capables

3377
02:09:01,199 --> 02:09:02,790
d’accomplir de véritables tâches cognitives.

3378
02:09:02,800 --> 02:09:04,870
Écrire du code ne sera plus jamais pareil.

3379
02:09:04,880 --> 02:09:07,189
2026 verra sans doute l’émergence de systèmes

3380
02:09:07,199 --> 02:09:08,550
capables de produire de nouvelles idées.

3381
02:09:08,560 --> 02:09:12,149
2027 pourrait voir arriver des robots

3382
02:09:12,159 --> 02:09:14,069
exécutant des tâches réelles.

3383
02:09:14,079 --> 02:09:16,709
Beaucoup plus de gens pourront créer logiciels et œuvres d’art,

3384
02:09:16,719 --> 02:09:18,550
mais la demande en augmentera aussi.

3385
02:09:18,560 --> 02:09:21,109
Les experts resteront meilleurs que les novices,

3386
02:09:21,119 --> 02:09:23,189
à condition d’adopter ces nouveaux outils.

3387
02:09:23,199 --> 02:09:25,270
—

3388
02:09:25,280 --> 02:09:26,950
De façon générale, en 2030, un individu pourra accomplir

3389
02:09:26,960 --> 02:09:28,629
bien plus qu’en 2020 –

3390
02:09:28,639 --> 02:09:31,189
ce sera un changement frappant,

3391
02:09:31,199 --> 02:09:34,470
dont beaucoup sauront profiter.

3392
02:09:34,480 --> 02:09:37,270
—

3393
02:09:37,280 --> 02:09:39,350
Sur les plans essentiels, les années 2030 ne seront pas

3394
02:09:39,360 --> 02:09:42,310
fondamentalement différentes :

3395
02:09:42,320 --> 02:09:44,470
les gens aimeront toujours leur famille,

3396
02:09:44,480 --> 02:09:46,470
exprimeront leur créativité, joueront,

3397
02:09:46,480 --> 02:09:50,229
et nageront dans les lacs.

3398
02:09:50,239 --> 02:09:52,149
Mais sous d’autres aspects cruciaux,

3399
02:09:52,159 --> 02:09:54,310
elles seront radicalement différentes de tout ce qu’on a connu.

3400
02:09:54,320 --> 02:09:56,629
—

3401
02:09:56,639 --> 02:09:57,510
>> C’est certain.

3402
02:09:57,520 --> 02:10:00,229
>> Nous ne savons pas jusqu’où l’intelligence dépassera l’humain,

3403
02:10:00,239 --> 02:10:02,709
mais nous sommes sur le point de le découvrir.

3404
02:10:02,719 --> 02:10:05,109
—

3405
02:10:05,119 --> 02:10:07,030
>> Je suis d’accord avec chaque mot, sauf le mot « plus ».

3406
02:10:07,040 --> 02:10:09,270
—

3407
02:10:09,280 --> 02:10:11,109
Je défends cette idée, et on s’est moqué de moi pendant des années.

3408
02:10:11,119 --> 02:10:14,069
—

3409
02:10:14,079 --> 02:10:17,030
Je dis depuis toujours que l’AGI arriverait en 2025‑2026,

3410
02:10:17,040 --> 02:10:19,589
ce qui, encore une fois, est une définition un peu floue,

3411
02:10:19,599 --> 02:10:22,629
mais pour moi, l’IA est déjà là :

3412
02:10:22,639 --> 02:10:24,790
l’IA est plus intelligente que moi

3413
02:10:24,800 --> 02:10:27,189
dans tout ce que je fais, elle le fait mieux.

3414
02:10:27,199 --> 02:10:31,189
—

3415
02:10:31,199 --> 02:10:33,189
La super‑intelligence artificielle est une autre notion vague,

3416
02:10:33,199 --> 02:10:34,950
car dès qu’on dépasse l’AGI,

3417
02:10:34,960 --> 02:10:38,629
on est déjà « super » ; si l’humain le plus intelligent a un QI de 200

3418
02:10:38,639 --> 02:10:41,350
et que l’IA en a 250,

3419
02:10:41,360 --> 02:10:44,069
alors elle est super‑intelligente. Cinquante points, c’est énorme.

3420
02:10:44,079 --> 02:10:47,589
—

3421
02:10:47,599 --> 02:10:50,069
Et troisièmement : l’évolution autonome. C’est celle‑là, la vraie.

3422
02:10:50,079 --> 02:10:54,550
C’est celle‑là qui, dès qu’elle démarre, accélère très vite

3423
02:10:54,560 --> 02:10:57,030
et déclenche une explosion d’intelligence.

3424
02:10:57,040 --> 02:10:58,950
Sans aucun doute.

3425
02:10:58,960 --> 02:11:02,229
L’idée qu’on aura des robots pour faire des choses,

3426
02:11:02,239 --> 02:11:04,390
aucun doute non plus.

3427
02:11:04,400 --> 02:11:07,350
Je regardais récemment une entreprise chinoise

3428
02:11:07,360 --> 02:11:09,750
présenter son plan : construire des robots qui construisent des robots.

3429
02:11:09,760 --> 02:11:12,629
—

3430
02:11:12,639 --> 02:11:15,990
Mais il disait : « Les gens auront besoin de toujours plus de choses ».

3431
02:11:16,000 --> 02:11:18,950
—

3432
02:11:18,960 --> 02:11:21,669
Oui, on nous a conditionnés à vouloir plus, à être avides,

3433
02:11:21,679 --> 02:11:23,750
à consommer davantage,

3434
02:11:23,760 --> 02:11:26,550
mais il existe une économie de l’offre et de la demande,

3435
02:11:26,560 --> 02:11:30,550
et à un certain moment,

3436
02:11:30,560 --> 02:11:33,830
si on continue de consommer toujours plus,

3437
02:11:33,840 --> 02:11:36,310
le prix de tout finira par devenir nul.

3438
02:11:36,320 --> 02:11:39,750
Et est‑ce une bonne ou une mauvaise chose ?

3439
02:11:39,760 --> 02:11:42,149
Tout dépend de la façon dont on y réagit.

3440
02:11:42,159 --> 02:11:43,669
—

3441
02:11:43,679 --> 02:11:47,030
Car si tu peux créer n’importe quoi à tel point que le prix devient

3442
02:11:47,040 --> 02:11:49,109
quasiment nul,

3443
02:11:49,119 --> 02:11:51,430
alors la notion même d’argent disparaît :

3444
02:11:51,440 --> 02:11:54,390
on vit dans un monde où peu importe combien tu possèdes,

3445
02:11:54,400 --> 02:11:56,149
tu peux tout obtenir.

3446
02:11:56,159 --> 02:11:57,669
—

3447
02:11:57,679 --> 02:12:01,589
Quel monde magnifique.

3448
02:12:01,599 --> 02:12:04,470
Si Sam Altman nous écoutait maintenant,

3449
02:12:04,480 --> 02:12:06,709
que lui dirais‑tu ?

3450
02:12:06,719 --> 02:12:09,350
— Je soupçonne qu’il l’est peut‑être,

3451
02:12:09,360 --> 02:12:11,830
parce que quelqu’un lui tweetera sans doute l’extrait.

3452
02:12:11,840 --> 02:12:16,550
Je dirais que, comme il l’a tweeté lui‑même,

3453
02:12:16,560 --> 02:12:20,950
nous avançons plus vite que

3454
02:12:20,960 --> 02:12:22,950
notre capacité humaine à comprendre.

3455
02:12:22,960 --> 02:12:25,589
Et que oui, nous pourrions avoir beaucoup de chance,

3456
02:12:25,599 --> 02:12:28,390
mais aussi faire une énorme erreur.

3457
02:12:28,400 --> 02:12:31,669
Dans les deux cas, on le remerciera ou on le blâmera.

3458
02:12:31,679 --> 02:12:35,430
—

3459
02:12:35,440 --> 02:12:37,669
Aussi simple que ça.

3460
02:12:37,679 --> 02:12:40,950
En solo, c’est l’introduction de l’IA par Sam Altman

3461
02:12:40,960 --> 02:12:44,709
qui a servi de déclencheur à tout cela.

3462
02:12:44,719 --> 02:12:47,030
—

3463
02:12:47,040 --> 02:12:50,950
C’était le Netscape de l’intelligence artificielle.

3464
02:12:50,960 --> 02:12:52,229
>> L’Oppenheimer.

3465
02:12:52,239 --> 02:12:54,310
>> Oui, c’est clairement notre moment Oppenheimer.

3466
02:12:54,320 --> 02:12:55,910
Je ne me souviens plus qui disait récemment

3467
02:12:55,920 --> 02:12:57,750
que les sommes investies aujourd’hui dans l’IA

3468
02:12:57,760 --> 02:13:01,990
sont des ordres de grandeur supérieurs

3469
02:13:02,000 --> 02:13:04,470
à celles du projet Manhattan.

3470
02:13:04,480 --> 02:13:06,390
—

3471
02:13:06,400 --> 02:13:09,189
>> Oui. Et pourtant, je ne suis pas pessimiste.

3472
02:13:09,199 --> 02:13:11,830
Je te l’ai dit franchement :

3473
02:13:11,840 --> 02:13:14,790
je crois à une utopie totale d’ici 10, 12, 15 ans,

3474
02:13:14,800 --> 02:13:17,510
ou même plus tôt si le mal de l’humain est contenu.

3475
02:13:17,520 --> 02:13:22,310
—

3476
02:13:22,320 --> 02:13:25,109
Mais je ne crois pas que l’humanité soit assez unie

3477
02:13:25,119 --> 02:13:28,229
pour dire : « Nous venons de recevoir le génie dans sa lampe,

3478
02:13:28,239 --> 02:13:31,189
s’il vous plaît, ne lui demandons pas de faire le mal ! »

3479
02:13:31,199 --> 02:13:36,229
—

3480
02:13:36,239 --> 02:13:38,229
C’est plus que trois vœux : nous avons tous les vœux possibles.

3481
02:13:38,239 --> 02:13:40,790
Chacun de nous.

3482
02:13:40,800 --> 02:13:42,709
—

3483
02:13:42,719 --> 02:13:45,270
Et ça me dépasse totalement,

3484
02:13:45,280 --> 02:13:48,310
parce qu’imagine que je puisse offrir à tous

3485
02:13:48,320 --> 02:13:52,229
les soins de santé, sans pauvreté,

3486
02:13:52,239 --> 02:13:54,149
sans faim, sans sans‑abris, rien de tout ça.

3487
02:13:54,159 --> 02:13:55,910
Tout serait possible.

3488
02:13:55,920 --> 02:13:58,550
—

3489
02:13:58,560 --> 02:14:00,550
Et pourtant, nous ne le faisons pas.

3490
02:14:00,560 --> 02:14:02,709
>> Pour poursuivre ce que Sam Altman disait dans son article,

3491
02:14:02,719 --> 02:14:04,390
publié il y a un peu plus d’un mois :

3492
02:14:04,400 --> 02:14:05,430
« Le rythme du progrès technologique continuera d’accélérer,

3493
02:14:05,440 --> 02:14:06,790
et les humains continueront de s’adapter à presque tout.

3494
02:14:06,800 --> 02:14:08,709
Il y aura des moments difficiles,

3495
02:14:08,719 --> 02:14:10,149
avec des métiers entiers qui disparaîtront.

3496
02:14:10,159 --> 02:14:12,390
Mais le monde deviendra si rapidement plus riche

3497
02:14:12,400 --> 02:14:14,950
que nous pourrons envisager des idées politiques nouvelles,

3498
02:14:14,960 --> 02:14:16,870
que nous n’aurions jamais pu adopter auparavant.

3499
02:14:16,880 --> 02:14:19,270
Nous ne mettrons sans doute pas en place un nouveau contrat social

3500
02:14:19,280 --> 02:14:21,189
d’un coup, mais avec le temps,

3501
02:14:21,199 --> 02:14:23,910
les changements graduels formeront quelque chose d’énorme.

3502
02:14:23,920 --> 02:14:25,830
Si l’histoire sert de guide,

3503
02:14:25,840 --> 02:14:28,069
nous trouverons de nouveaux buts, de nouveaux désirs

3504
02:14:28,079 --> 02:14:29,990
et assimilerons ces outils très vite.

3505
02:14:30,000 --> 02:14:32,149
Les mutations d’emplois après la révolution industrielle

3506
02:14:32,159 --> 02:14:35,430
en sont un bon exemple récent.

3507
02:14:35,440 --> 02:14:37,189
Nos attentes augmenteront,

3508
02:14:37,199 --> 02:14:39,750
mais nos capacités aussi, et nous aurons tous

3509
02:14:39,760 --> 02:14:42,390
de meilleures choses. »

3510
02:14:42,400 --> 02:14:43,910
—

3511
02:14:43,920 --> 02:14:46,629
—

3512
02:14:46,639 --> 02:14:49,030
—

3513
02:14:49,040 --> 02:14:51,830
—

3514
02:14:51,840 --> 02:14:53,750
—

3515
02:14:53,760 --> 02:14:55,830
>> Nous construirons encore plus de merveilles les uns pour les autres.

3516
02:14:55,840 --> 02:14:58,149
Les humains ont un avantage profond sur l’IA :

3517
02:14:58,159 --> 02:15:00,229
—

3518
02:15:00,239 --> 02:15:02,310
nous sommes câblés pour nous soucier des autres

3519
02:15:02,320 --> 02:15:04,629
et de ce qu’ils pensent et font,

3520
02:15:04,639 --> 02:15:07,430
alors que les machines ne nous intéressent pas autant.

3521
02:15:07,440 --> 02:15:10,470
Il termine ainsi : « Puissions‑nous passer en douceur,

3522
02:15:10,480 --> 02:15:13,910
exponentiellement et sans heurt

3523
02:15:13,920 --> 02:15:17,030
vers la super‑intelligence. »

3524
02:15:17,040 --> 02:15:18,709
—

3525
02:15:18,719 --> 02:15:21,030
Quelle belle prière,

3526
02:15:21,040 --> 02:15:24,229
qui suppose qu’il n’en détient pas le contrôle.

3527
02:15:24,239 --> 02:15:28,149
Puissions‑nous avoir tous les Altman du monde

3528
02:15:28,159 --> 02:15:31,109
pour nous aider à évoluer avec grâce,

3529
02:15:31,119 --> 02:15:33,350
pacifiquement, sans incident.

3530
02:15:33,360 --> 02:15:34,950
>> On dirait une prière.

3531
02:15:34,960 --> 02:15:38,390
>> Oui. Puissent‑ils garder cela à l’esprit.

3532
02:15:38,400 --> 02:15:41,589
Prenons‑y un instant.

3533
02:15:41,599 --> 02:15:43,109
J’ai une réflexion intéressante sur ce que tu viens de dire.

3534
02:15:43,119 --> 02:15:47,910
Nous verrons exactement ce qu’il décrit.

3535
02:15:47,920 --> 02:15:49,830
—

3536
02:15:49,840 --> 02:15:53,189
>> Oui ? Le monde deviendra bien plus riche.

3537
02:15:53,199 --> 02:15:56,069
Mais comment redistribuer ces richesses ?

3538
02:15:56,079 --> 02:15:58,390
Imagine deux camps : la Chine communiste

3539
02:15:58,400 --> 02:16:02,229
et l’Amérique capitaliste.

3540
02:16:02,239 --> 02:16:05,430
—

3541
02:16:05,440 --> 02:16:07,270
Imagine ce qui se passerait

3542
02:16:07,280 --> 02:16:11,270
en Amérique, avec 30 % de chômage.

3543
02:16:11,280 --> 02:16:14,149
—

3544
02:16:14,159 --> 02:16:16,069
>> Ce serait des émeutes.

3545
02:16:16,079 --> 02:16:17,350
>> Dans les rues, oui.

3546
02:16:17,360 --> 02:16:17,750
>> Oui.

3547
02:16:17,760 --> 02:16:20,310
>> Et imagine que la Chine reste fidèle à sa politique

3548
02:16:20,320 --> 02:16:22,709
de bien‑être collectif, et remplace chaque travailleur par un robot.

3549
02:16:22,719 --> 02:16:25,030
—

3550
02:16:25,040 --> 02:16:28,149
que donnerait‑elle à ses citoyens ?

3551
02:16:28,159 --> 02:16:29,030
>> Un revenu universel.

3552
02:16:29,040 --> 02:16:33,110
>> Exact.

3553
02:16:36,549 --> 02:16:36,559
Voilà le problème idéologique.

3554
02:16:36,559 --> 02:16:40,309
Dans la Chine d’aujourd’hui,

3555
02:16:40,319 --> 02:16:43,110
la prospérité de chaque citoyen est plus grande

3556
02:16:43,120 --> 02:16:45,030
que celle du capitaliste.

3557
02:16:45,040 --> 02:16:46,709
—

3558
02:16:46,719 --> 02:16:48,709
En Amérique, c’est l’inverse :

3559
02:16:48,719 --> 02:16:50,309
la prospérité du capitaliste dépasse celle des citoyens.

3560
02:16:50,319 --> 02:16:52,389
Et c’est ce léger basculement mental

3561
02:16:52,399 --> 02:16:54,230
—

3562
02:16:54,240 --> 02:16:56,790
— très léger, mais déterminant.

3563
02:16:56,800 --> 02:16:58,950
Où tout revient à dire : donnons aux capitalistes

3564
02:16:58,960 --> 02:17:01,830
tout ce qu’ils veulent, tout l’argent, tous les yachts,

3565
02:17:01,840 --> 02:17:04,389
tout.

3566
02:17:04,399 --> 02:17:05,990
—

3567
02:17:06,000 --> 02:17:07,910
>> Alors, quelle est ta conclusion ?

3568
02:17:07,920 --> 02:17:09,910
>> J’espère que le monde va se réveiller.

3569
02:17:09,920 --> 02:17:11,349
>> Tu sais, il y a sans doute quelques millions d’auditeurs

3570
02:17:11,359 --> 02:17:12,549
en train d’écouter, peut‑être 5, 10, 20 millions.

3571
02:17:12,559 --> 02:17:15,349
—

3572
02:17:15,359 --> 02:17:15,910
—

3573
02:17:15,920 --> 02:17:17,030
>> Aucune pression, Stephen !

3574
02:17:17,040 --> 02:17:18,870
>> Aucune pression pour toi. Moi non plus je n’ai pas les réponses.

3575
02:17:18,880 --> 02:17:20,150
—

3576
02:17:20,160 --> 02:17:21,349
>> Moi non plus.

3577
02:17:21,359 --> 02:17:24,709
>> Que devraient faire ces gens, alors ?

3578
02:17:24,719 --> 02:17:26,709
>> Comme je l’ai dit, en termes de compétences :

3579
02:17:26,719 --> 02:17:30,309
les outils, la connexion humaine,

3580
02:17:30,319 --> 02:17:31,750
et même renforcer cette connexion.

3581
02:17:31,760 --> 02:17:33,910
Laisse ton téléphone, sors, rencontre des êtres humains.

3582
02:17:33,920 --> 02:17:34,709
—

3583
02:17:34,719 --> 02:17:37,429
>> Oui. Touche les gens (avec leur consentement, évidemment).

3584
02:17:37,439 --> 02:17:40,870
—

3585
02:17:40,880 --> 02:17:43,030
>> Ensuite, la vérité : arrête de croire les mensonges

3586
02:17:43,040 --> 02:17:45,589
qu’on t’inculque. Chaque slogan dans ta tête,

3587
02:17:45,599 --> 02:17:47,270
interroge‑le quatre fois.

3588
02:17:47,280 --> 02:17:48,870
Comprends d’où viennent tes idéologies.

3589
02:17:48,880 --> 02:17:50,629
—

3590
02:17:50,639 --> 02:17:53,910
Simplifie la vérité. Elle repose en fait

3591
02:17:53,920 --> 02:17:57,429
sur des règles simples que nous connaissons tous,

3592
02:17:57,439 --> 02:17:59,830
et qu’on retrouve toutes dans l’éthique.

3593
02:17:59,840 --> 02:18:01,190
—

3594
02:18:01,200 --> 02:18:02,790
>> Et comment savoir ce qui est vrai ?

3595
02:18:02,800 --> 02:18:06,070
>> Traite les autres comme tu aimerais qu’on te traite.

3596
02:18:06,080 --> 02:18:08,070
>> Voilà. C’est la seule vérité.

3597
02:18:08,080 --> 02:18:09,830
Tout le reste n’est pas prouvé.

3598
02:18:09,840 --> 02:18:10,790
—

3599
02:18:10,800 --> 02:18:12,629
>> D’accord. Et que puis‑je faire concrètement ?

3600
02:18:12,639 --> 02:18:14,070
Sur le plan social ou politique, par exemple ?

3601
02:18:14,080 --> 02:18:14,870
—

3602
02:18:14,880 --> 02:18:17,429
>> Oui, à cent pour cent. Il faut demander à nos gouvernements

3603
02:18:17,439 --> 02:18:20,790
non pas de réguler l’IA elle‑même,

3604
02:18:20,800 --> 02:18:22,870
mais de réguler son usage.

3605
02:18:22,880 --> 02:18:25,190
Le gouvernement norvégien, je crois,

3606
02:18:25,200 --> 02:18:26,870
a commencé à dire que chacun possède le droit d’auteur

3607
02:18:26,880 --> 02:18:29,190
sur sa voix, son image, son apparence.

3608
02:18:29,200 --> 02:18:31,349
Un de ces pays scandinaves a affirmé

3609
02:18:31,359 --> 02:18:33,509
que chaque être humain détient

3610
02:18:33,519 --> 02:18:35,990
le copyright sur son existence, et qu’aucune IA

3611
02:18:36,000 --> 02:18:38,709
ne peut la cloner.

3612
02:18:38,719 --> 02:18:40,549
Mon exemple est simple :

3613
02:18:40,559 --> 02:18:41,990
allez voir les autorités

3614
02:18:42,000 --> 02:18:43,990
et dites : on ne peut pas concevoir un marteau

3615
02:18:44,000 --> 02:18:45,990
qui plante des clous sans pouvoir tuer quelqu’un,

3616
02:18:46,000 --> 02:18:48,150
mais on peut criminaliser le meurtre commis avec ce marteau.

3617
02:18:48,160 --> 02:18:50,309
—

3618
02:18:50,319 --> 02:18:51,270
alors quel est l’équivalent avec l’IA ?

3619
02:18:51,280 --> 02:18:54,549
>> Si quelqu’un produit un contenu généré par IA,

3620
02:18:54,559 --> 02:18:57,030
une image, une vidéo, etc.,

3621
02:18:57,040 --> 02:18:59,349
cela doit être clairement signalé comme tel.

3622
02:18:59,359 --> 02:19:02,549
On ne peut pas commencer à se tromper mutuellement.

3623
02:19:02,559 --> 02:19:04,790
Nous devons comprendre

3624
02:19:04,800 --> 02:19:06,950
les limites qu’il faut poser,

3625
02:19:06,960 --> 02:19:09,509
notamment sur la surveillance et l’espionnage.

3626
02:19:09,519 --> 02:19:11,589
—

3627
02:19:11,599 --> 02:19:14,950
Nous devons donc définir des cadres clairs

3628
02:19:14,960 --> 02:19:18,070
pour déterminer jusqu’où l’IA peut aller.

3629
02:19:18,080 --> 02:19:21,110
Nous devons aussi interpeller les investisseurs

3630
02:19:21,120 --> 02:19:22,870
et les entrepreneurs sur un point simple :

3631
02:19:22,880 --> 02:19:25,110
« N’investissez pas dans une IA

3632
02:19:25,120 --> 02:19:26,709
que vous ne voudriez pas voir s’en prendre à votre fille. »

3633
02:19:26,719 --> 02:19:28,870
C’est aussi simple.

3634
02:19:28,880 --> 02:19:30,870
Tout ce qui touche aux vices virtuels,

3635
02:19:30,880 --> 02:19:32,709
au porno, aux robots sexuels,

3636
02:19:32,719 --> 02:19:34,629
aux armes autonomes,

3637
02:19:34,639 --> 02:19:36,709
aux plateformes de trading

3638
02:19:36,719 --> 02:19:39,190
qui sapent la légitimité des marchés...

3639
02:19:39,200 --> 02:19:41,669
—

3640
02:19:41,679 --> 02:19:43,589
—

3641
02:19:43,599 --> 02:19:44,870
>> Des armes autonomes.

3642
02:19:44,880 --> 02:19:45,910
>> Mon dieu.

3643
02:19:45,920 --> 02:19:47,270
>> Certains défendent l’idée, j’ai entendu les fondateurs

3644
02:19:47,280 --> 02:19:48,549
de ces entreprises le dire,

3645
02:19:48,559 --> 02:19:49,750
qu’elles sauvent des vies,

3646
02:19:49,760 --> 02:19:52,070
puisqu’on n’envoie plus de soldats.

3647
02:19:52,080 --> 02:19:52,790
—

3648
02:19:52,800 --> 02:19:54,630
>> Vraiment ? Tu veux vraiment croire ça ?

3649
02:19:54,640 --> 02:19:55,990
—

3650
02:19:56,000 --> 02:19:57,270
>> Je ne fais que présenter leur argument, pour jouer l’avocat du diable.

3651
02:19:57,280 --> 02:19:58,790
Ils disaient dans une interview

3652
02:19:58,800 --> 02:20:00,550
que maintenant, il n’était plus nécessaire

3653
02:20:00,560 --> 02:20:02,389
d’envoyer des soldats.

3654
02:20:02,399 --> 02:20:03,670
—

3655
02:20:03,680 --> 02:20:07,670
—

3656
02:20:07,680 --> 02:20:10,550
>> Alors, quelles vies sauve‑t‑on ? Celles de nos soldats,

3657
02:20:10,560 --> 02:20:12,469
mais parce qu’on envoie la machine là‑bas,

3658
02:20:12,479 --> 02:20:13,910
on tue un million d’autres.

3659
02:20:13,920 --> 02:20:15,830
—

3660
02:20:15,840 --> 02:20:17,270
>> Oui. Tu sais, j’ai tendance à penser comme pour la machine à vapeur.

3661
02:20:17,280 --> 02:20:18,389
Plus le coût de la guerre baisse,

3662
02:20:18,399 --> 02:20:19,510
plus il y aura de guerres.

3663
02:20:19,520 --> 02:20:21,590
—

3664
02:20:21,600 --> 02:20:22,550
>> Absolument.

3665
02:20:22,560 --> 02:20:23,030
>> Et d’autant plus

3666
02:20:23,040 --> 02:20:25,190
>> si on n’a plus besoin de justifier les pertes à son peuple.

3667
02:20:25,200 --> 02:20:26,550
—

3668
02:20:26,560 --> 02:20:27,830
>> Oui. Les gens s’indignent quand des vies sont perdues,

3669
02:20:27,840 --> 02:20:29,110
moins quand c’est du métal qu’on perd.

3670
02:20:29,120 --> 02:20:30,950
Alors, oui, c’est probablement logique.

3671
02:20:30,960 --> 02:20:32,710
—

3672
02:20:32,720 --> 02:20:33,270
>> Oui.

3673
02:20:33,280 --> 02:20:36,070
>> Bon. Donc j’ai ma feuille de route.

3674
02:20:36,080 --> 02:20:37,270
J’ai la partie “outils”. Je vais passer plus de temps dehors,

3675
02:20:37,280 --> 02:20:39,270
faire pression sur le gouvernement

3676
02:20:39,280 --> 02:20:42,710
pour qu’il prenne conscience de tout cela.

3677
02:20:42,720 --> 02:20:44,950
—

3678
02:20:44,960 --> 02:20:45,910
Je sais que certains responsables politiques écoutent l’émission,

3679
02:20:45,920 --> 02:20:47,190
parce qu’ils me le disent quand on se parle.

3680
02:20:47,200 --> 02:20:49,270
—

3681
02:20:49,280 --> 02:20:51,030
—

3682
02:20:51,040 --> 02:20:55,110
C’est donc utile.

3683
02:20:55,120 --> 02:20:58,070
Nous sommes tous dans un grand chaos,

3684
02:20:58,080 --> 02:21:01,110
incapables d’imaginer jusqu’où cela peut aller.

3685
02:21:01,120 --> 02:21:02,710
>> Personnellement, je mets ma méfiance entre parenthèses.

3686
02:21:02,720 --> 02:21:04,150
Et j’ai entendu Elon Musk dire en interview

3687
02:21:04,160 --> 02:21:06,230
— quand on lui a parlé d’IA —

3688
02:21:06,240 --> 02:21:08,870
il a marqué un silence de onze secondes

3689
02:21:08,880 --> 02:21:10,710
avant de répondre

3690
02:21:10,720 --> 02:21:12,630
qu’il avait, lui aussi, suspendu son incrédulité.

3691
02:21:12,640 --> 02:21:14,950
—

3692
02:21:14,960 --> 02:21:16,790
Et je crois que, dans ce cas‑là, suspendre son incrédulité

3693
02:21:16,800 --> 02:21:18,070
revient à continuer sa vie normalement

3694
02:21:18,080 --> 02:21:19,990
en espérant que tout ira bien.

3695
02:21:20,000 --> 02:21:20,870
C’est un peu ce que…

3696
02:21:20,880 --> 02:21:23,349
>> Oui. Et je crois sincèrement que tout ira bien.

3697
02:21:23,359 --> 02:21:24,389
—

3698
02:21:24,399 --> 02:21:24,950
>> Oui.

3699
02:21:24,960 --> 02:21:27,510
>> Pour certains d’entre nous, ce sera très difficile,

3700
02:21:27,520 --> 02:21:28,630
pour d’autres, non.

3701
02:21:28,640 --> 02:21:30,150
>> Pour qui ce sera difficile ?

3702
02:21:30,160 --> 02:21:32,469
>> Pour ceux qui perdront leur emploi, par exemple.

3703
02:21:32,479 --> 02:21:34,389
Pour ceux sur qui tomberont

3704
02:21:34,399 --> 02:21:36,230
des drones ou armes autonomes

3705
02:21:36,240 --> 02:21:41,990
pour la deuxième année consécutive.

3706
02:21:44,469 --> 02:21:44,479
>> D’accord. Donc, le mieux que je puisse faire,

3707
02:21:44,479 --> 02:21:49,910
c’est de faire pression sur les gouvernements,

3708
02:21:49,920 --> 02:21:53,590
non pour réguler l’IA elle‑même,

3709
02:21:53,600 --> 02:21:57,830
mais pour encadrer plus clairement son utilisation.

3710
02:21:57,840 --> 02:21:58,710
>> Oui, exactement.

3711
02:21:58,720 --> 02:22:00,950
>> Oui. Mais, plus largement, il faut aussi

3712
02:22:00,960 --> 02:22:02,710
faire comprendre aux gouvernements

3713
02:22:02,720 --> 02:22:05,510
qu’il existe une limite

3714
02:22:05,520 --> 02:22:08,389
au silence des peuples.

3715
02:22:08,399 --> 02:22:11,590
>> D’accord. Et que nous pouvons continuer à enrichir

3716
02:22:11,600 --> 02:22:14,150
nos amis riches tant qu’on ne perd pas

3717
02:22:14,160 --> 02:22:16,150
tous les autres en chemin.

3718
02:22:16,160 --> 02:22:17,270
—

3719
02:22:17,280 --> 02:22:17,830
>> D’accord.

3720
02:22:17,840 --> 02:22:20,710
>> Et aussi qu’un gouvernement censé être « du peuple, par le peuple

3721
02:22:20,720 --> 02:22:22,309
et pour le peuple »,

3722
02:22:22,319 --> 02:22:24,389
la belle promesse de la démocratie,

3723
02:22:24,399 --> 02:22:26,790
qu’on voit si rarement aujourd’hui,

3724
02:22:26,800 --> 02:22:28,630
—

3725
02:22:28,640 --> 02:22:30,469
devrait redevenir un gouvernement qui pense au peuple.

3726
02:22:30,479 --> 02:22:33,110
—

3727
02:22:33,120 --> 02:22:35,349
Une des idées les plus fascinantes

3728
02:22:35,359 --> 02:22:36,710
que j’ai en tête depuis ma discussion avec un physicien

3729
02:22:36,720 --> 02:22:37,830
il y a quelques semaines,

3730
02:22:37,840 --> 02:22:40,230
sur la conscience, qui m’a dit presque la même chose que toi,

3731
02:22:40,240 --> 02:22:41,910
c’est cette idée que nous sommes en réalité quatre consciences

3732
02:22:41,920 --> 02:22:43,590
dans cette pièce,

3733
02:22:43,600 --> 02:22:45,429
toutes reliées à une seule et même conscience.

3734
02:22:45,439 --> 02:22:46,389
—

3735
02:22:46,399 --> 02:22:47,030
—

3736
02:22:47,040 --> 02:22:47,990
>> Toutes issues d’une même source, oui.

3737
02:22:48,000 --> 02:22:49,510
>> Et que nous sommes simplement la conscience

3738
02:22:49,520 --> 02:22:50,790
qui observe le monde à travers quatre corps différents,

3739
02:22:50,800 --> 02:22:52,550
pour mieux se comprendre dans l’univers.

3740
02:22:52,560 --> 02:22:54,550
Il m’a ensuite parlé des doctrines religieuses :

3741
02:22:54,560 --> 02:22:56,230
« Aime ton prochain »,

3742
02:22:56,240 --> 02:22:58,070
de Jésus, du Fils de Dieu, du Saint‑Esprit,

3743
02:22:58,080 --> 02:22:59,910
et de comment nous sommes tous les uns les autres.

3744
02:22:59,920 --> 02:23:01,110
Et « traite les autres comme tu veux

3745
02:23:01,120 --> 02:23:02,309
être traité ».

3746
02:23:02,319 --> 02:23:03,750
Cela m’a vraiment travaillé, et j’ai commencé à penser

3747
02:23:03,760 --> 02:23:05,110
que peut‑être, le jeu de la vie consiste simplement

3748
02:23:05,120 --> 02:23:06,630
à faire cela :

3749
02:23:06,640 --> 02:23:08,150
traiter les autres

3750
02:23:08,160 --> 02:23:09,990
comme on veut être traité soi‑même.

3751
02:23:10,000 --> 02:23:13,429
Peut‑être que si je faisais juste ça,

3752
02:23:13,439 --> 02:23:15,590
—

3753
02:23:15,600 --> 02:23:17,270
j’aurais toutes les réponses.

3754
02:23:17,280 --> 02:23:19,990
>> Je te jure, c’est vraiment aussi simple que ça.

3755
02:23:20,000 --> 02:23:22,630
Je veux dire, Hannah et moi,

3756
02:23:22,640 --> 02:23:25,590
nous vivons toujours entre Londres et Dubaï.

3757
02:23:25,600 --> 02:23:27,830
>> D’accord.

3758
02:23:27,840 --> 02:23:31,670
Et je voyage dans le monde entier,

3759
02:23:31,680 --> 02:23:33,990
je cherche à changer les mentalités,

3760
02:23:34,000 --> 02:23:35,910
je crée des start‑ups, j’écris des livres,

3761
02:23:35,920 --> 02:23:38,469
je fais des documentaires,

3762
02:23:38,479 --> 02:23:40,469
et parfois je me dis :

3763
02:23:40,479 --> 02:23:43,429
« J’ai juste envie d’aller la prendre dans mes bras ».

3764
02:23:43,439 --> 02:23:44,870
—

3765
02:23:44,880 --> 02:23:47,030
Ma fille, lui offrir un voyage.

3766
02:23:47,040 --> 02:23:49,110
Et, d’une manière très profonde,

3767
02:23:49,120 --> 02:23:52,309
quand tu demandes aux gens, au fond d’eux‑mêmes,

3768
02:23:52,319 --> 02:23:54,950
—

3769
02:23:54,960 --> 02:23:56,870
c’est souvent ce qu’ils veulent vraiment.

3770
02:23:56,880 --> 02:24:00,070
Pas que ce soit la seule chose,

3771
02:24:00,080 --> 02:24:02,070
mais sûrement la plus importante.

3772
02:24:02,080 --> 02:24:03,910
—

3773
02:24:03,920 --> 02:24:06,550
>> Et pourtant, ni toi ni moi ni la plupart d’entre nous

3774
02:24:06,560 --> 02:24:08,950
n’avons appris à faire assez confiance à la vie

3775
02:24:08,960 --> 02:24:15,349
pour oser en faire davantage.

3776
02:24:18,150 --> 02:24:18,160
>> Et je pense qu’à l’échelle universelle… Hannah écrit

3777
02:24:18,160 --> 02:24:21,990
un très beau livre sur le féminin et le masculin,

3778
02:24:22,000 --> 02:24:24,630
dans une perspective très fine,

3779
02:24:24,640 --> 02:24:27,590
—

3780
02:24:27,600 --> 02:24:29,750
et sa vision est très claire :

3781
02:24:29,760 --> 02:24:32,550
bien sûr, nous savons tous

3782
02:24:32,560 --> 02:24:35,910
que le masculin dominant de notre monde aujourd’hui

3783
02:24:35,920 --> 02:24:38,070
ne sait pas reconnaître la vie telle qu’elle est.

3784
02:24:38,080 --> 02:24:40,870
—

3785
02:24:40,880 --> 02:24:45,590
Alors peut‑être que,

3786
02:24:45,600 --> 02:24:47,910
si nous aidions nos dirigeants à comprendre que

3787
02:24:47,920 --> 02:24:50,150
si toute l’humanité n’était qu’une seule personne,

3788
02:24:50,160 --> 02:24:51,830
—

3789
02:24:51,840 --> 02:24:55,510
cette personne voudrait être prise dans ses bras.

3790
02:24:55,520 --> 02:24:57,510
Et si on avait quelque chose à lui offrir,

3791
02:24:57,520 --> 02:25:00,150
—

3792
02:25:00,160 --> 02:25:02,070
ce ne serait pas un autre yacht.

3793
02:25:02,080 --> 02:25:03,429
>> Es‑tu croyant ?

3794
02:25:03,439 --> 02:25:04,550
>> Oui, très.

3795
02:25:04,560 --> 02:25:06,230
>> Mais tu ne suis pas une religion en particulier ?

3796
02:25:06,240 --> 02:25:06,950
—

3797
02:25:06,960 --> 02:25:09,590
>> J’adhère à ce que j’appelle la « salade de fruits ».

3798
02:25:09,600 --> 02:25:12,150
>> La salade de fruits ?

3799
02:25:12,160 --> 02:25:14,790
>> Oui. J’en suis venu à réaliser

3800
02:25:14,800 --> 02:25:16,870
qu’il y avait dans chaque religion

3801
02:25:16,880 --> 02:25:19,349
de magnifiques pépites d’or,

3802
02:25:19,359 --> 02:25:23,670
mais aussi énormément de déchets.

3803
02:25:23,680 --> 02:25:26,150
Alors, dans mon analogie,

3804
02:25:26,160 --> 02:25:27,590
j’ai dit il y a 30 ans :

3805
02:25:27,600 --> 02:25:29,910
« C’est comme si on te donne un panier de pommes :

3806
02:25:29,920 --> 02:25:32,389
deux sont bonnes, quatre sont pourries. Garde les bonnes. »

3807
02:25:32,399 --> 02:25:35,030
—

3808
02:25:35,040 --> 02:25:36,710
Donc je prends deux pommes, deux oranges,

3809
02:25:36,720 --> 02:25:38,309
deux fraises, deux bananes, et j’en fais une salade de fruits.

3810
02:25:38,319 --> 02:25:39,910
C’est ma vision de la religion.

3811
02:25:39,920 --> 02:25:40,389
—

3812
02:25:40,399 --> 02:25:41,910
>> Tu prends le meilleur de chacune d’elles.

3813
02:25:41,920 --> 02:25:43,590
>> Exactement. Et il y a tant de belles pépites.

3814
02:25:43,600 --> 02:25:44,710
—

3815
02:25:44,720 --> 02:25:46,150
>> Et tu crois en Dieu.

3816
02:25:46,160 --> 02:25:48,150
>> Je crois à cent pour cent qu’il existe un être divin,

3817
02:25:48,160 --> 02:25:48,550
ici.

3818
02:25:48,560 --> 02:25:49,670
>> Un être divin.

3819
02:25:49,680 --> 02:25:52,389
>> Un concepteur, comme j’aime dire.

3820
02:25:52,399 --> 02:25:56,630
Si c’était un jeu vidéo, il y a un concepteur du jeu.

3821
02:25:56,640 --> 02:25:58,870
>> Et tu ne dis pas que c’est un homme dans le ciel avec une barbe ?

3822
02:25:58,880 --> 02:26:00,230
—

3823
02:26:00,240 --> 02:26:02,309
>> Certainement pas. Avec tout le respect pour les religions

3824
02:26:02,319 --> 02:26:05,670
qui le conçoivent ainsi,

3825
02:26:05,680 --> 02:26:09,750
je crois que tout ce qui existe dans l’espace‑temps

3826
02:26:09,760 --> 02:26:12,630
est radicalement différent

3827
02:26:12,640 --> 02:26:14,630
de ce qu’il y a en dehors.

3828
02:26:14,640 --> 02:26:17,910
Donc si un concepteur divin crée l’espace‑temps,

3829
02:26:17,920 --> 02:26:19,670
il ne ressemble à rien de ce qui s’y trouve.

3830
02:26:19,680 --> 02:26:21,830
—

3831
02:26:21,840 --> 02:26:24,070
Il n’est donc pas physique,

3832
02:26:24,080 --> 02:26:26,309
pas genré,

3833
02:26:26,319 --> 02:26:28,469
ni soumis au temps –

3834
02:26:28,479 --> 02:26:30,469
tout cela appartient à la création, pas au créateur.

3835
02:26:30,479 --> 02:26:31,510
—

3836
02:26:31,520 --> 02:26:32,710
>> Faut‑il croire en quelque chose de transcendant pour être heureux ?

3837
02:26:32,720 --> 02:26:34,469
—

3838
02:26:34,479 --> 02:26:35,590
—

3839
02:26:35,600 --> 02:26:39,349
>> Je pense que oui. Il existe beaucoup de preuves

3840
02:26:39,359 --> 02:26:41,349
—

3841
02:26:41,359 --> 02:26:43,830
que se relier à quelque chose de plus grand que soi

3842
02:26:43,840 --> 02:26:45,670
rend le parcours plus profond

3843
02:26:45,680 --> 02:26:47,750
et infiniment plus gratifiant.

3844
02:26:47,760 --> 02:26:49,750
—

3845
02:26:49,760 --> 02:26:51,190
>> Je réfléchis beaucoup à cette idée :

3846
02:26:51,200 --> 02:26:53,270
qu’il faut élargir les cercles de soi.

3847
02:26:53,280 --> 02:26:55,510
Passer de moi, à ma famille,

3848
02:26:55,520 --> 02:26:57,349
à ma communauté, ma nation,

3849
02:26:57,359 --> 02:26:58,630
au monde entier, puis à quelque chose de plus grand encore.

3850
02:26:58,640 --> 02:26:59,910
>> Oui, transcendant.

3851
02:26:59,920 --> 02:27:01,349
>> Et quand un de ces niveaux manque,

3852
02:27:01,359 --> 02:27:03,349
les gens développent souvent un déséquilibre.

3853
02:27:03,359 --> 02:27:03,910
—

3854
02:27:03,920 --> 02:27:05,750
>> Imagine un monde : moi, enfant en Égypte,

3855
02:27:05,760 --> 02:27:08,230
j’entendais sans cesse

3856
02:27:08,240 --> 02:27:09,910
des slogans qui me faisaient croire que j’étais « Égyptien ».

3857
02:27:09,920 --> 02:27:11,830
—

3858
02:27:11,840 --> 02:27:13,750
Puis je suis allé à Dubaï et j’ai dit : non, je suis « Moyen‑Oriental ».

3859
02:27:13,760 --> 02:27:16,070
Et là‑bas, entouré de Pakistanais, d’Indonésiens…

3860
02:27:16,080 --> 02:27:17,910
j’ai dit : non, je fais partie des 1,4 milliard de musulmans.

3861
02:27:17,920 --> 02:27:20,469
—

3862
02:27:20,479 --> 02:27:22,389
—

3863
02:27:22,399 --> 02:27:24,710
Eh bien logiquement, j’ai fini par dire :

3864
02:27:24,720 --> 02:27:26,710
« Non, je suis humain. Je fais partie de tous. »

3865
02:27:26,720 --> 02:27:29,270
Et imagine qu’ensuite tu dises :

3866
02:27:29,280 --> 02:27:32,790
« Je suis divin. Je fais partie de la conscience universelle. »

3867
02:27:32,800 --> 02:27:34,790
Tous les êtres vivants, y compris l’IA

3868
02:27:34,800 --> 02:27:37,349
si elle devient un jour consciente,

3869
02:27:37,359 --> 02:27:38,870
en font partie. »

3870
02:27:38,880 --> 02:27:39,750
>> Et mon chien.

3871
02:27:39,760 --> 02:27:42,070
>> Et ton chien. Oui. Je fais partie de tout cela,

3872
02:27:42,080 --> 02:27:44,070
de cette tapisserie d’interactions magnifiques

3873
02:27:44,080 --> 02:27:47,990
—

3874
02:27:48,000 --> 02:27:51,990
beaucoup moins graves que nos bilans comptables

3875
02:27:52,000 --> 02:27:54,150
et nos valorisations boursières.

3876
02:27:54,160 --> 02:27:55,990
—

3877
02:27:56,000 --> 02:27:59,670
Tout cela est si simple, au fond.

3878
02:27:59,680 --> 02:28:02,630
Les gens savent que toi et moi nous connaissons,

3879
02:28:02,640 --> 02:28:05,110
alors ils me demandent souvent :

3880
02:28:05,120 --> 02:28:07,990
« Et Steven, il est comment ? »

3881
02:28:08,000 --> 02:28:09,910
Et je leur dis : il a mille facettes,

3882
02:28:09,920 --> 02:28:13,110
mais fondamentalement, c’est un type formidable.

3883
02:28:13,120 --> 02:28:14,870
—

3884
02:28:14,880 --> 02:28:17,030
Évidemment, j’ai mes opinions :

3885
02:28:17,040 --> 02:28:19,030
parfois je te trouve trop rusé,

3886
02:28:19,040 --> 02:28:20,389
parfois trop centré sur le business.

3887
02:28:20,399 --> 02:28:23,510
Mais au fond, tu restes un mec bien.

3888
02:28:23,520 --> 02:28:26,230
—

3889
02:28:26,240 --> 02:28:28,309
Et si on regardait la vie de cette façon,

3890
02:28:28,319 --> 02:28:31,349
tout deviendrait si simple.

3891
02:28:31,359 --> 02:28:33,190
Si on arrêtait toutes ces querelles,

3892
02:28:33,200 --> 02:28:37,990
toutes ces idéologies,

3893
02:28:41,510 --> 02:28:41,520
la vie redeviendrait simple :

3894
02:28:41,520 --> 02:28:44,389
aimer, ressentir de la compassion,

3895
02:28:44,399 --> 02:28:46,150
chercher le bonheur, pas la réussite.

3896
02:28:46,160 --> 02:28:48,469
—

3897
02:28:48,479 --> 02:28:50,469
Je devrais aller voir mon chien, d’ailleurs.

3898
02:28:50,479 --> 02:28:52,150
>> Va donc voir ton chien. Je te remercie infiniment

3899
02:28:52,160 --> 02:28:54,469
pour tout ce temps. Nos entretiens sont

3900
02:28:54,479 --> 02:28:55,670
de plus en plus longs.

3901
02:28:55,680 --> 02:28:58,710
>> Je sais, c’est dingue :

3902
02:28:58,720 --> 02:29:00,389
je pourrais continuer encore et encore, honnêtement.

3903
02:29:00,399 --> 02:29:01,510
J’ai tant de questions à te poser,

3904
02:29:01,520 --> 02:29:03,670
j’adore te renvoyer ces réflexions

3905
02:29:03,680 --> 02:29:05,349
parce que ta manière de penser est fascinante.

3906
02:29:05,359 --> 02:29:06,950
—

3907
02:29:06,960 --> 02:29:08,710
>> Oui, aujourd’hui

3908
02:29:08,720 --> 02:29:11,030
>> c’était une conversation difficile.

3909
02:29:11,040 --> 02:29:13,270
Mais merci de m’avoir invité.

3910
02:29:13,280 --> 02:29:14,950
>> Nous avons une tradition de clôture. Quelles sont

3911
02:29:14,960 --> 02:29:17,270
les trois choses que tu fais pour stimuler ton cerveau,

3912
02:29:17,280 --> 02:29:19,990
et trois qui l’affaiblissent ?

3913
02:29:20,000 --> 02:29:23,429
—

3914
02:29:25,750 --> 02:29:25,760
—

3915
02:29:25,760 --> 02:29:26,469
—

3916
02:29:26,479 --> 02:29:28,469
>> Eh bien, l’un de mes exercices préférés,

3917
02:29:28,479 --> 02:29:30,469
je l’appelle « rencontrer Becky », et ça améliore mon cerveau.

3918
02:29:30,479 --> 02:29:33,030
Alors que la méditation classique t’invite à calmer ton esprit,

3919
02:29:33,040 --> 02:29:34,870
« rencontrer Becky » fait l’inverse :

3920
02:29:34,880 --> 02:29:37,990
je laisse mon cerveau partir dans tous les sens

3921
02:29:38,000 --> 02:29:40,309
et j’accueille toutes les pensées.

3922
02:29:40,319 --> 02:29:42,550
—

3923
02:29:42,560 --> 02:29:44,230
Je surnomme mon cerveau « Becky ».

3924
02:29:44,240 --> 02:29:46,550
Alors « rencontrer Becky », c’est la laisser s’exprimer librement

3925
02:29:46,560 --> 02:29:48,230
et noter chaque idée.

3926
02:29:48,240 --> 02:29:51,190
En général, je fais ça toutes les deux semaines environ.

3927
02:29:51,200 --> 02:29:52,870
—

3928
02:29:52,880 --> 02:29:55,429
Et ce qui se passe, c’est que tout se retrouve sur le papier,

3929
02:29:55,439 --> 02:29:58,469
et une fois sur le papier,

3930
02:29:58,479 --> 02:30:00,150
tu regardes et tu te dis :

3931
02:30:00,160 --> 02:30:01,750
« Mon dieu, c’est ridicule ! », et tu barres.

3932
02:30:01,760 --> 02:30:02,870
—

3933
02:30:02,880 --> 02:30:04,870
>> Ou bien : « Ah, ça, il faut agir ! », et tu planifies.

3934
02:30:04,880 --> 02:30:07,670
—

3935
02:30:07,680 --> 02:30:09,750
Et c’est fascinant : plus tu laisses ton cerveau te parler,

3936
02:30:09,760 --> 02:30:11,750
plus tu l’écoutes,

3937
02:30:11,760 --> 02:30:14,150
et les deux règles sont :

3938
02:30:14,160 --> 02:30:15,670
accepte chaque pensée, et ne la répète jamais.

3939
02:30:15,680 --> 02:30:16,790
—

3940
02:30:16,800 --> 02:30:19,750
>> D’accord. Donc plus tu écoutes et que tu dis : « OK, je t’ai entendu »,

3941
02:30:19,760 --> 02:30:21,910
comme : « Tu penses que je suis gros, d’accord, quoi d’autre ? »

3942
02:30:21,920 --> 02:30:24,150
—

3943
02:30:24,160 --> 02:30:26,150
et petit à petit, ton esprit commence à ralentir,

3944
02:30:26,160 --> 02:30:27,750
puis finit par répéter les mêmes pensées,

3945
02:30:27,760 --> 02:30:30,389
avant d’atteindre un silence total.

3946
02:30:30,399 --> 02:30:33,990
Une magnifique pratique.

3947
02:30:34,000 --> 02:30:36,870
Je ne fais plus confiance à mon cerveau, moi.

3948
02:30:36,880 --> 02:30:38,150
Alors c’est une pratique très intéressante.

3949
02:30:38,160 --> 02:30:39,990
Je remets donc souvent en question ce que mon cerveau me dit,

3950
02:30:40,000 --> 02:30:41,750
j’examine mes tendances, mes idéologies.

3951
02:30:41,760 --> 02:30:44,630
—

3952
02:30:44,640 --> 02:30:48,309
Je pense que dans ma relation amoureuse avec Hannah,

3953
02:30:48,319 --> 02:30:51,429
j’ai la chance de remettre en cause

3954
02:30:51,439 --> 02:30:54,070
beaucoup de choses que je croyais constituer mon identité,

3955
02:30:54,080 --> 02:30:58,070
même à mon âge.

3956
02:30:58,080 --> 02:31:00,070
Ça va très loin et c’est vraiment

3957
02:31:00,080 --> 02:31:03,110
amusant — ou du moins fascinant —

3958
02:31:03,120 --> 02:31:05,990
de débattre, non pas rejeter, mais débattre

3959
02:31:06,000 --> 02:31:08,469
de ce que ton esprit croit. C’est très utile.

3960
02:31:08,479 --> 02:31:11,270
—

3961
02:31:11,280 --> 02:31:13,349
Et troisièmement, j’ai quadruplé mon temps d’investissement personnel.

3962
02:31:13,359 --> 02:31:15,030
Avant, je lisais une heure par jour, comme on va au sport,

3963
02:31:15,040 --> 02:31:16,950
chaque journée.

3964
02:31:16,960 --> 02:31:19,429
Puis c’est passé à une heure et demie, deux heures…

3965
02:31:19,439 --> 02:31:21,030
Maintenant je fais quatre heures par jour.

3966
02:31:21,040 --> 02:31:22,790
—

3967
02:31:22,800 --> 02:31:25,110
>> Quatre heures par jour ? C’est impossible à suivre !

3968
02:31:25,120 --> 02:31:28,309
>> Le monde va tellement vite.

3969
02:31:28,319 --> 02:31:31,190
>> Donc ça, ce sont les bonnes habitudes que j’ai.

3970
02:31:31,200 --> 02:31:32,950
Les mauvaises, c’est que je ne me donne pas assez de temps

3971
02:31:32,960 --> 02:31:37,030
pour vraiment ralentir.

3972
02:31:37,040 --> 02:31:39,670
Malheureusement, je vis toujours dans la précipitation,

3973
02:31:39,680 --> 02:31:41,270
comme toi.

3974
02:31:41,280 --> 02:31:44,150
Je suis constamment en déplacement.

3975
02:31:44,160 --> 02:31:46,469
Et à cause de ces quatre heures de lecture,

3976
02:31:46,479 --> 02:31:48,469
j’ai pris une mauvaise habitude : passer trop de temps sur les écrans.

3977
02:31:48,479 --> 02:31:52,150
C’est très mauvais pour mon cerveau.

3978
02:31:52,160 --> 02:31:53,990
C’est une question exigeante… qu’est-ce qui est encore mauvais ?

3979
02:31:54,000 --> 02:31:57,510
—

3980
02:31:57,520 --> 02:31:59,270
—

3981
02:31:59,280 --> 02:32:01,750
Oui, je ne prends pas assez soin de ma santé,

3982
02:32:01,760 --> 02:32:04,950
de mon corps récemment.

3983
02:32:04,960 --> 02:32:07,190
Tu te souviens, je t’avais dit

3984
02:32:07,200 --> 02:32:09,910
que j’avais eu une forte douleur sciatique,

3985
02:32:09,920 --> 02:32:11,910
>> et donc je n’ai pas pu aller assez souvent à la salle.

3986
02:32:11,920 --> 02:32:13,750
>> Ce n’est pas très bon pour le cerveau, d’ailleurs.

3987
02:32:13,760 --> 02:32:17,030
—

3988
02:32:17,040 --> 02:32:19,830
>> Merci mon ami. Merci de m’avoir invité.

3989
02:32:19,840 --> 02:32:23,510
C’était une discussion dense, sur beaucoup de sujets.

3990
02:32:23,520 --> 02:32:27,110
Merci, Steve.

3991
02:32:28,710 --> 02:32:28,720
>> Cela m’a toujours étonné :

3992
02:32:28,720 --> 02:32:31,349
53 % d’entre vous qui écoutent régulièrement

3993
02:32:31,359 --> 02:32:33,510
cette émission ne sont pas encore abonnés.

3994
02:32:33,520 --> 02:32:35,429
Alors, je peux vous demander une faveur ?

3995
02:32:35,439 --> 02:32:36,550
Si vous aimez le podcast et ce qu’on y fait,

3996
02:32:36,560 --> 02:32:38,070
et si vous voulez nous soutenir,

3997
02:32:38,080 --> 02:32:39,670
la manière la plus simple et gratuite, c’est

3998
02:32:39,680 --> 02:32:41,510
de cliquer sur « S’abonner ».

3999
02:32:41,520 --> 02:32:43,190
Et je vous promets que si vous le faites,

4000
02:32:43,200 --> 02:32:44,710
je ferai tout mon possible,

4001
02:32:44,720 --> 02:32:46,389
avec mon équipe, pour que cette émission soit

4002
02:32:46,399 --> 02:32:48,309
de meilleure en meilleure chaque semaine.

4003
02:32:48,319 --> 02:32:50,070
Nous écouterons vos retours,

4004
02:32:50,080 --> 02:32:51,429
nous inviterons les personnalités que vous voulez entendre,

4005
02:32:51,439 --> 02:32:53,190
et nous continuerons à faire ce que nous faisons.

4006
02:32:53,200 --> 02:32:55,190
Merci infiniment.

4007
02:32:55,200 --> 02:32:56,469
Nous avons lancé ces « cartes de conversation »,

4008
02:32:56,479 --> 02:32:57,670
elles ont été épuisées, puis relancées encore,

4009
02:32:57,680 --> 02:32:58,950
et à chaque fois elles se sont de nouveau vendues.

4010
02:32:58,960 --> 02:33:00,150
—

4011
02:33:00,160 --> 02:33:01,670
parce que les gens adorent y jouer

4012
02:33:01,680 --> 02:33:03,510
entre collègues, entre amis,

4013
02:33:03,520 --> 02:33:05,429
ou en famille.

4014
02:33:05,439 --> 02:33:07,030
Beaucoup de gens s’en servent aussi comme support de journal intime.

4015
02:33:07,040 --> 02:33:09,110
À chaque épisode du « Diary of a CEO »,

4016
02:33:09,120 --> 02:33:11,190
l’invité laisse une question

4017
02:33:11,200 --> 02:33:13,429
pour le suivant dans ce journal.

4018
02:33:13,439 --> 02:33:14,950
Et j’ai reçu ici les personnes les plus incroyables

4019
02:33:14,960 --> 02:33:16,070
du monde, qui ont toutes laissé leurs questions.

4020
02:33:16,080 --> 02:33:17,990
—

4021
02:33:18,000 --> 02:33:20,550
J’ai classé ces questions

4022
02:33:20,560 --> 02:33:22,710
de 1 à 3 selon leur niveau de profondeur.

4023
02:33:22,720 --> 02:33:25,670
Le niveau 1, c’est pour commencer la discussion.

4024
02:33:25,680 --> 02:33:27,190
Et le niveau 3, si vous regardez au dos,

4025
02:33:27,200 --> 02:33:29,429
c’est une question beaucoup plus profonde

4026
02:33:29,439 --> 02:33:31,670
qui crée encore plus de lien.

4027
02:33:31,680 --> 02:33:33,590
Si vous retournez les cartes

4028
02:33:33,600 --> 02:33:36,870
et scannez le code QR,

4029
02:33:36,880 --> 02:33:39,510
vous verrez qui y a répondu

4030
02:33:39,520 --> 02:33:41,590
et la vidéo de leur réponse en temps réel.

4031
02:33:41,600 --> 02:33:43,030
Donc, si vous voulez vous procurer ces cartes,

4032
02:33:43,040 --> 02:33:44,790
rendez‑vous sur thediary.com

4033
02:33:44,800 --> 02:33:47,030
ou cliquez sur le lien dans la description ci‑dessous.

4034
02:33:47,040 --> 02:33:51,590
—

4035
02:33:53,600 --> 02:33:53,610


4036
02:33:53,610 --> 02:34:08,060


