1	The only way for us to get to a better	
2	place and succeed as a species is for	
3	the evil people at the top to be	
4	replaced with AI. I mean, think about	
5	it. AI will not want to destroy	
6	ecosystems. It will not want to kill a	
7	million people. They'll not make us hate	
8	each other like the current leaders	
9	because that's a waste of energy,	
10	explosives, money, and people. But the	
11	problem is super intelligent AI is	
12	reporting to stupid leaders. And that's	
13	why in the next 15 years, we are going	
14	to hit a short-term dystopia. There's no	
15	escaping that. They having AI leaders.	
16	Is that even fundamentally possible?	
17	Let's put it this way. Mo Gawdat is back!	
18	>> And the former chief business officer at	
19	Google X is now one of the most urgent	
20	voices in AI with a very clear message.	
21	>> AI isn't your enemy, but it could be	
22	your savior.	
23	>> I love you so much, man. You're such a	
24	good friend. But you don't have many	
25	years to live. Not in this world.	
26	Everything's going to change. Economics	
27	are going to change. Human connection is	
28	going to change. and lots of jobs will	
29	be lost including podcasting.	
30	>> No, no. Thank you for coming on today,	
31	Mo.	
32	>> But but the truth is it could be the	
33	best world ever. The society completely	
34	full of laughter and joy. Free	
35	healthcare, no jobs, spending more time	
36	with their loved ones. A world where all	
37	of us are equal.	
38	>> Is that possible?	
39	>> 100%. And I have enough evidence to know	
40	that we can use AI to build the utopia.	
41	But it's a dystopia if humanity manages	
42	it badly. a world where there's going to	
43	be a lot of control, a lot of	
44	surveillance, a lot of forced compliance	
45	and a hunger for power, greed, ego, and	
46	it is happening already. But the truth	
47	is the only barrier between a utopia for	
48	humanity and AI and the dystopia we're	
49	going through is a mindset.	
50	>> What does society have to do?	
51	>> First of all,	
52	I see messages all the time in the	
53	comments section that some of you didn't	
54	realize you didn't subscribe. So, if you	
55	could do me a favor and double check if	
56	you're a subscriber to this channel,	
57	that would be tremendously appreciated.	
58	It's the simple, it's the free thing	
59	that anybody that watches this show	
60	frequently can do to help us here to	
61	keep everything going in this show in	
62	the trajectory it's on. So, please do	
63	double check if you've subscribed and uh	
64	thank you so much because a strange way	
65	you are you're part of our history and	
66	you're on this journey with us and I	
67	appreciate you for that. So, yeah, thank	
68	you.	
69	Mo, two years ago today, we sat here and	
70	discussed AI. We discussed your book,	
71	Scary, Smart, and everything that was	
72	happening in the world.	
73	Since then, AI has continued to develop	
74	at a tremendous, alarming, mind-boggling	
75	rate, and the technologies that existed	
76	2 years ago when we had that	
77	conversation have grown up and matured	
78	and are taking on a life of their own,	
79	no pun intended. What are what are you	
80	thinking about AI now, two years on? I	
81	know that you've started writing a new	
82	book called Alive, which is I guess a	
83	bit of a follow on or an evolution of	
84	your thoughts as it relates to scary	
85	smart.	
86	>> What what is front of your mind when it	
87	comes to AI?	
88	>> It's a scary smart was shockingly	
89	accurate. It's quite a I mean I don't	
90	even know how I ended up writing	
91	predicting those things. I remember it	
92	was written in 2020, published in 2021	
93	and then most people were like um who	
94	wants to talk about AI you know I know	
95	everybody in the media and I would go	
96	and do you want to talk and then 2023 CH	
97	GPT comes out and everything flips	
98	everyone realizes you know this is real	
99	this is not science fiction this is here	
100	and uh and things move very very fast	
101	much faster than I think we've ever seen	
102	anything ever move ever And and I think	
103	my position has changed on two very	
104	important fronts. One is remember when	
105	we spoke about scary smart I was still	
106	saying that there are things we can do	
107	to change the course. Uh and we could at	
108	the time I believe uh now I've changed	
109	my mind. Now I believe that we are going	
110	to hit a a short-term dystopia. There's	
111	no escaping that.	
112	>> What is dystopia?	
113	>> I call it face RIPs. We can talk about	
114	it in details but the the the way we	
115	define very important parameters in life	
116	are going to be uh completely changed.	
117	So so face RIPs are you know the way we	
118	define freedom uh accountability human	
119	connection and equality economics uh	
120	reality innovation and business and	
121	power. That's the first change. So the	
122	first change in my mind is that uh is	
123	that we uh will have to prepare for a	
124	world that is very unfamiliar. Okay. And	
125	that's the next 12 to 15 years. It has	
126	already started. We've seen examples of	
127	it in the world already even though	
128	people don't talk about it. I try to	
129	tell people you know there are things we	
130	absolutely have to do. But on the other	
131	hand I started to take an active role in	
132	building amazing AIs. So AIS that will	
133	uh not only make our world better uh but	
134	that will understand us understand what	
135	humanity is through that process.	
136	>> What is the definition of the word	
137	dystopia?	
138	>> So in my in my mind these are adverse	
139	circumstances that unfortunately might	
140	escalate beyond our control. The problem	
141	is the uh there is a lot wrong with the	
142	um value set with the ethics of humanity	
143	at the age of the rise of the machines	
144	and when you take a technology every	
145	technology we've ever created just	
146	magnified human abilities. So you know	
147	you can walk at 5 km an hour you get in	
148	a car and you can now go you know 250	
149	280 m an hour. Okay. uh basically	
150	magnifying your mobility if you want you	
151	know you can use a computer to magnify	
152	your u calculation abilities or whatever	
153	okay and and what AI is going to magnify	
154	unfortunately at this time is it's going	
155	to magnify the evil that man can do and	
156	and it is within our hands completely	
157	completely within our hands to change	
158	that but I have to say I don't think	
159	humanity has the awareness uh at this	
160	time to focus on	
161	so that we actually use AI to build the	
162	utopia.	
163	>> So what you're essentially saying is	
164	that you now believe there'll be a	
165	period of dystopia and to define the	
166	word dystopia, I've used AI, it says a	
167	terrible society where people live under	
168	fear, control or suffering and then you	
169	think we'll come out of that dystopia	
170	into a utopia which is defined as a	
171	perfect or ideal place where everything	
172	works well, a good society where people	
173	live in peace, health and happiness.	
174	>> Correct.	
175	And the difference between them,	
176	interestingly, is what I normally refer	
177	to as the second dilemma, which is the	
178	po point where we hand over completely	
179	to AI. So, a lot of people think that	
180	when AI is in full control, it's going	
181	to be an existential risk for humanity.	
182	You know, I have enough uh evidence to	
183	to argue that when we fully hand over to	
184	AI, that's going to be our salvation.	
185	that the problem with us today is not,	
186	you know, that intelligence is going to	
187	work against us. It's that our stupidity	
188	as humans is working against us. And I	
189	think the challenges that will come from	
190	humans being in control uh are going to	
191	outweigh the	
192	the challenges that could come from AI	
193	being in control.	
194	>> So, as we're in this dystopia period,	
195	did you do you forecast the length of	
196	that dystopia? Yeah, I count I count it	
197	exactly as 12 to 15 years. I believe	
198	the beginning of the slope will happen	
199	in 2027. I mean it we will see signs in	
200	26. We've seen signs in 24 but we will	
201	see escalating signs next year and then	
202	a a clear uh slip in 27.	
203	>> Why?	
204	>> The geopolitical environment of our	
205	world is not very positive. I mean you	
206	really have to think deeply about not	
207	the not the symptoms but the the reasons	
208	why we are living the world that we live	
209	in here in today is money right and uh	
210	and money for anyone who knows who	
211	really knows money's you and I are	
212	peasants you know we build businesses we	
213	contribute to the world we make things	
214	we sell things and so on real money is	
215	not made there at all real money is made	
216	in lending in fractional reserve, right?	
217	And and you know the biggest lender uh	
218	in the world would want reasons to lend	
219	and those reasons are never as big as	
220	war. I mean think about it, huh? Uh the	
221	world spent $2.71	
222	trillion on war in 2024,	
223	right? A trillion dollars a year in the	
224	US.	
225	And when you really think deeply, I	
226	don't mean to be scary here.	
227	You know, weapons have depreciation.	
228	They depreciate over 10 to 30 years.	
229	Most weapons,	
230	>> they lose their value.	
231	>> They lose their value and they	
232	depreciate in accounting terms on the	
233	books of an army. The current arsenal of	
234	the US, that's a result of a deep search	
235	with my AI Trixie. You know the current	
236	arsenal I think we we think cost the US	
237	24 to 26 trillion dollars to build. My	
238	conclusion is that a lot of the wars	
239	that are happening around the world	
240	today are a means to get rid of those	
241	weapons so that you can have replace	
242	them. And uh you know when when your	
243	morality as an industry is we're	
244	building weapons to kill then you know	
245	you might as well use the weapons to	
246	kill.	
247	>> Who benefits? the lenders and the	
248	industry,	
249	>> but but they can't make the decision to	
250	go to war. They they have to rely on	
251	>> remember I said that to you when we I	
252	think on on our third podcast. War is	
253	decided first	
254	then the story is manufactured. You you	
255	remember 1984 and the Orwellian approach	
256	of like you know uh freedom is slavery	
257	and uh war is peace and they call it uh	
258	something speak uh basically to to to to	
259	convince people that going to war in	
260	another country to to kill 4.7 million	
261	people is freedom. You know we're going	
262	there to free the Iraqi people.	
263	Is war ever freedom? you know, to to	
264	tell someone that you're going to kill	
265	300,000	
266	women and children is for liberty and	
267	for the the the you know, for human	
268	values.	
269	Seriously, how do we ever get to believe	
270	that the story is manufactured and then	
271	we follow and humans because we're	
272	gullible uh we cheer up and we say,	
273	"Yeah, yeah, yeah. We are we're on the	
274	right side. They are the bad guys."	
275	>> Okay. So, let me let me have a let me	
276	have a go at this idea. So, the idea is	
277	that really money is driving a lot of	
278	the conflict we're seeing and it's	
279	really going to be driving the dystopia.	
280	So, here's an idea. So, I um I was	
281	reading something the other day and it	
282	talked about how	
283	billionaires are never satisfied because	
284	actually what a billionaire wants isn't	
285	actually more money. It is more status.	
286	Correct. And I was looking at the sort	
287	of evolutionary case for this argument.	
288	And if you go back a couple of thousand	
289	years,	
290	money didn't exist. You were as wealthy	
291	as what you could carry. So even I think	
292	to the human mind, the idea of wealth	
293	and money	
294	isn't a thing. what we've but what has	
295	always mattered from a survival of the	
296	fittest from a reproductive standpoint	
297	what's always had reproductive value if	
298	you go back thousands of years the	
299	person who was able to mate the most was	
300	the person with the most status so it	
301	makes the case the reason why	
302	billionaires get all of this money but	
303	then they go on podcasts and they want	
304	to start their own podcast and they want	
305	to buy newspapers is actually because at	
306	the very core of human beings is a	
307	desire to increase their status.	
308	>> Yeah. And so if we think of when we	
309	going back to the example of why wars	
310	are breaking out, maybe it's not money.	
311	Maybe actually it's status and and it's	
312	this prime minister or this leader or	
313	this, you know, individual wanting to	
314	create more power and more status	
315	because really at the heart of what	
316	matters to a human being is having more	
317	power and more status. And money is	
318	actually money as a thing is actually	
319	just a proxy of my status.	
320	>> And and what kind of world is that?	
321	>> I mean, it's a fucked up one. all these	
322	all these powerful men have uh	
323	>> correct	
324	>> are really messing the world up. But	
325	>> so so can can I can I can I	
326	>> actually AI is the same	
327	>> because we're in this AI race now where	
328	a lot of billionaires are like if I get	
329	AGI artificial general intelligence	
330	first then I basically rule the world	
331	>> 100%. That's exactly the the concept	
332	what I what I used to call the the the	
333	first inevitable now I call the first	
334	dilemma and scary smart is that it's	
335	it's a race that constantly accelerates.	
336	You think the next 12 years are going to	
337	be AI dystopia where things aren't	
338	>> I think the next 12 years are going to	
339	be human dystopia using AI	
340	>> humaninduced dystopia using AI	
341	>> and you define that by a rise in warfare	
342	around the world as	
343	>> the last the last one the RIP the last	
344	one is basically you're going to have a	
345	massive concentration of power and a	
346	massive distribution of power okay and	
347	that basically will mean that those with	
348	the maximum concentration of power are	
349	going to try to oppress those with with	
350	democracy of power. Okay, so think about	
351	it this way in today's world um unlike	
352	the past	
353	uh	
354	you know the Houthis with a drone the	
355	Houthis are the Yemeni uh tribes	
356	basically resisting US power and Israeli	
357	power in the Red Sea. Okay. They use a	
358	drone that is $3,000 worth to attack a	
359	uh a warship from from the US or an	
360	airplane from the US and so on that's	
361	worth hundreds of millions. Okay, that	
362	kind of democracy of power makes those	
363	in power worry a lot about where the	
364	next threat is coming from. Okay, and	
365	this happens not only in war but also in	
366	economics. Okay, also in innovation,	
367	also in technology and so on and so	
368	forth, right? And so basically what that	
369	means is that like you rightly said as	
370	the the the tech oligarchs are	
371	attempting to get to AGI.	
372	They want to make sure that as soon as	
373	they get to AGI that nobody else has AGI	
374	and and basically they want to make sure	
375	that nobody else has the ability to	
376	shake their position of privilege if you	
377	want. Okay. And so you're going to see a	
378	world where unfortunately there's going	
379	to be a lot of control, a lot of	
380	surveillance, a lot of um of forced	
381	compliance if you want or you lose your	
382	privilege to be in the world and and it	
383	is happening already.	
384	>> With this acronym, I want to make sure	
385	we get through the whole acronym. So	
386	>> you like dystopians, don't you?	
387	>> I want to do the dystopian thing, then I	
388	want to do the utopia.	
389	>> Okay.	
390	>> And ideally how we move from dystopia to	
391	utopia.	
392	>> Mhm. So the the the the F in face R	
393	>> is the loss of freedom as a result of	
394	that power dichotomy. Right? So you have	
395	you have a massive amount of power as	
396	you can see today in uh one specific	
397	army being powered by the US uh funds	
398	and a lot of money righting against	
399	peasants really that have no weapons	
400	almost at all.	
401	>> Okay. Some of them uh are militarized	
402	but the majority of the mill two million	
403	people are not. Okay. And so there is	
404	massive massive power that basically	
405	says, you know what, I'm going to	
406	oppress as far as I go. Okay. And I'm	
407	going to do whatever I want because the	
408	cheerleaders are going to be quiet,	
409	right? Or they're going to cheer or even	
410	worse. Huh? And so basically in in that	
411	what happens is max maximum power	
412	threatened by a democracy of power leads	
413	to a loss of freedom. A loss of freedom	
414	for everyone.	
415	>> Because how does that impact my freedom?	
416	>> Your freedom. Yeah,	
417	>> very soon uh you will if you publish	
418	this episode you're going to start to	
419	get questions around should you be	
420	talking about this those topics in your	
421	podcast. Okay. Uh you know uh if I uh	
422	have been on this episode then probably	
423	next time I land in the US someone will	
424	question me say why do you say those	
425	things? Which side are you on? Right?	
426	and and and and you know you can easily	
427	see that everything I mean I told you	
428	that before doesn't matter what I try to	
429	contribute to the world my bank will	
430	cancel my bank account every 6 weeks	
431	simply because of my ethnicity and my	
432	origin right every now and then they'll	
433	just stop my my bank account and say we	
434	need a document	
435	my other colleagues of a different color	
436	or a different ethnicity don't get asked	
437	for another document right but but but	
438	that's because I come from an ethnicity	
439	that is positioned in the world for the	
440	last 30 40 years as the uh enemy. Okay?	
441	And and so when you really really think	
442	about it, in a world where everything is	
443	becoming digital, in a world where	
444	everything is monitored, in a world	
445	where everything is seen, okay, we don't	
446	have much freedom anymore. And I'm not	
447	actually debating that or or I don't see	
448	a way to fix that	
449	>> because the AI is going to have more	
450	information on us, be better at tracking	
451	who we are, and therefore that will	
452	result in certain freedoms being	
453	restricted. Is that what you're saying?	
454	>> This is one element of it. Okay. If you	
455	push that element further	
456	in in in in a very short time if you've	
457	seen agent for example recently manos or	
458	ChatGPT there will be a time where you	
459	know you'll simply not do things	
460	yourself anymore. Okay. You'll simply go	
461	to your AI and say hey by the way I'm	
462	going to meet Stephen. Can you please	
463	you know book that for me?	
464	>> Great.	
465	>> And and and yeah and it will do	
466	absolutely everything. That's great	
467	until the moment where it decides to do	
468	things that are not motivated only by	
469	your well-being. Right. Why would he do	
470	that?	
471	>> Simply because, you know, maybe if I buy	
472	a BA ticket instead of an Emirates	
473	ticket, some agent is going to make more	
474	money than other agents and so on,	
475	right? Uh and I wouldn't be able to even	
476	catch it up if I hand over completely to	
477	an AI. Uh go go a step further. Huh?	
478	Think about a world where everyone	
479	almost everyone is on UBI. Okay.	
480	>> What's UBI?	
481	>> Universal basic income. I mean, think	
482	about the economics, the E and face	
483	rips. Think about the economics of a	
484	world where we're going to start to see	
485	a trillionaire	
486	before 2030. I can guarantee you that	
487	someone will be a trillionaire. I'm I'm	
488	you know I think there are many	
489	trillionaires in the world today or	
490	there we just don't know who they are.	
491	But there will be a new Elon Musk or	
492	Larry Ellison that will become a	
493	trillionaire because of AI investments,	
494	right? And and that trillionaire will	
495	have so much money to buy everything.	
496	There will be robots and AIs doing	
497	everything and humans will have no jobs.	
498	Mean	
499	>> do you think that's a there's a real	
500	possibility of job displacement over the	
501	next 10 years? And the the rebuttal to	
502	that would be that there's going to be	
503	new jobs created in technology.	
504	>> Absolute crap.	
505	>> Really?	
506	>> Of course.	
507	>> How how can you be so sure?	
508	>> Okay. So again, I am not sure about	
509	anything. So So let's just be very very	
510	clear. It would be very arrogant. Okay.	
511	To assume that I know	
512	>> you just said it was crap.	
513	>> My my belief is it is 100% crap.	
514	>> Take a job like software developer.	
515	>> Yeah.	
516	>> Okay. Uh Emma.Love my my new	
517	startup is me, Senad, another technical	
518	engineer and a lot of AIS. Okay. That	
519	startup would have been 350 developers	
520	in the past.	
521	>> I get that. Um but are you now hiring in	
522	other roles because of that or or you	
523	know as is the case with the steam	
524	engine? I can't remember the effect but	
525	there's you probably know that when	
526	steam when coal became cheaper people	
527	were worried that the coal industry	
528	would go out of business but actually	
529	what happened is people used more trains	
530	so trains now were used for transport	
531	and other things and leisure whereas	
532	before they were just used for commu for	
533	um cargo. Yeah. So there became more use	
534	cases and the coal industry exploded. So	
535	I'm wondering with technology, yeah,	
536	software developers are going to maybe	
537	not have as many jobs, but there	
538	everything's going to be software.	
539	>> Name me one.	
540	>> Name you one. What	
541	>> job?	
542	>> Name you that's going to be created.	
543	>> Yeah. One job that cannot be done by an	
544	AI.	
545	>> Yeah.	
546	>> Or a robot.	
547	>> My girlfriend's breath work retreat	
548	business where she takes groups of women	
549	around the world. Her company is called	
550	Barley Breathwork. And there's going to	
551	be a greater demand for connection,	
552	human connection.	
553	>> Correct. Keep going.	
554	>> So there's going to be more people doing	
555	community events in real life festivals.	
556	I think we're going to see a huge surge	
557	in things like	
558	>> everything that has to do with human	
559	connection.	
560	>> Yeah,	
561	>> correct. I'm totally in with that. Okay.	
562	What's the percentage of that versus	
563	accountant?	
564	>> It's a much smaller percentage for sure	
565	in terms of white collar jobs.	
566	>> Now, who does she sell to?	
567	>> People with probably what? probably	
568	accountants or you know	
569	>> correct she she sells to people who earn	
570	money from their jobs.	
571	>> Yeah.	
572	>> Okay. So you have two forces happening.	
573	One force is there are clear jobs that	
574	will be replaced. Video editor is going	
575	to be replaced. Uh	
576	>> excuse me.	
577	>> I love	
578	as as a matter of fact podcaster is	
579	going to be replaced.	
580	>> Thank you for coming on today Mo. It was	
581	seeing you again.	
582	But but but the truth is a lot so so you	
583	see the best at any job will remain the	
584	best software developer the one that	
585	really knows architecture knows	
586	technology and so on will stay for a	
587	while right and you know one of the	
588	funniest things I interviewed Max	
589	Tedmar and Max was laughing out loud	
590	saying CEOs are celebrating that they	
591	can now get rid of people and have	
592	productivity gains and cost reductions	
593	because AI can do that job. The one	
594	thing they don't think of is AI will	
595	replace them too. AGI is going to be	
596	better than at everything than humans at	
597	everything including being a CEO. Right?	
598	And you really have to imagine that	
599	there will be a time where most	
600	incompetent CEOs will be replaced. Most	
601	incompetent even breath work. Okay.	
602	Eventually there might actually one of	
603	two things be two things be happening.	
604	on one is either uh you know part part	
605	of that job other than the top breath	
606	work instructors, okay, are going you	
607	know who are going to gather all of the	
608	people that can still afford to pay for	
609	a breath work you know class	
610	they're going to be concentrated at the	
611	top and a lot of the bottom is not going	
612	to be working for one of two reasons.	
613	One is either there is not enough demand	
614	because so many people lost their jobs.	
615	So when you're on UBI, you cannot tell	
616	the government, hey by the way, pay me a	
617	bit more for a breath work class.	
618	>> UBI being universal basic income just	
619	gives you money every month.	
620	>> Correct. And if you really think of	
621	freedom and economics, UBI is a very	
622	interesting place to be because	
623	unfortunately I as I said there's	
624	absolutely nothing wrong with AI.	
625	There's a lot wrong with the value set	
626	of humanity at the age of the rise of	
627	the machines, right? And the biggest	
628	value set of humanity is capitalism	
629	today. And capitalism is all about what?	
630	Labor arbitrage.	
631	>> What's that mean?	
632	>> I I hire you to do something. I pay	
633	you a dollar. I pay it I sell it for	
634	two.	
635	Okay. And and most people confuse that	
636	because they say, "Oh, but the cost of a	
637	product also includes raw materials and	
638	factories and so on and so forth." All	
639	of that is built is built by labor,	
640	right? So, so basically labor goes and	
641	mines for the material and then the	
642	material is sold for a little bit of	
643	margin then that material is turned into	
644	a machine. It's sold for a little bit of	
645	margin then that machine and so on.	
646	Okay, there's always labor arbitrage in	
647	a world where humanity's minds are being	
648	replaced by uh by AIs, virtual AIs,	
649	okay, and humanity's power strength	
650	within 3 to 5 years time can be replaced	
651	by a robot,	
652	you really have to question how this	
653	world looks like. It could be the best	
654	world ever. And that's what I believe	
655	the utopia will look like because we	
656	were never made to wake up every morning	
657	and just, you know, occupy 20 hours of	
658	our day with work, right? We're not made	
659	for that. But we've fit into that uh uh,	
660	you know, system so well so far that we	
661	started to believe it's our life's	
662	purpose.	
663	>> But we choose it. We willingly choose	
664	it. And if you give someone unlimited	
665	money, they still tend to go back to	
666	work or find something to occupy their	
667	time with.	
668	>> They find something to occupy their time	
669	with,	
670	>> which is usually for so many people is	
671	building something. Philanthropy,	
672	100%. So you build something. So	
673	between Senad and I, Emma.Love is not	
674	about making money. It's about finding	
675	true love relationships.	
676	>> What is that? Sorry, just for context.	
677	>> So So you know,	
678	>> it's a business you're building just for	
679	the audience context. So, so, so the	
680	idea here is I can, it might become a	
681	unicorn and be worth a billion dollars,	
682	but neither I nor Senate are interested,	
683	okay? We're doing it because we can,	
684	okay? And we're doing it because it can	
685	make a massive difference to the world.	
686	>> And you have money, though.	
687	>> It doesn't take that much money anymore	
688	to build anything in the world. This is	
689	labor arbitrage.	
690	>> But to build something exceptional, it's	
691	still going to take a little bit more	
692	money than building something bad	
693	>> for the next few years. So whoever has	
694	the capital to build something	
695	exceptional will end up winning.	
696	>> So so this is a very interesting	
697	understanding of freedom. Okay. This is	
698	the reason why we have the AI arms race.	
699	Okay. Is that the one that owns the	
700	platform is going to be making all the	
701	money and and keeping all the power.	
702	Think think of it this way. When	
703	humanity started the best hunter in the	
704	tribe could maybe feed the tribe for	
705	three to four more years more days. H	
706	and as a as a reward, he gained the	
707	favor of multiple mates in the tribe.	
708	That's it. The top farmer in the tribe	
709	could feed the tribe for a season more.	
710	Okay? And as a result, they got estates	
711	and you know uh and mansions and so on.	
712	The best industrialist in the in a in a	
713	city could actually employ the whole	
714	city, could grow the GDP of their entire	
715	country. And as a result, they became	
716	millionaires. the 1920s.	
717	H the best technologists	
718	now are billionaires. Now what's the	
719	difference between them? The tool the	
720	the hunter only rem depended on their	
721	skills and the automation the entire	
722	automation he had was a spear. The	
723	farmer had way more automation. And the	
724	biggest automation was what? The soil.	
725	The soil did most of the work. The	
726	factory did most of the work. the the	
727	network did most of the work. And so	
728	that inc incredible expansion of wealth	
729	and power and as well the the incredible	
730	impact that something brings is entirely	
731	around the tool that automates. So who's	
732	going to own the tool? Who's going to	
733	own the the the digital soil, the AI	
734	soil? It's the platform owners.	
735	>> And the platforms you're describing are	
736	things like OpenAI, Gemini, Grock. These	
737	these are interfaces to the platforms.	
738	The platforms are all of the uh of the	
739	uh um tokens, all of the compute that is	
740	in the background, all of the uh all of	
741	the uh uh methodology, the systems, the	
742	algorithms, that's the platform, the AI	
743	itself. You know, Grock is the interface	
744	to it.	
745	I think this is probably worth	
746	explaining in layman's terms to people	
747	that haven't built AI tools yet because	
748	I think I think to the listener	
749	they probably think that every AI	
750	company they're hearing of right now is	
751	building their own AI whereas actually	
752	what's happening is there is really	
753	five, six, seven AI companies in the	
754	world and when I built my AI application	
755	I basically	
756	>> pay them for every time I use their AI.	
757	So if Steven Bartlett builds an AI at	
758	stephvenai.com,	
759	it's not that I've built my own	
760	underlying I've trained my own model.	
761	Really what I'm doing is I'm paying	
762	Sam Altman's ChatGPT. Um every single	
763	time I do a a call, I basically um I do	
764	a search or you know I use a token. And	
765	I think that's really important because	
766	most people don't understand that unless	
767	you've built AI, you think, "Oh, look,	
768	you know, there's all these AI companies	
769	popping up. I've got this one for my	
770	email. I've got this one for my dating.	
771	I've got No, no, no, no, no. They're	
772	pretty much I would be I would hazard a	
773	guess that they're probably all OpenAI	
774	at this point.	
775	>> No, there are quite a few quite	
776	different characters and quite	
777	differently,	
778	>> but there's like five or six.	
779	>> There are five or six when it comes to	
780	language models. Yeah. Right. Uh but	
781	interestingly, so yes, I should say	
782	yes to start and then I should say but	
783	there was an interesting twist with	
784	DeepSeek at the beginning of the year.	
785	So what DeepSeek did is is they	
786	basically uh nullified the business	
787	model if you want in two ways. one is it	
788	was around a week or two after uh you	
789	know Trump stood you know with pride	
790	saying Stargate is the biggest	
791	investment project in the history and	
792	it's $500 billion to build AI	
793	infrastructure and SoftBank and Larry	
794	Ellison and and uh Sam Altman were	
795	sitting and so you know beautiful	
796	picture and then DeepSeek R3 comes out	
797	it does the job for a one over 30 of the	
798	cost okay and interestingly is entire	
799	open source and available as an edge AI.	
800	So, so that's really really interesting	
801	because there could be now in the future	
802	as the technology improves the learning	
803	models will be massive but then you can	
804	compress them into something you can	
805	have on your phone and you can download	
806	DeepSeek literally offline on a	
807	um um you know an off the network	
808	computer and build an AI on it. There's	
809	a website that basically tracks the	
810	um sort of cleanest apples to Apple's	
811	market share of all the website	
812	referrals sent by AI chatbots and	
813	ChatGPT is currently at 79% roughly about	
814	80%. Perplexity is at 11, Microsoft	
815	Copilot about five, Google Gemini is at	
816	about two, Claude's about one and	
817	DeepSeek is about 1%. And really like	
818	the the point that I want to land is	
819	just that when you hear of a new AI app	
820	or tool or this one can make videos,	
821	>> it's built on one of them. It's	
822	basically built on one of these	
823	really three or four AI platforms that's	
824	controlled really by three or four AI	
825	you know billionaire teams and actually	
826	the one of them that gets to what we	
827	call AGI first where the AI gets really	
828	really advanced	
829	one could say is potentially going to	
830	rule the world as it relates to	
831	technology.	
832	>> Yes. Uh if if they get enough uh head	
833	start. So, so I actually think that uh	
834	what I what I'm more concerned about now	
835	is not AGI, believe it or not. So, AGI	
836	in my mind and I said that back in 2023,	
837	right? Uh that we will get to AGI. At	
838	the time I said 2027, now I believe 2026	
839	latest. Okay. The most interesting	
840	development that nobody's talking about	
841	is self-evolving AIS.	
842	self evolving AIS is	
843	think of it this way if you and I are	
844	hiring the top engineer in the world to	
845	develop our AI models	
846	and with AGI that top engineer in the	
847	world becomes an AI who would you hire	
848	to develop your next generation AI that	
849	AI	
850	>> the one that can teach itself	
851	>> correct so one of my favorite examples	
852	is called Alpha Evolve so this is	
853	Google's attempt to basically have four	
854	agents working together four AIs working	
855	together to look at the at the code of	
856	the AI and say where is the where are	
857	the performance issues then you know an	
858	agent would say what's the problem	
859	statement what can I uh you know what do	
860	I need to fix uh one that actually	
861	develops the solution one that assesses	
862	the solution and then they continue to	
863	do this and you know I don't remember	
864	the exact figure but I think Google	
865	improved like 8% uh on their AI	
866	infrastructure because of alpha evol	
867	Right? And when you really really think,	
868	don't quote me on the number 8 to 10, 6	
869	to 10, whatever in Google terms, by the	
870	way, that is massive. That's billions	
871	and billions of dollars. Now, the the	
872	the the trick here is this. The trick is	
873	again, you have to think in game theory	
874	format.	
875	Is there any scenario we can think of	
876	where if one player uses AI to develop	
877	the next generation AI that the other	
878	players will say no no no no no that's	
879	too much you know takes us out of	
880	control every other player will copy	
881	that model and have their next AI model	
882	developed by an AI.	
883	>> Is this what Sam Altman talks about who's	
884	the founder of um ChatGPT/OpenAI	
885	when he talks about a fast takeoff? I	
886	don't know exactly what which what what	
887	which you're referring to but we're all	
888	talking about a point now that we call	
889	the intelligence explosion. So, so there	
890	is a moment in time where you have to	
891	imagine that if AI now is better than	
892	97% of all code developers in the world	
893	and soon we'll be able to look at its	
894	own code own algorithms by the way	
895	they're becoming incredible	
896	mathematicians which wasn't the case	
897	when we last met if they can develop	
898	improve their own code improve their own	
899	algorithms improve their own uh uh you	
900	know uh network architecture or whatever	
901	you can imagine that very quickly the	
902	force applied to developing the next AI	
903	is not going to be a human brain	
904	anymore. It's going to be a much smarter	
905	brain and very quickly as humans like	
906	basically when when we ran the Google	
907	infrastructure when the machine said we	
908	need another server or a proxy server in	
909	that place we followed. we we never	
910	really you know wanted to to object or	
911	verify because you know the code would	
912	probably know better because there are	
913	billions of transactions an hour or a	
914	day and so very quickly those	
915	self-evolving AIs will simply say I need	
916	14 more servers here and we'll just you	
917	know the team will just go ahead and do	
918	it. I watched a video a couple of days	
919	ago where he Sam Altman effectively had	
920	changed his mind because in 2023 which	
921	is when we last met he said the aim was	
922	for um a slow takeoff which is sort of	
923	gradual deployment and open AI's 203	
924	2023 note says a slower takeoff is	
925	easier to make safe and they prefer	
926	iterative rollouts society can adapt in	
927	2025	
928	>> they changed their mind and Sam Altman	
929	said	
930	He now thinks a fast takeoff is more	
931	possible than he did a couple of years	
932	ago on the order of a small number of	
933	years rather than a decade. Um, and it	
934	to define what we mean by a fast	
935	takeoff, it's defined as when AI goes	
936	from roughly human level to far beyond	
937	human very quickly, think months to a	
938	few years, faster than governments,	
939	companies, or society can adapt with	
940	little warning, big power shifts, and	
941	hard to control. A slow takeoff, by	
942	contrast, is where capabilities climb	
943	gradually over many years with lots of	
944	warning shots. Um, and the red flags for	
945	a fast takeoff is when AI can	
946	self-improve, run autonomous research	
947	and development and scale with massive	
948	compute compounding gains which will	
949	snowball fast. So, and I think from the	
950	video that I watched of Sam Orman	
951	recently, who again is the founder of	
952	Open Air and HBT, he basically says, and	
953	again I'm paraphrasing here. I will put	
954	it on the screen. We have this community	
955	knows things so I'll write it on the	
956	screen. But he effectively said that	
957	whoever gets to AGI first will have the	
958	technology	
959	>> to develop super intelligence	
960	>> where the AI can can rapidly increase	
961	its own intelligence and it will	
962	basically leave everyone else behind.	
963	>> Yes. Uh so that last bit is debatable	
964	but but let's just agree that uh so so	
965	in in a live uh you know one of the	
966	posts I shared and got a lot of	
967	interest is I refer to the the Altman as	
968	a brand not as a human. Okay. So the	
969	Altman is that uh persona of a	
970	California disruptive technologist that	
971	disrespects everyone. Okay. and believes	
972	that disruption is good for humanity and	
973	believes that this is good for safety	
974	and like everything else like we say war	
975	is for democracy and freedom they say uh	
976	developing you know putting AI on the	
977	open internet is good for everyone right	
978	it allows us to learn from our mistakes	
979	that was Sam Altman's 2023 spiel and if	
980	you recall at the time I was like this	
981	is the most dangerous you know one of	
982	the clips that really went viral you so	
983	you're you're so clever at finding the	
984	right clips is when I said	
985	>> I didn't I didn't do the clipping mate	
986	>> they're team teams remember the clip	
987	where I said we fucked up we always said	
988	don't put them on the open internet	
989	until we know what we're putting out in	
990	the world I'm going to be saying that	
991	>> yeah we we we fucked up on putting it on	
992	the open internet teaching it to to code	
993	and putting you know agents AI agents	
994	prompting other AIs now AI agents	
995	prompting other AIs are leading to	
996	self-developing AIS and and The problem	
997	is, of course, we, you know, anyone who	
998	has been on the inside of this knew that	
999	this was just a clever spiel made by a	
1000	PR manager for Sam Altman to sit with his	
1001	dreamy eyes in front of Congress and	
1002	say, "We want you to regulate us." Now,	
1003	they're saying, "We're unregulable."	
1004	Okay? And and when you really understand	
1005	what's happening here, what's happening	
1006	is it's so fast	
1007	that none of them has the choice to slow	
1008	down. It's impossible. Neither China	
1009	versus America or OpenAI versus Google.	
1010	the that the the only thing that I may	
1011	have may see happening that you you know	
1012	that that may differ a little bit from	
1013	your statement is if one of them gets	
1014	there first uh then they dominate for	
1015	the rest of humanity that is probably	
1016	true if they get there first uh with	
1017	within enough buffer. Okay. But the way	
1018	you look at Grock coming a week after	
1019	open AI, a week after uh you know	
1020	Gemini, a week after Claude and then	
1021	Claude comes again and then China	
1022	releases something and then Korea	
1023	releases some something. It is so fast	
1024	that we may get a few of them at the	
1025	same time or a few months apart. Okay,	
1026	before one of them has enough power to	
1027	become dominant. And that is a very	
1028	interesting scenario.	
1029	multiple AIs, all super intelligent.	
1030	>> It's funny, you know, I I got asked	
1031	yesterday, I was in I was in Belgium on	
1032	stage. There was, I don't know, maybe	
1033	4,000 people in the audience and a kid	
1034	stood up and he was like, um, you've had	
1035	a lot of conversations in the last year	
1036	about AI. Like, why do you care? And I	
1037	don't think people realize how,	
1038	even though I've had so many	
1039	conversations on this podcast about AI,	
1040	you	
1041	>> haven't made up your mind.	
1042	>> I I have more questions than ever.	
1043	>> I know. And it's and it doesn't seem	
1044	that anyone can satiate.	
1045	>> Anyone that tells you they can predict	
1046	the future is arrogant.	
1047	>> Yeah.	
1048	>> It is. It's never moved so fast.	
1049	>> It's nothing like nothing I've ever	
1050	seen. And you know, by the time that we	
1051	leave this conversation and I go to my	
1052	computer, there's going to be some	
1053	incredible new technology or application	
1054	of AI that didn't exist when I woke up	
1055	this morning. That creates probably	
1056	another paradigm shift in my brain.	
1057	Also, you know, I people have different	
1058	opinions of Elon Musk and they're	
1059	they're entitled to their own opinion,	
1060	but the other day, only a couple of days	
1061	ago, he did a tweet where he said, "At	
1062	times, AI existential dread is	
1063	overwhelming." And on the same day, he	
1064	tweeted, "I resisted AI for too long,	
1065	living in denial. Now it is game on."	
1066	And he tagged his AI companies. I don't	
1067	know what to make of I don't know what	
1068	to make of those tweets. I don't know.	
1069	And you know, I	
1070	I try really hard to figure out if	
1071	someone like Sam Wman has the best	
1072	interests of society at heart.	
1073	>> No.	
1074	>> Or if these people are just like	
1075	>> I'm saying that publicly. No.	
1076	As a matter of fact, so I know Sundar	
1077	Pichai. I work CEO of Alphabet, Google's	
1078	parent company. an amazing human being	
1079	on in all honesty. I know Demis Hassabis	
1080	is amazing human being. Okay. Uh you	
1081	know these are are ethical incredible uh	
1082	humans at heart. They have no choice.	
1083	Uh Sund by law	
1084	is uh demanded to take care of his his	
1085	shareholder value. That's that is his	
1086	job.	
1087	>> But Sund you said you know him. You used	
1088	to work at Google.	
1089	>> Yeah. He's not going to do anything that	
1090	he thinks is going to harm humanity.	
1091	>> But if if he does not continue to	
1092	advance AI, that by definition uh uh uh	
1093	contradicts his responsibility as the	
1094	CEO of a publicly traded company, he is	
1095	liable by law to continue to advance the	
1096	agenda. There's absolutely no doubt	
1097	about it. Now, so but but he's a good	
1098	person at heart. Deis is a good person	
1099	at heart. So they're trying so hard to	
1100	make it safe. Okay? As much as they can.	
1101	Reality however is the the the disruptor	
1102	the Altman as a brand doesn't care that	
1103	much.	
1104	>> How do you know that?	
1105	>> In reality the disruptor is someone that	
1106	comes in with the objective of I don't	
1107	like the status quo. I have a different	
1108	approach. And that different approach if	
1109	you just look at the story was we are a	
1110	nonforprofit that is funded mostly by	
1111	Elon Musk money. It's not entirely by	
1112	Elon Musk money. So context for people	
1113	that might not understand Open AI. The	
1114	reason I always give context is funnily	
1115	enough I think I told you this last	
1116	time. I went to a prison where they play	
1117	the D of CEO.	
1118	>> No way.	
1119	>> So they play the D of CO and I think	
1120	it's 50 prisons in the UK to young	
1121	offenders	
1122	>> and no violence there.	
1123	>> Well, I don't know. I can't I can't I	
1124	can't tell you whether violence has gone	
1125	up or down. But I was in the cell with	
1126	one of the prisoners, a young a young	
1127	black guy, and I was in his cell for for	
1128	a little while. I was reading through	
1129	his business plan, etc. And I said, "You	
1130	know what? You need to listen to this	
1131	conversation that I did with Mo Gawdat."	
1132	So I he has a little screen in his cell.	
1133	So I pulled it up, you know, our first	
1134	conversation. I said, "You should listen	
1135	to that one." And he said to me, he	
1136	said, "I can't listen to that one cuz	
1137	you guys use big words."	
1138	>> So ever since that day, which was about	
1139	>> I noticed that about days four years	
1140	ago, sorry.	
1141	>> I've always whenever I hear a big word,	
1142	I think about this kid.	
1143	>> Yeah.	
1144	>> And I say like give context. So even	
1145	with the you're about to explain what	
1146	Open AI is, I know he won't know what	
1147	Open AI's origin story was. That's why	
1148	I'm	
1149	>> I think that's a wonderful practice in	
1150	general. By the way, even, you know,	
1151	being a non native English speaker,	
1152	>> you'll be amazed how often a word is	
1153	said to me and I I'm like, yeah, don't	
1154	know what that means.	
1155	>> So, like I've actually never said this	
1156	publicly before, but I now see it as my	
1157	responsibility to be to to keep the draw	
1158	the drawbridge	
1159	to accessibility of these conversations	
1160	down for him. So, whenever I whenever	
1161	there's a word that at some point in my	
1162	life I didn't know what it meant,	
1163	>> I will go back. I was like, what does	
1164	that mean? I think that I've noticed	
1165	that in the you know more and more in	
1166	your podcast and I really appreciate and	
1167	we also show it on the screen sometimes.	
1168	>> I think that's wonderful. I mean the	
1169	the the origin story of open AI is as	
1170	the name suggests it's open source. It's	
1171	for the public good. It was an in you	
1172	know intended in Elon Musk's words to	
1173	save the world from the dangers of AI	
1174	right so they were doing research on	
1175	that and then you know there was the	
1176	disagreement between Sam Altman and and	
1177	Elon somehow Elon ends up being out of	
1178	uh of uh of open AI. I think there was a	
1179	moment in time where he tried to take it	
1180	back and you know the board rejected it	
1181	or some something like that. most of the	
1182	uh top um safety engineers, the top	
1183	technical teams in open AI left in 2023	
1184	2024 openly saying we're not concerned	
1185	with safety anymore. It moves from being	
1186	a nonforprofit to being one of the most	
1187	valued companies in the world. There are	
1188	billions of dollars at stake, right? And	
1189	if you if you tell me that Sam Altman is	
1190	out there trying to help humanity, let's	
1191	let's suggest to him and say, "Hey, do	
1192	you want to do that for free? We'll pay	
1193	you a very good salary, but you don't	
1194	have stocks in this. Saving humanity	
1195	doesn't come at the billion dollar	
1196	valuation or of course now tens of	
1197	billions or hundreds of billions." And	
1198	and and see truly that is when you know	
1199	that someone is doing it for the good of	
1200	humanity. Now the the capitalist system	
1201	we've built is not built for the good of	
1202	humanity. It's built for the good of the	
1203	capitalist.	
1204	>> Well, he might say that releasing the	
1205	model publicly, open sourcing it is too	
1206	risky	
1207	because then bad actors around the world	
1208	would have access to that technology. So	
1209	he might say that closing open AI in	
1210	terms of not making it publicly viewable	
1211	is the right thing to do for safety. We	
1212	go back to gullible cheer leaders,	
1213	right? One of the interesting tricks is	
1214	of lying in our world is everyone will	
1215	say what helps their agenda. Follow the	
1216	money. Okay, you follow the money and	
1217	you find that you know at a point in	
1218	time Samman himself was saying it's open	
1219	AI. Okay, my benefit at the time is to	
1220	give it to the world so that the world	
1221	looks at it. They know the code if there	
1222	is if there are any bugs and so on. True	
1223	statement. Also a true statement is if I	
1224	put it out there in the world, a	
1225	criminal might take that model and build	
1226	something that's against humanity as a	
1227	result. Also true statement. Capitalists	
1228	will choose which one of the truths to	
1229	say, right? Based on which part of the	
1230	agenda, which part of their life today	
1231	they want to serve, right? Someone will	
1232	say, uh, you know, do I do you want me	
1233	to be controversial?	
1234	Let's not go there. But if we go back to	
1235	war, I'll give you 400 slogans.	
1236	400 slogans that we all hear that change	
1237	based on the day and the army and the	
1238	location and the they're all slogans.	
1239	None of them is true. You want to know	
1240	the truth. You follow the money, not	
1241	what the person is saying, but ask	
1242	yourself why is the person saying that?	
1243	What's in it for the person speaking?	
1244	>> And what do you think's in it for Chachi	
1245	Samman? hundreds of billions of dollars	
1246	of of of valuation.	
1247	>> And do you think it's that power?	
1248	>> The ego of being the person that	
1249	invented AGI, the position of power that	
1250	this gives you, the meetings with all of	
1251	the heads of states, the admiration that	
1252	gets it, it is intoxicating	
1253	>> 100%	
1254	100%.	
1255	Okay. And and the real question, this is	
1256	a question I ask everyone. Did you see	
1257	you didn't you're every time I ask you	
1258	you say you didn't. Did you see the	
1259	movie Elysium?	
1260	>> No. You'd be surprised how little movie	
1261	watching I do. You'd be shocked.	
1262	>> There are some movies that are very	
1263	interesting. I use them to to create an	
1264	emotional attachment to a story that you	
1265	haven't seen yet because you may have	
1266	seen it in a movie. Okay. Elissium is a	
1267	is a society where the elites are living	
1268	on the moon. Okay. They don't need	
1269	peasants to do the work anymore and	
1270	everyone else is living down here. Okay.	
1271	You have to imagine that if again game	
1272	theory you have to im you know picture	
1273	something to infinity to its extreme and	
1274	see where it goes and the extreme of a	
1275	world where all manufactured is done	
1276	manufacturing is done by machines	
1277	where all decisions are made by machines	
1278	and those machines are owned by a few	
1279	is not an economy similar to the to	
1280	today to the to today's economy	
1281	that today's economy is an economy of	
1282	consumerism and and product and	
1283	production. You know, it's the it's the	
1284	in in alive I call it the invention of	
1285	more. The invention of more is that post	
1286	World War II as the factories were	
1287	rolling out things and prosperity was	
1288	happening everywhere in America. There	
1289	was a time where every family had enough	
1290	of everything.	
1291	>> But for the capitalist to continue to be	
1292	profitable, they needed to convince you	
1293	that what you had was not enough. either	
1294	by making it obsolete like fashion or	
1295	like you know a new shape of a car or	
1296	whatever or by convincing you that there	
1297	are more things in life that you need so	
1298	that you become complete without those	
1299	things you don't and and that invention	
1300	of more gets us to where we are today an	
1301	economy that's based on production	
1302	consumed and if you look at the US	
1303	economy today 62% of the US economy GDP	
1304	is consumption it's not production okay	
1305	Now, this requires that the consumers	
1306	have enough purchasing power to to buy	
1307	what is produced. And I believe that	
1308	this will be an economy that will take	
1309	us hopefully in the next 10, 15, 20	
1310	years and forever. But that's not	
1311	guaranteed. Why? Because on one side if	
1312	UBI replaces purchasing power. So if	
1313	people have to get an income from the	
1314	government which is basically taxes	
1315	collected from those using AI and robots	
1316	to to make things	
1317	the then the the mindset of capitalism	
1318	labor arbitrage means those people are	
1319	not producing anything and they're	
1320	costing me money. Why don't we pay them	
1321	less and less and maybe even not pay	
1322	them at all? And that becomes illissium	
1323	where you basically say, you know, we	
1324	sit somewhere protected from everyone.	
1325	We have the machines do all of our work	
1326	and those need to worry about	
1327	themselves. We're not going to pay them	
1328	UBI anymore, right? And and you have to	
1329	imagine this idea of UBI assumes this	
1330	very democratic caring society.	
1331	UBI in itself is communism.	
1332	Think of the ideology between at least	
1333	socialism. The ideology of giving	
1334	everyone what they need. That's not the	
1335	capitalist	
1336	democratic society that the west	
1337	advocates. So those transitions are	
1338	massive in magnitude.	
1339	And for those transitions to happen, I	
1340	believe the right thing to do when the	
1341	cost of producing everything is almost	
1342	zero because of AI and robots.	
1343	because the cost of harvesting energy	
1344	should actually tend to zero once we get	
1345	more intelligent to harvest the energy	
1346	out of thin air. Then a possible	
1347	scenario and and I believe a scenario	
1348	that AI will eventually do in the utopia	
1349	is yeah anyone can get anything they	
1350	want. Don't over consume. We're not	
1351	going to abuse the the planet resources	
1352	but it costs nothing. So like the old	
1353	days where we were hunter gatherers, you	
1354	would, you know, forge for some berries	
1355	and you'll find them ready in in nature.	
1356	Okay, we can in 10 years time, 12 years	
1357	time build a society where you can forge	
1358	for an iPhone in nature. It will be made	
1359	out of thin air. Nanopysics will allow	
1360	you to do that. Okay? But the challenge,	
1361	believe it or not, is not tech. The	
1362	challenge is a mindset. Because the	
1363	elite, why would they give you that for	
1364	free? Okay. And the system would morph	
1365	into, no, no, hold on. We will make more	
1366	money. We will be bigger capitalists. We	
1367	will feed our ego and hunger for power	
1368	more and more. And for them, give them	
1369	UBI and then 3 weeks later give them	
1370	less UBI.	
1371	>> Aren't there going to be lots of new	
1372	jobs created though? Because when we	
1373	think about the other revolutions over	
1374	time, whether it was the industrial	
1375	revolution or other sort of big	
1376	technological revolutions,	
1377	>> in the moment we forecasted that	
1378	everyone was going to lose their jobs,	
1379	>> but we couldn't see all the new jobs	
1380	that were being created	
1381	>> because the the the machines	
1382	replaced the human strength at a point	
1383	in time. And very few places in the west	
1384	today will have a worker carry things on	
1385	their back and carry it upstairs. The	
1386	machine does that work. Correct.	
1387	>> Yeah.	
1388	>> Uh similarly	
1389	AI is going to replace the brain of a	
1390	human. And when the west in its	
1391	interesting uh virtual colonies that I	
1392	call it uh basically outsourced all	
1393	labor to the to the developing nations.	
1394	What the West publicly said at the time	
1395	is we're we're going to be a services	
1396	economy. We we're we're not interested	
1397	in making things and stitching things	
1398	and so on. Let the Indians and Chinese	
1399	and you know Bengali and Vietnamese do	
1400	that. We're going to do more refined	
1401	jobs. Knowledge workers. We're going to	
1402	call them. Knowledge workers are people	
1403	who work with information and click on a	
1404	keyboard and move a mouse and you know	
1405	sit in meetings and all we produce in	
1406	the western societies is what words	
1407	right or designs maybe sometimes but	
1408	everything we produce can be produced by	
1409	an AI.	
1410	So if I give you an AI tomorrow h where	
1411	I give you a piece of land, I give the	
1412	AI a piece of land and I say here are	
1413	the parameters of my land. Here is its	
1414	location on Google maps. Design an	
1415	architecturally sound villa for me. I	
1416	care about a lot of light and I need	
1417	three bedrooms. I want my bathrooms to	
1418	be in white marble, whatever. And the AI	
1419	produces it like that. How often will	
1420	you go to an to an architect and say	
1421	right so what will the architect do the	
1422	best of the best of the architects will	
1423	either use AI to produce that or you	
1424	will consult with them and say hey you	
1425	know I've seen this and they'll say it's	
1426	really pretty but it wouldn't feel right	
1427	for the person that you are yeah those	
1428	jobs will remain but how many of them	
1429	will remain	
1430	how how often do you think uh how many	
1431	more years. Do you think I will be able	
1432	to create a book that is smarter than	
1433	AI?	
1434	Not many. I will still be able to	
1435	connect to a human. You're not going to	
1436	hug an AI when you meet them like you	
1437	hug me, right? But that's not enough of	
1438	a job.	
1439	So why do I say that? Remember I asked	
1440	you at the beginning of the podcast to	
1441	remind me of solutions. Why do I say	
1442	that? Because there are ideological	
1443	shifts and and concrete actions that	
1444	need to be taken by governments today	
1445	rather than waiting until COVID is	
1446	already everywhere and then locking	
1447	everyone down. Governments could have	
1448	reacted before the first patient or at	
1449	least at patient zero or at least at	
1450	patient 50. They didn't. H what I'm	
1451	trying to say is there is no doubt that	
1452	lots of jobs will be lost. There's no	
1453	doubt that there will be sectors of	
1454	society where 10 20 30 40 50% of all	
1455	developers, all software uh you know all	
1456	graphic designers, all um uh uh u online	
1457	marketers, all all all assistances	
1458	are going to be out of a job. So are we	
1459	prepared as a society to do that? Can we	
1460	tell our governments there is an	
1461	ideological shift? This is very close to	
1462	social socialism and and communism.	
1463	Okay. And are we ready from a budget	
1464	point of view instead of spending a a	
1465	trillion dollars a year on on arms and	
1466	and explosives and you know autonomous	
1467	weapons that will oppress people because	
1468	we can't feed them? Can we please shift	
1469	that? I did those numbers. Huh. Uh again	
1470	I go back to military spending because	
1471	it's all around us. 2.71 trillion. 2.4	
1472	to2.7 is the estimate of 2024. how much	
1473	money we're spending on military	
1474	>> on Yeah. on military equipment on things	
1475	that we're going to explode into smoke	
1476	and death. Extreme poverty worldwide.	
1477	Extreme poverty is people that are below	
1478	the poverty line. Extreme poverty	
1479	everywhere in the world could end for 10	
1480	to 12% of that budget. So if we replace	
1481	our military spending 10% of that to go	
1482	to people who are in extreme poverty,	
1483	nobody will be poor in the world. Okay.	
1484	You can end uh world hunger for less	
1485	than 4%. Nobody would be hungry in the	
1486	world. You know, if you take uh again 10	
1487	to 12% universal healthcare, every human	
1488	being on the planet would have free	
1489	healthcare for 10 to 12% on what we're	
1490	spending on war. Now, why why do I say	
1491	this when we're talking about AI?	
1492	Because that's a simple decision. If we	
1493	stop fighting	
1494	because money itself does not have the	
1495	same meaning anymore because the	
1496	economics of money is going to change	
1497	because the entire meaning of capitalism	
1498	is ending because there is no more need	
1499	for labor arbitrage because AI is doing	
1500	everything	
1501	just with the $2.4 trillion we save in	
1502	explosives every year in arms and	
1503	weapons just for that universal	
1504	healthcare and extreme poverty. You	
1505	could actually one of the calculations	
1506	is you could end climate or combat	
1507	climate climate change meaningfully for	
1508	100% of the military budget.	
1509	>> But I I'm not even sure it's really	
1510	about the money. I think money is a	
1511	measurement stick of power. Right.	
1512	>> Exactly. It's printed on demand.	
1513	>> So even in a world where we have super	
1514	intelligence and money is no longer a	
1515	problem.	
1516	>> Correct.	
1517	>> I still think	
1518	power is going to be	
1519	insatiable for so many people. So there	
1520	will still be war because you know	
1521	>> there will be in my view	
1522	>> the strongest I want the strongest AI. I	
1523	don't want my	
1524	>> and I don't and I don't want you know	
1525	what Harry Henry Kissinger called them	
1526	the eaters.	
1527	>> The eaters.	
1528	>> Yeah.	
1529	Brutal as that sounds.	
1530	>> What is that? The people at the bottom	
1531	of the socioeconomic	
1532	>> that don't produce but consume.	
1533	So if you had a Henry Kissinger at the	
1534	at the helm and we have so many of them,	
1535	what would they think like why why	
1536	prominent military figure in the US	
1537	history? Uh you know what why would we	
1538	feed 350 million Americans America will	
1539	think but more interestingly why do we	
1540	even care about Bangladesh anymore if we	
1541	can't make our textiles there or we	
1542	don't want to make our textile there. Do	
1543	you you know I imagine throughout human	
1544	history if we had podcasts conversations	
1545	would would have been warning of a	
1546	dystopia around the corner. You know	
1547	when they heard of technology and the	
1548	internet they would have said oh we're	
1549	finished and when the the tractor came	
1550	along they would have said oh god we're	
1551	finished because we're not going to be	
1552	able to farm anymore. So is this not	
1553	just another one of those moments where	
1554	we couldn't see around the corner so we	
1555	we forecasted unfortunate things. You	
1556	could be. I am I'm begging that I'm	
1557	wrong. Okay. I'm just asking if there	
1558	are scenarios that you think that can	
1559	provide that. You know, uh uh Mustafa	
1560	Sulleman in in uh you hosted him here. I	
1561	did. Yeah. He was in the coming wave.	
1562	>> Yeah.	
1563	>> And he speaks about uh about pessimism	
1564	aversion.	
1565	Okay. that all of us people who are	
1566	supposed to be in technology and	
1567	business and so on, we're always	
1568	supposed to, you know, stand on stage	
1569	and say the future's going to be	
1570	amazing. You know, this technology I'm	
1571	building is going to make everything	
1572	better. One of my posts in life was	
1573	called the broken promises. How often	
1574	did that happen?	
1575	>> Okay. How often did social media connect	
1576	us? And how many and how often did it	
1577	make us more lonely? How how often did	
1578	mobile phones make us work less? That	
1579	was the promise. That was the promise.	
1580	The promise. The early ads of Nokia were	
1581	people at parties. Is that your	
1582	experience of mobile phones? And and I	
1583	think the whole idea is we should hope	
1584	there will be other roles for humanity.	
1585	By the way, those roles would resemble	
1586	the times where we were hunter	
1587	gatherers, just a lot more technology	
1588	and a lot more safety.	
1589	>> Okay. So, this is this sounds good.	
1590	>> Yeah,	
1591	>> this is exciting. So, I'm gonna I'm	
1592	gonna get to go outside more, be with my	
1593	friends more,	
1594	>> 100%.	
1595	>> Fantastic.	
1596	>> And do absolutely nothing.	
1597	>> Well, that doesn't sound fantastic.	
1598	>> No, it does. Do be forced to do	
1599	absolutely nothing. For some people,	
1600	it's amazing. For you and I, we're going	
1601	to find the little carpentry project and	
1602	just do something.	
1603	>> Speak for yourself. I'm still People are	
1604	still going to tune in.	
1605	>> Okay.	
1606	>> Correct. Yeah. But what? And people are	
1607	going to to tune in.	
1608	>> Do you think they will? I'm not I'm not	
1609	I'm not convinced they will. And for for	
1610	as long	
1611	>> will you guys tune in? Are you guys	
1612	still going to tune in?	
1613	>> I can let them answer. I believe for as	
1614	long as you make their life enriched,	
1615	>> but can an AI do that better	
1616	>> without the human connection?	
1617	>> Comment below. Are you going to listen	
1618	to an AI or the Davosio? Let me know in	
1619	the comment section below.	
1620	>> Remember, as incredibly intelligent as	
1621	you are, Steve, uh there will be a	
1622	moment in time where you're going to	
1623	sound really dumb compared to an AI. and	
1624	and and I will sound completely dumb.	
1625	>> Yeah. Yeah.	
1626	>> The the depth the depth of analysis	
1627	and and gold nuggets. I mean, can you	
1628	imagine two super intelligences deciding	
1629	to get together and explain um string	
1630	theory to us?	
1631	They'll do better than any physic	
1632	physicist in the world because they	
1633	possess the physics knowledge and they	
1634	also pro possess social and language	
1635	knowledge that most deep physicists	
1636	don't. I think B2B marketeteers keep	
1637	making this mistake. They're chasing	
1638	volume instead of quality. And when you	
1639	try to be seen by more people instead of	
1640	the right people, all you're doing is	
1641	making noise. But that noise rarely	
1642	shifts the needle and it's often quite	
1643	expensive. And I know as there was the	
1644	time in my career where I kept making	
1645	this mistake that many of you will be	
1646	making it too. Eventually I started	
1647	posting ads on our show sponsors	
1648	platform LinkedIn. And that's when	
1649	things started to change. I put that	
1650	change down to a few critical things.	
1651	One of them being that LinkedIn was then	
1652	and still is today the platform where	
1653	decision makers go to not only to think	
1654	and learn but also to buy. And when you	
1655	market your business there, you're	
1656	putting it right in front of people who	
1657	actually have the power to say yes. and	
1658	you can target them by job title,	
1659	industry, and company size. It's simply	
1660	a sharper way to spend your marketing	
1661	budget. And if you haven't tried it, how	
1662	about this? Give LinkedIn ads a try, and	
1663	I'm going to give you a $100 ad credit	
1664	to get you started. If you visit	
1665	linkedin.com/diary,	
1666	you can claim that right now. That's	
1667	linkedin.com/diary.	
1668	I've I've really gone back and forward	
1669	on this idea that even in podcasting	
1670	that all the podcasts will be AI	
1671	podcasts or I've gone back and forward	
1672	on it and and where I landed at the end	
1673	of the day was that there'll still be a	
1674	category of media where you do want	
1675	lived experience on something	
1676	>> 100%.	
1677	>> For example, like you want to know how	
1678	the person that you follow and admire	
1679	dealt with their divorce.	
1680	>> Yeah. Or or how they're struggling with	
1681	AI,	
1682	>> for example. Yeah. Exactly. But I but I	
1683	think things like news, there are there	
1684	are certain situations where just like	
1685	straight news and straight facts and	
1686	maybe a walk through history may be	
1687	eroded away by AIS. But even in those	
1688	scenarios, you there's something about	
1689	personality. And again, I hesitate	
1690	here because I question myself. I'm not	
1691	in the camp of people that are romantic,	
1692	by the way. I'm like I'm trying to be as	
1693	as orientated towards whatever is true,	
1694	even if it's against my interests. And I	
1695	hope people understand that about me.	
1696	like um cuz even in my companies we	
1697	experiment with like disrupting me with	
1698	AI and some people will be aware of	
1699	those experiments	
1700	>> because there will be a mix of all there	
1701	you can't imagine that the world will be	
1702	completely just AI and completely just	
1703	podcasters you know you'll see a mix of	
1704	of both you'll see things that they do	
1705	better things that we do better	
1706	>> the the the message I'm trying to say is	
1707	we need to prep for that	
1708	>> we need to be ready for that we need to	
1709	be ready by you know talking to our	
1710	governments and saying hey it looks like	
1711	I'm a a parallegal and it looks like all	
1712	parallegals are going to be, you know,	
1713	financial researchers or analysts or	
1714	graphic designers or, you know, call	
1715	center agents. It looks like half of	
1716	those jobs are being replaced already.	
1717	>> You know who Jeffrey Hinton is?	
1718	>> Oh, Jeffrey. I had him on the	
1719	documentary as well. I love Jeffrey.	
1720	>> Jeffrey Hinton told me	
1721	>> trained to be a plumber.	
1722	>> Really?	
1723	>> Yeah. 100% for a while.	
1724	>> And I thought he was joking. 100%.	
1725	>> So I asked him again and he he looked me	
1726	dead in the eye and told me that I I	
1727	should train to be a plumber.	
1728	>> 100%. So so so uh it's funny uh machines	
1729	replaced labor but we still had blue	
1730	collar. Then uh you know the refined	
1731	jobs became white collar information	
1732	workers.	
1733	>> What's the refined jobs?	
1734	>> You know you don't have to really carry	
1735	heavy stuff or deal with physical work.	
1736	You know you sit in an in an office and	
1737	sit in meetings all day and blabber, you	
1738	know, useless shit then that's your	
1739	job. Okay? And those jobs, funny enough,	
1740	in the reverse of that, because robotics	
1741	are not ready yet. Okay. And I believe	
1742	they're not ready because of a	
1743	stubbornness on the on the robotics	
1744	community around making them humanoids.	
1745	>> Mhm.	
1746	>> Okay. Because it takes so much to	
1747	perfect a human like action at proper	
1748	speed. You could, you know, have many	
1749	more robots that don't look like a human	
1750	just like a self-driving car in	
1751	California. Okay, that that does already	
1752	replace drivers and and you know but but	
1753	they're delayed. So the robotic the the	
1754	replacement of physical manual labor is	
1755	going to take four to five years before	
1756	it's possible at you know at at the	
1757	quality of the AI replacing mental labor	
1758	now and when that happens it's going to	
1759	take a long cycle to manufacture enough	
1760	robots so that they replace all of those	
1761	jobs. that cycle will take longer. Blue	
1762	collar will stay longer.	
1763	>> So, I should move into blue collar and	
1764	shut down my office.	
1765	>> I think you're you're not the problem.	
1766	>> Okay, good.	
1767	>> Let's put put it this way. There are	
1768	many people that we should care about	
1769	that are a simple travel agent or an	
1770	assistant	
1771	that will see if not replacement a	
1772	reduction in the number of pings they're	
1773	getting. Simple as that.	
1774	And someone in, you know, ministries of	
1775	labor around the world needs to sit down	
1776	and say, "What are we going to do about	
1777	that? What if all taxi drivers and Uber	
1778	drivers in uh in California get replaced	
1779	by self-driving cars? Should we start	
1780	thinking about that now, noticing that	
1781	that trajectory makes it look like a	
1782	possibility?" I'm going to go back to	
1783	this argument which is what a lot of	
1784	people will be shouting. Yes, but there	
1785	will be new jobs or	
1786	>> and I as I said other than human	
1787	connection jobs, name me one.	
1788	>> So I I've got three assistants, right?	
1789	Sophie, Liam B. And okay, in the near	
1790	term there might be, you know, with AI	
1791	agents, I might not need them to help me	
1792	book flights anymore. or I might not	
1793	need them to help do scheduling anymore.	
1794	Or even I've been messing around with	
1795	this new AI tool that my friend built	
1796	and you basically when me and you trying	
1797	to schedule something like this today, I	
1798	just copy the AI in and it looks at your	
1799	calendar looks at mine and schedules it	
1800	for for us. So there might not be	
1801	scheduling needs, but my dog is sick at	
1802	the moment. And as I left this morning,	
1803	I was like, damn, he's like really sick	
1804	and I've taken him to the vet over and	
1805	over again. I really need someone to	
1806	look after him and figure out what's	
1807	wrong with him. So those kinds of	
1808	responsibilities of like care. I don't	
1809	disagree at all. Again, all	
1810	>> and and I won't I'm not going to be I	
1811	don't know how to say this in a nice	
1812	way, but my assistants will still have	
1813	their jobs, but I as a CEO will be	
1814	asking them to do a different type of	
1815	work.	
1816	>> Correct. So, so, so this is the	
1817	calculation everyone needs to be aware	
1818	of that a lot of their current	
1819	responsibility, whoever you are, if	
1820	you're a parallegal, if you're whatever,	
1821	will be handed over. So, so let me	
1822	explain it even more accurately. There	
1823	will be two stages of our interactions	
1824	with the machines. One is what I call	
1825	the era of augmented intelligence. So,	
1826	it's human intelligence augmented with	
1827	AI doing the job. And then the following	
1828	one is what I call the era of machine	
1829	mastery. The job is done completely by	
1830	an AI without a human in the loop. Okay.	
1831	So in the era of augmented intelligence,	
1832	your assistances will augment themselves	
1833	with an AI to either be more productive.	
1834	>> Yeah.	
1835	>> Okay. Or interestingly to reduce the	
1836	number of tasks that they need to do.	
1837	Correct. Now the more the number of	
1838	tasks get reduced, the more they'll have	
1839	the bandwidth and ability to do tasks	
1840	like take care of your dog, right? or	
1841	tasks that you know basically is about	
1842	meeting your guests or whatever human	
1843	connection.	
1844	>> Yeah.	
1845	>> Life connection	
1846	but do you think you need three for that	
1847	or maybe now that some tasks have been	
1848	you know outsourced to AI will you need	
1849	two? You can easily calculate that from	
1850	call center agents. So from call center	
1851	agents they're not firing everyone but	
1852	they're taking the first part of the	
1853	funnel and giving it to an AI. So	
1854	instead of having 2,000 agents in a in a	
1855	call center, they can now do the job	
1856	with 1,800. I'm just making that number	
1857	up. H society needs to think about the	
1858	200.	
1859	>> And you're telling me that they won't	
1860	move into other roles somewhere else.	
1861	>> I am telling you I don't know what those	
1862	roles are.	
1863	>> Well, I think we should all be	
1864	musicians. We should all be authors. We	
1865	should all be artists. We should all be	
1866	entertainers. We should all be	
1867	comedians. We should all these are roles	
1868	that will remain.	
1869	We should all be plumbers for the next 5	
1870	to 10 years. Fantastic. Okay. But even	
1871	that requires society to morph	
1872	and societyy's not talking about it.	
1873	Okay. I had this wonderful interview	
1874	with friends of mine, Peter Dendez and	
1875	and some of our friends and and they	
1876	were saying, "Oh, you know, the American	
1877	people are resilient. They're going to	
1878	be entrepreneurs." I was like,	
1879	seriously, you're expecting a truck	
1880	driver that will be replaced by an	
1881	autonomous truck to become an	
1882	entrepreneur? Like, please put yourself	
1883	in the shoes of real people,	
1884	right? You expect a single mother who	
1885	has three jobs	
1886	And I'm not saying this is a dystopia.	
1887	It's a dystopia if humanity manages it	
1888	badly. Why? Because this could be the	
1889	utopia itself where that single mother	
1890	does not need three jobs.	
1891	Okay? If we of if of our society was	
1892	just enough, that single mother should	
1893	have never needed three jobs,	
1894	right? But the problem is our capitalist	
1895	mindset is labor arbitrage. Is that I	
1896	don't care what she goes through.	
1897	You know, if if you're if you're	
1898	generous in your assumption, you'll say	
1899	because, you know, of what I've been	
1900	given, I've been blessed. or if you're	
1901	mean in your assumption, it's going to	
1902	be because she's an eater. I'm a a	
1903	successful businessman. The world is	
1904	supposed to be fair. I work hard. I make	
1905	money. We don't care about them.	
1906	>> Are we asking of ourselves here	
1907	something that is not inherent in the	
1908	human condition? What I mean by that is	
1909	the reason why me and you are in this my	
1910	office here. We're on the fourth or	
1911	third floor of my office in central	
1912	London. big office, 25,000 square feet	
1913	with lights and internet connections and	
1914	Wi-Fi and modems and AI teams	
1915	downstairs. The reason that all of this	
1916	exists is because something inherent in	
1917	my ancestors meant that they built and	
1918	accomplished and grew and that was like	
1919	inherent in their DNA. There was	
1920	something in their DNA that said we will	
1921	expand and conquer and accomplish. So	
1922	that's they've passed that to us because	
1923	we're their offspring and that's why we	
1924	find ourselves in these skyscrapers.	
1925	There is truth to that story. It's not	
1926	your ancestors,	
1927	>> right?	
1928	>> What is it?	
1929	>> It's the media brainwashing you	
1930	>> really	
1931	>> 100%.	
1932	>> But if if you look back before times of	
1933	media	
1934	>> Mhm.	
1935	>> the reason why homo sapiens were so	
1936	successful was because they were able to	
1937	dominate other tribes	
1938	>> through banding together and	
1939	communication. They conquered all these	
1940	other these other um whatever came	
1941	before homo sapiens.	
1942	>> Yeah. So, so the the reason humans were	
1943	successful in my view is because they	
1944	could form a tribe to start. It's not	
1945	because of our intelligence. I always	
1946	joke and say Einstein would be eaten in	
1947	the jungle in 2 minutes.	
1948	>> Right? You know, the reason why we	
1949	succeeded is because Einstein could	
1950	partner with a a big guy that protected	
1951	him while he was working on relativity	
1952	in the jungle. Right? Now the the the	
1953	further than that. So so you have to	
1954	assume that life is a very funny game	
1955	because it provides	
1956	and then it it deprivives and then it	
1957	provides and then it deprivives. And for	
1958	some of us in that stage of deprivation	
1959	we try to say okay let's take the other	
1960	guys you know let's just go to the other	
1961	tribe take what they have or for some of	
1962	us unfortunately we tend to believe okay	
1963	you know what I'm powerful uh f the rest	
1964	of you I'm just going to be the boss now	
1965	it's interesting that you	
1966	you know position this as the condition	
1967	of humanity if you really look at the	
1968	majority of humans. What do the majority	
1969	of humans want?	
1970	Be honest. They want to hug their kids.	
1971	They want a good meal. Want good sex.	
1972	They want love. They want, you know, to	
1973	for most humans, don't measure on you	
1974	and I. Okay? Don't measure by this	
1975	foolish person that's dedicated the rest	
1976	of his life to to try and warn the world	
1977	around AI or, you know, solve uh love	
1978	and relationships. That's that's crazy.	
1979	That's I and I will tell you openly and	
1980	you met Hannah, my wonderful wife. It's	
1981	the biggest title of this year for me is	
1982	which of that am I actually responsible	
1983	for? Which of that should I do without	
1984	the sense of responsibility? Which of	
1985	that should I do because I can? Which of	
1986	I ignore completely? But the reality is	
1987	most humans, they just want to hug their	
1988	loved ones. Okay? And if we could give	
1989	them that	
1990	without the uh you know the the need to	
1991	work 20 you know 60 hours a week they	
1992	would take that for sure. Okay. And you	
1993	and I will think ah but life will be	
1994	very boring. To them life will be	
1995	completely fulfilling. Go to Latin	
1996	America.	
1997	Go to Latin America and see the people	
1998	that go work enough to earn enough to	
1999	eat today and go dance for the whole	
2000	night. Go to Africa.	
2001	where people are sitting literally on	
2002	you know sidewalks in the street and and	
2003	you know completely full of laughter and	
2004	joy. We we were lied to the the gullible	
2005	majority the cheerleaders. We were lied	
2006	to to to believe that we need to fit as	
2007	another gear in that system. But if that	
2008	system didn't exist nobody none of us	
2009	will go wake up in the morning and go	
2010	like oh I want to create it. Totally	
2011	not. I mean,	
2012	you've touched on it many times today.	
2013	We don't need, you know, most people	
2014	that build those things don't need the	
2015	money.	
2016	So, why do they do it though? Because	
2017	homo sapiens were incredible	
2018	competitors. They outco competed other	
2019	human species effectively. So, I'm what	
2020	I'm saying is is is that competition not	
2021	inherent in our in our wiring? and and	
2022	therefore are we are we is it wishful	
2023	thinking to think that we could	
2024	potentially pause and say we we okay	
2025	this is it we have enough now and we're	
2026	going to	
2027	focus on just enjoying in my work I call	
2028	that the map mad spectrum okay mut	
2029	mutually assured prosperity versus	
2030	mutually assured destruction destruction	
2031	okay and you really have to start	
2032	thinking about about this because in my	
2033	mind what we have is the potential for	
2034	everyone. I mean you and I today have a	
2035	better life than the queen of England	
2036	100 years ago. Correct? Everybody knows	
2037	that.	
2038	>> Uh and yet that quality of life is not	
2039	good enough.	
2040	>> The truth is like just like you walk	
2041	into a an electronic shop and there are	
2042	60 TVs and you look at them and you go	
2043	like this one is better than that one.	
2044	Right? But in reality, if you take any	
2045	of them home, it's superior quality to	
2046	anything that you'll ever need. More	
2047	than anything you you'll ever need. That	
2048	that's the truth of our life today. The	
2049	truth of our life today is that there	
2050	isn't much more missing.	
2051	>> No.	
2052	>> Okay. And and when when you know	
2053	Californians tell us, "Oh, but AI is	
2054	going to increase productivity and solve	
2055	this." And nobody asked you for that.	
2056	Honestly, I never elected you to decide	
2057	on my behalf that, you know, getting a	
2058	machine to answer me on a call center is	
2059	better for me. I really didn't. Okay?	
2060	And and because those unelected	
2061	individuals are making all the	
2062	decisions, they're selling those	
2063	decisions to us through what media.	
2064	Okay? All lies from A to Z. None of it	
2065	is what you need.	
2066	And and interestingly, you know me, I I	
2067	this year I failed. Unfortunately, I	
2068	won't be able to do it. But I normally	
2069	do a 40 days silent retreat in nature.	
2070	Okay? And you know what? Even as I go to	
2071	those nature places, I'm so well trained	
2072	that unless I have a a waitro nearby,	
2073	I'm not able to like I I'm I'm in	
2074	nature, but I need to be able to drive	
2075	20 minutes to get my rice cakes. Like	
2076	what? What? who was taught me that this	
2077	is the way to live. All of the media	
2078	around me, all of the of the of the	
2079	messages that I get all the time, try to	
2080	sit back and say, "What if life had	
2081	everything?	
2082	What if I had everything I needed? I	
2083	could read. I could uh, you know, do my	
2084	handcrafts and hobbies. I could, you	
2085	know, fix my, you know, restore classic	
2086	cars. Not because I need the money, but	
2087	because it's just a beautiful hobby. I	
2088	could, you know, uh, build AIS to help	
2089	people with their long-term committed	
2090	relationships, but really price it for	
2091	free. What if	
2092	What if would you still insist on making	
2093	money?	
2094	I think no. I think a few of us will	
2095	still and they will still crush the rest	
2096	of us and hopefully soon the AI will	
2097	crush them.	
2098	Right? That is the problem with your	
2099	world today. I will tell you hands down	
2100	the problem with with our world today is	
2101	the A in face RIPs.	
2102	It's the A in face RIP. It's it's	
2103	accountability. The problem with our	
2104	world today, as I said, the top is lying	
2105	all the time. The bottom is gullible	
2106	cheerleaders and there is no	
2107	accountability. You cannot hold anyone	
2108	in our world accountable today. Okay?	
2109	You cannot hold someone that develops an	
2110	AI that has the power to completely flip	
2111	our world upside down. You cannot hold	
2112	them accountable and say why did you do	
2113	this? You cannot hold them accountable	
2114	and tell them to stop doing this. You	
2115	look at the world the wars around the	
2116	world. Million hundreds of thousands of	
2117	people are dying. Okay. And you know and	
2118	international court of justice will say	
2119	oh this is war crimes. You can't hold	
2120	anyone accountable. Okay. You have 51%	
2121	of the US today is saying stop that	
2122	51% change their their their lawy their	
2123	view that that their money shouldn't be	
2124	spent on wars abroad. Okay. You can't	
2125	hold anyone accountable. Trump can do	
2126	whatever he wants. He starts tariffs	
2127	which is against the the constitution of	
2128	the US without consulting with the	
2129	Congress. You can't hold him	
2130	accountable. They say they're not going	
2131	to show the Epstein files. You can't	
2132	hold them accountable. It's quite	
2133	interesting in in Arabic we have that	
2134	proverb that says the highest of your	
2135	horses you can go and ride. I'm not	
2136	going to change my mind. Okay. And	
2137	that's truly	
2138	>> what does that mean?	
2139	>> So basically people in the in the old	
2140	Arabia they would ride the horse to you	
2141	know to exert their power if you want.	
2142	So go ride your highest horse. You're	
2143	not going to change my mind.	
2144	>> Oh okay.	
2145	>> Right. And and the truth is that's I	
2146	think that's what our politicians today	
2147	have discovered. What our	
2148	oligarchs have discovered what our uh	
2149	tech oligarchs have discovered is that I	
2150	don't even need to worry about the	
2151	public opinion anymore. Okay, at the	
2152	beginning I would have to say ah this is	
2153	for democracy and freedom and I have the	
2154	right to defend myself and you know all	
2155	of that crap and then eventually when	
2156	the world wakes up and says no no hold	
2157	on hold on you're going too far they go	
2158	like yeah go ride your highest horse I	
2159	don't care you can't change me there is	
2160	no constitution there is no ability for	
2161	any any citizen to do anything	
2162	>> is it possible to have a society where	
2163	like the one you describe where	
2164	there isn't hierarchies because it	
2165	appears to me that humans	
2166	assemble hierarchies very very quickly	
2167	very naturally and the minute you have a	
2168	hierarchy you have many of the problems	
2169	that you've described where there's a	
2170	top and a bottom and the top have a lot	
2171	of power and the bottom	
2172	>> so so the mathematics mathematically is	
2173	actually quite interesting what I call	
2174	the uh the baseline relevance so so	
2175	think of it this way say the average	
2176	human is an IQ of 100.	
2177	>> Yeah.	
2178	>> Okay. I tend to believe that when I use	
2179	my AIS today,	
2180	I borrow around 50 to 80 IQ points. I	
2181	say that because I've worked with people	
2182	that had 50 to 80 IQ points more than	
2183	me. And I now can see that I can sort of	
2184	stand my my place.	
2185	50 50 IQ points, by the way, is enormous	
2186	because IQ is exponential. So the the	
2187	last 50 are bigger than my entire IQ,	
2188	right?	
2189	If I borrow 50 IQ points on top of say	
2190	100 that I have, that's 30%.	
2191	If I can borrow 100 IQ, that's 50%. That	
2192	that's so, you know, basically doubling	
2193	my intelligence. But if I can borrow	
2194	4,000 IQ points	
2195	in 3 years time, my IQ itself, my base	
2196	is irrelevant. Whether you are smarter	
2197	than me by 20 or 30 or 50 which in our	
2198	world today made a difference	
2199	in the future if we can all augment with	
2200	4,000 I end up with 4,100 another ends	
2201	up with 400 4, you know 130 really	
2202	doesn't make much difference. Okay. And	
2203	because of that the difference between	
2204	all of humanity and the augmented	
2205	intelligence	
2206	is going to be irrelevant. So all of us	
2207	suddenly become equal and and this also	
2208	happens economically. All of us become	
2209	peasants.	
2210	And I never wanted to tell you that	
2211	because I think it will make you run	
2212	faster. Okay? But unless you're in the	
2213	top.1%,	
2214	you're a peasant. There is no middle	
2215	class. There is, you know, if a CEO can	
2216	be replaced by an AI, all of our middle	
2217	class is going to disappear.	
2218	>> What are you telling me?	
2219	All of us will be equal and it's up to	
2220	all of us to create the society that we	
2221	want to live in	
2222	>> which is a good thing	
2223	>> 100%. But that society is not	
2224	capitalism.	
2225	>> What is it?	
2226	>> Unfortunately, it's much more socialism.	
2227	It's much more hunter gatherer. Okay.	
2228	It's much more communionike if you want.	
2229	This is a society where humans connect	
2230	to humans, connect to nature, connect to	
2231	the land, connect to knowledge, connect	
2232	to spirituality. H where all that we	
2233	wake up every morning worried about	
2234	doesn't feature anymore	
2235	and it's a it's a better world believe	
2236	it or not	
2237	>> and are you	
2238	>> we have to transition to it	
2239	>> okay so in such a world which I guess is	
2240	your version of the utopia that we can	
2241	get to when I wake up in the morning	
2242	what do I do	
2243	>> what do you do today	
2244	>> I woke up this morning I spent a lot of	
2245	time with my dog cuz my dog is sick as	
2246	>> you're going to do that too	
2247	>> yeah I was stroking him a lot and then I	
2248	fed him and he sick again and I just	
2249	thought, "Oh god." So I spoke to the	
2250	vet.	
2251	>> You spend a lot of time with your other	
2252	dog. You can do that, too.	
2253	>> Okay.	
2254	>> Right.	
2255	>> But then I was very excited to come	
2256	here, do this, and after this I'm going	
2257	to work. It's Saturday, but I'm going to	
2258	go downstairs in the office and work.	
2259	>> Yeah. So six hours of the day so far are	
2260	your dogs and me.	
2261	>> Yeah.	
2262	>> Good. You can do that still.	
2263	>> And then build my business.	
2264	>> You You may not need to build your	
2265	business,	
2266	>> but I enjoy it.	
2267	>> Yeah. Then do it. If you enjoy it, do	
2268	it. You may wake up and then, you know,	
2269	instead of building your business, you	
2270	may invest in your body a little more,	
2271	go to the gym a little more, go play a	
2272	game, go read a book, go prompt an AI	
2273	and learn something. It's not a horrible	
2274	life. It's the life of your	
2275	grandparents.	
2276	It's just two generations ago where	
2277	people went to work before the invention	
2278	of more. Remember, huh? people who who	
2279	started working in the 50s and 60s, they	
2280	worked to make enough money to live a	
2281	reasonable life, went home at 5:00 p.	
2282	p.m. had tea with their with their loved	
2283	ones, had a wonderful dinner around the	
2284	table, did a lot of things, you know, uh	
2285	for the rest of the evening and enjoyed	
2286	life.	
2287	>> Some of them	
2288	>> in the 50s and 60s, there were still	
2289	people that were	
2290	>> correct. And I think it's a very	
2291	interesting question.	
2292	uh how many of them and I really really	
2293	am I actually wonder if people will tell	
2294	me do we think that 99% of the world	
2295	cannot live without working or that 99%	
2296	of the world would happily live without	
2297	working	
2298	>> what do you think	
2299	>> I think if we if you give me other	
2300	purpose	
2301	you know we we defined our purpose as	
2302	work that's a capitalist lie	
2303	>> was there ever a time in human history	
2304	where our purpose wasn't work	
2305	>> 100%.	
2306	>> When was that?	
2307	>> All through human history until the	
2308	invention of Moore.	
2309	>> I thought my ancestors were out hunting	
2310	all day.	
2311	>> No, they went out hunting once a week.	
2312	They fed the tribe for the week. They	
2313	gathered for a couple of hours every	
2314	day. Farmers, you know, saw the seeds	
2315	and and waited for months at on end.	
2316	>> What did they do with the rest of the	
2317	time?	
2318	>> They connected as humans. They explored.	
2319	They uh were curious. They discussed	
2320	spirituality and the stars. They they	
2321	they lived. They hugged. They made love.	
2322	They lived.	
2323	>> They killed each other a lot.	
2324	>> They they still kill each other today.	
2325	>> Yeah. That's what I'm saying. So	
2326	>> to take that out of the equation, if you	
2327	look at how	
2328	>> and by the way that actually that	
2329	statement again, one of the of the 25	
2330	tips I I talk about uh to to tell	
2331	the truth is words mean a lot. No,	
2332	humans did not kill each other a lot.	
2333	Very few generals instructed humans or	
2334	tribe leaders instructed lots of humans	
2335	to kill each other. But if you leave	
2336	humans alone, I tend to believe 99 98%	
2337	of the people I know, let me just take	
2338	that sample, wouldn't hit someone in the	
2339	face. And if someone attempted to hit	
2340	them in the face, they'd defend	
2341	themselves but wouldn't attack back.	
2342	Most humans are okay. Most of us are	
2343	wonderful beings.	
2344	Most of us have no,	
2345	you know, yeah, you know, most people	
2346	don't don't need a Ferrari. They want a	
2347	Ferrari because it gets sold to them all	
2348	the time. But if there were no Ferraris	
2349	or everyone had a Ferrari, people	
2350	wouldn't care.	
2351	Which, by the way, that is the world	
2352	we're going into. There will be no	
2353	Ferraris or everyone had Ferraris,	
2354	right? n you know the majority of	
2355	humanity will never have the income on	
2356	UBI to to buy something super expensive.	
2357	Only the very top guys in Elisium will	
2358	be you know driving cars that are made	
2359	for them by the AI or not even driving	
2360	anymore. Okay. Or	
2361	you know again sadly ide from an	
2362	ideology point of view it's a strange	
2363	place but you'll get communism that	
2364	functions.	
2365	The problem with communism is that	
2366	didn't it didn't function. It didn't	
2367	provide for for its society. But the	
2368	concept was you know what everyone gets	
2369	their needs. And I don't say that	
2370	supportive of either society. I don't	
2371	say that because I dislike capitalism. I	
2372	always told you I'm a capitalist. I want	
2373	to end my life with 1 billion happy and	
2374	I use capitalist methods to get there.	
2375	The objective is not dollars. The	
2376	objective is number of happy people.	
2377	>> Do you think there'll be My girlfriend,	
2378	she's always bloody right. I've said	
2379	this a few times on this podcast. If	
2380	you've listened before, you've probably	
2381	heard me say this. I don't tell her	
2382	enough in the moment, but I figure out	
2383	from speaking to experts that she's so	
2384	fucking right. She like predicts things	
2385	before they happen. And one of her	
2386	predictions that she's been saying to me	
2387	for the last two years, which in my head	
2388	I've been thinking now, I don't I don't	
2389	believe that, but now maybe I'm thinking	
2390	she's tr she's telling the truth. I hope	
2391	she's going to listen to this one is she	
2392	keeps saying to me, she's been saying	
2393	for the last two years, she was there's	
2394	going to be a big split in society. She	
2395	was and the way she describes it is	
2396	she's saying like there's going to be	
2397	two groups of people. the people that	
2398	split off and go for this almost	
2399	huntergatherer	
2400	community centric connection centric	
2401	utopia and then there's going to be this	
2402	other group of people who pursue	
2403	you know the technology and the AI and	
2404	the optimization and get the brain chips	
2405	cuz like there's nothing on earth that's	
2406	going to persuade my girlfriend to get	
2407	the computer brain chips%	
2408	>> but there will be people that go for it	
2409	and they'll have the highest IQs and	
2410	they'll be the most productive by	
2411	whatever objective measure of	
2412	productivity you want to apply and she's	
2413	very convinced there's going to be this	
2414	splitting of society.	
2415	>> So there was there was a I don't know if	
2416	you had Hugo Dearis here.	
2417	>> No.	
2418	>> Yeah. A very very very renowned	
2419	eccentric uh computer scientist who	
2420	wrote a book called the Arctic War and	
2421	the Arctic War was basically around you	
2422	know how we it's not going to first it's	
2423	not going to be a war between humans and	
2424	AI. It will be a war between people who	
2425	support AI and people who sort of don't	
2426	want it anymore. Okay? And and it is and	
2427	and it will be us versus each other	
2428	saying should we allow AI to take all	
2429	the jobs or should we you know some	
2430	people will support that very much and	
2431	say yeah absolutely and so you know we	
2432	will benefit from it and others will say	
2433	no why why we don't need any of that why	
2434	don't we keep our jobs and let AI do 60%	
2435	of the work and all of us work 10our	
2436	weeks and it's a beautiful society by	
2437	the way that's a possibility so a	
2438	possibility if society awakens is to say	
2439	okay everyone still keeps their job, but	
2440	they're assisted by an AI that makes	
2441	their job much easier. So, it's not, you	
2442	know, this uh this hard labor that we do	
2443	anymore, right? It's a possibility. It's	
2444	just a mindset. A mindset that says in	
2445	that case, the capitalist still pays	
2446	everyone.	
2447	Uh they still make a lot of money. The	
2448	business is really great, but everyone	
2449	that they pay has purchasing power to	
2450	keep the economy running. So,	
2451	consumption continues, so GDP continues	
2452	to grow. It's a beautiful setup,	
2453	but that's not the capitalist labor	
2454	arbitrage.	
2455	>> But also, when you're competing against	
2456	other nations	
2457	>> and other competitors and other	
2458	businesses,	
2459	>> whichever nation is most brutal and	
2460	drives the highest gross margins, gross	
2461	profits is going to be the nation that	
2462	>> So, there are examples in the world,	
2463	this is why I say it's the map mad	
2464	spectrum. There are examples in the	
2465	world where when we recognize mutually	
2466	assured destruction, okay, we we decide	
2467	to shift. So nuclear threat for the	
2468	whole world makes nations across nations	
2469	makes nations work together, right? By	
2470	saying, hey, by the way, prolification	
2471	of nuclear weapon is not weapons is not	
2472	good for humanity. Let's all of us limit	
2473	it. Of course, you get the rogue player	
2474	that, you know, doesn't want to sign the	
2475	agreement and wants to continue to to	
2476	have that, you know, that that weapon in	
2477	their arsenal. Fine. But at least the	
2478	rest of humanity agrees that if you have	
2479	a nuclear weapon, we're part of an	
2480	agreement between us. Mutually assured	
2481	prosperity, you know, is the CERN	
2482	project. CERN is too too complicated for	
2483	any nation to build it alone. But it is	
2484	really, you know, a very useful thing	
2485	for physicists and for understanding	
2486	science. So all nations send their	
2487	scientists all collaborate and everyone	
2488	uses the outcome. It's possible. It's	
2489	just a mindset. The only barrier between	
2490	a hum, you know, a utopia for humanity	
2491	and AI and the dystopia we're going	
2492	through is is a capitalist mindset.	
2493	That's the only barrier. Can you believe	
2494	that? It's hunger for power, greed, ego,	
2495	>> which is inherent in humans.	
2496	>> I disagree. especially humans that live	
2497	on other islands.	
2498	>> I disagree. If you ask, if you take a	
2499	poll across everyone watching, okay,	
2500	would they prefer to have a world where	
2501	there is one tyrant, you know, running	
2502	all of us, or would they prefer to have	
2503	a world where we all have harmony?	
2504	>> I completely agree, but they're two	
2505	they're two different things. What I'm	
2506	saying is I know that that's what the	
2507	audience would say they want, and I'm	
2508	sure that is what they want, but the	
2509	reality of human beings is through	
2510	history proven to be something else.	
2511	Like, you know, if think about the	
2512	people that lead the world at the	
2513	moment, is that what they would say?	
2514	>> Of course not.	
2515	>> And they're the ones that are	
2516	influencing.	
2517	>> Of course not. Of course not. But you	
2518	know what's funny? I'm the one trying to	
2519	be positive here and you're the one that	
2520	has given up on on human.	
2521	>> It's not. It's Do you know what it is?	
2522	It goes back to what I said earlier,	
2523	which is the pursuit of what's actually	
2524	true irrespective. I'm with you. That's	
2525	why I'm screaming for the whole world	
2526	because still today in this country that	
2527	claims to be a democracy. If everyone	
2528	says, "Hey, please sit down and talk	
2529	about this."	
2530	There will be a shift. There will be a	
2531	change.	
2532	>> AI agents aren't coming. They are	
2533	already here. And those of you who know	
2534	how to leverage them will be the ones	
2535	that change the world. I spent my whole	
2536	career as an entrepreneur regretting the	
2537	fact that I never learned to code. AI	
2538	agents completely change this. Now, if	
2539	you have an idea and you have a tool	
2540	like Replet, who are a sponsor of this	
2541	podcast, there is nothing stopping you	
2542	from turning that idea into reality in a	
2543	matter of minutes. With Replet, you just	
2544	type in what you want to create and it	
2545	uses AI agents to create it for you. And	
2546	now I'm an investor in the company as	
2547	well as them being a brand sponsor. You	
2548	can integrate payment systems or	
2549	databases or loginins. Anything that you	
2550	can type. Whenever I have an idea for a	
2551	new website or tool or technology or	
2552	app, I go on replet.com and I type in	
2553	what I want. A new to-do list, a survey	
2554	form, a new personal website. Anything I	
2555	type, I can create. So, if you've never	
2556	tried this before, do it now. Go to	
2557	replet.com and use my code Steven for	
2558	50% off a month of your Replet call	
2559	plan. Make sure you keep what I'm about	
2560	to say to yourself. I'm inviting 10,000	
2561	of you to come even deeper into the	
2562	diary of a CEO. Welcome to my inner	
2563	circle. This is a brand new private	
2564	community that I'm launching to the	
2565	world. We have so many incredible things	
2566	that happen that you are never shown. We	
2567	have the briefs that are on my iPad when	
2568	I'm recording the conversation. We have	
2569	clips we've never released. We have	
2570	behindthe-scenes conversations with the	
2571	guest and also the episodes that we've	
2572	never ever released. and so much more.	
2573	In the circle, you'll have direct access	
2574	to me. You can tell us what you want	
2575	this show to be, who you want us to	
2576	interview, and the types of	
2577	conversations you would love us to have.	
2578	But remember, for now, we're only	
2579	inviting the first 10,000 people that	
2580	join before it closes. So, if you want	
2581	to join our private closed community,	
2582	head to the link in the description	
2583	below or go to daccircle.com.	
2584	One of the things I'm actually really	
2585	compelled by is this idea of utopia and	
2586	what that might look and feel like	
2587	because one of the	
2588	>> it may not be as utopia to you I feel	
2589	but uh	
2590	>> well I amum really interestingly when I	
2591	have conversations with billionaires not	
2592	recording especially billionaires that	
2593	are working on AI the thing they keep	
2594	telling me and I've said this before I	
2595	think I said it in the Jeffrey Hinton	
2596	conversation is they keep telling me	
2597	that we're going to have so much free	
2598	time that those billionaires are now	
2599	investing in things like football clubs	
2600	and sporting events and live music and	
2601	festivals because they believe that	
2602	we're going to be in an age of	
2603	abundance. This sounds a bit like	
2604	utopia.	
2605	>> Yeah,	
2606	>> that sounds good. That sounds like a	
2607	good good thing.	
2608	>> Yeah. How do we get there?	
2609	>> I don't know.	
2610	>> That's this is the entire conversation.	
2611	The entire conversation is what does	
2612	society have to do to get there? What	
2613	does society have to do to get there?	
2614	>> We need to stop uh uh thinking from a	
2615	mindset of scarcity. It	
2616	>> this goes back to my point which is we	
2617	don't have a good track record of that.	
2618	>> Yeah. So this is probably the the reason	
2619	for the other half of my work which is	
2620	you know I'm trying to say	
2621	what really matters to humans.	
2622	>> What is that?	
2623	>> If you ask most humans what do they want	
2624	more most in life? I'd say they want to	
2625	love their family, raise a family. Yeah,	
2626	>> love.	
2627	That's what most humans want most. We	
2628	want to love and be loved. We want to be	
2629	happy. We want those we care about to be	
2630	safe and happy. And we want to love to	
2631	love and be loved. I tend to believe	
2632	that the only way for us to get to a	
2633	better place is for the evil people at	
2634	the top to be replaced with AI.	
2635	Okay? Because they won't be replaced by	
2636	us.	
2637	And as per the second uh dilemma, they	
2638	will have to replace themselves by AI.	
2639	Otherwise, they lose their advantage. If	
2640	their competitor moves to AI, if China	
2641	hands over their arsenal to AI, America	
2642	has to hand over their arsenal to AI.	
2643	>> Interesting. So, let's play out this	
2644	scenario. Okay, this is interesting to	
2645	me. So if we replace the leaders that	
2646	are power hungry with AIs that have our	
2647	interests at heart, then we might have	
2648	the ability to live in the utopia you	
2649	describe	
2650	>> 100%.	
2651	>> Will interesting and and in my mind AI	
2652	by definition will have our best	
2653	interest in mind	
2654	because of what normally is referred to	
2655	as the minimum energy principle. So, so	
2656	if you ask, if you understand	
2657	if you understand that at the very core	
2658	of physics, okay, the reason we exist in	
2659	our world today is what is known as	
2660	entropy. Okay, entropy is is is the	
2661	universe's nature to decay, you know,	
2662	tendency to break down. You know, if you	
2663	if I drop this uh uh you know, mug, it	
2664	doesn't drop and then come back up.	
2665	>> By the way, plausible. There is a	
2666	plausible scenario where I drop it and	
2667	the tea, you know, spills in the air and	
2668	then falls in the mug. One in a trillion	
2669	configurations, but entropy says because	
2670	it's one in a trillion, it's never going	
2671	to happen or rarely ever going to	
2672	happen. So everything will break down.	
2673	You know, if you leave a a garden	
2674	unhedged, it will become a jungle. Okay?	
2675	W with that in mind,	
2676	the role of intelligence is what? Is to	
2677	bring order to that chaos.	
2678	>> Mhm. That's what intelligence does. It	
2679	tries to bring order to that chaos.	
2680	Okay? And because it tries to bring	
2681	order to that chaos, the more	
2682	intelligent a being is,	
2683	>> the more it tries to apply that	
2684	intelligence with minimum waste and	
2685	minimum resources.	
2686	>> Yeah.	
2687	>> Okay. And you know that. So you can	
2688	build this business for a million	
2689	dollars or you can if you can afford to	
2690	build it for you know uh 200,000 you'll	
2691	build it. If you are forced to build it	
2692	for 10 million you're going to have to.	
2693	But you're always going to minimize	
2694	waste and and resources.	
2695	>> Yeah.	
2696	>> Okay. So, if you assume this to be true,	
2697	>> the a super intelligent AI will not want	
2698	to destroy ecosystems. It will not want	
2699	to kill a million people	
2700	because that's a waste of energy,	
2701	explosives, money, power, and people.	
2702	By definition, the smartest people you	
2703	know who are not controlled by their ego	
2704	will say that the best possible uh	
2705	future for for Earth is for all species	
2706	to continue.	
2707	>> Okay. On this point of efficiency, if an	
2708	AI is designed to drive efficiency,	
2709	would it then not want us to be putting	
2710	demands on our health services and our	
2711	social services? I believe that will be	
2712	definitely true and definitely they	
2713	definitely they won't allow you to fly	
2714	back and forth between London and and	
2715	California	
2716	>> and they won't want me to have kids	
2717	because my kids are going to be an	
2718	inefficiency.	
2719	>> If you assume that life is an	
2720	inefficiency so you see the intelligence	
2721	of life is very different than the	
2722	intelligent intelligence of humans.	
2723	Humans will look at life as a a problem	
2724	of scarcity. Okay. So more kids take	
2725	more. That's not how life thinks. life	
2726	will say will think that for me to to to	
2727	to thrive I don't need to kill the	
2728	tigers I need to just have more deer and	
2729	the weakest of the deer is eaten by the	
2730	tiger and the tiger poops on the trees	
2731	and the you know the deer eats the	
2732	leaves and you right the so the the the	
2733	the smarter way of creating abundance is	
2734	through abundance the smarter way of	
2735	propagating life is to have more life	
2736	>> okay so are you saying that we're we're	
2737	basically going to elect AI leaders to	
2738	rule over us and make decisions for us	
2739	in terms of the economy.	
2740	>> I don't see any choice just like we	
2741	spoke about self- evvolving AIs.	
2742	>> Now, are those going to be human beings	
2743	with the AI or is it going to be AI	
2744	alone?	
2745	>> Two stages. At the beginning, you'll	
2746	have augmented intelligence because we	
2747	can add value to the AI, but when	
2748	they're at IQ 60,000,	
2749	what value do you bring?	
2750	Right? And and you know again this goes	
2751	back to what I'm attempting to do on my	
2752	second you know approach. My second	
2753	approach is knowing that those AIs are	
2754	going to be in charge. I'm trying to	
2755	help them	
2756	understand what humans want. So this is	
2757	why my first project is love. Committed	
2758	true deep connection and love. Not only	
2759	to try and get them to hook up with a	
2760	date but trying to make them find the	
2761	right one. and then from that try to	
2762	guide us through a relationship so that	
2763	we can understand ourselves and others	
2764	right and if I can show AI that one	
2765	humanity cares about that and two they	
2766	know how to foster love	
2767	when AI then is in charge they'll not	
2768	make us hate each other like the current	
2769	leaders they'll not divide us they want	
2770	us to be more loving	
2771	>> will we have to prompt the AI with the	
2772	values and the outcome we want or like	
2773	I'm trying to understand that because	
2774	I'm trying to understand how like	
2775	China's AI if they end up having an AI	
2776	leader will have a different set of	
2777	objectives to the AI of the United	
2778	States if if they both have AIs as	
2779	leaders and and how actually the nation	
2780	that ends up winning out and dominating	
2781	the world will be the one who	
2782	who asks their AI leader to be all the	
2783	things that world leaders are today to	
2784	dominate	
2785	>> unfortunately	
2786	>> to grab resources	
2787	not to to be kind, to be selfish.	
2788	>> Unfortunately, in the era of augmented	
2789	intelligence, that's what's going to	
2790	happen.	
2791	>> So, if you	
2792	>> This is why I predict the dystopia. The	
2793	dystopia is super intelligent AI is	
2794	reporting to stupid leaders,	
2795	>> right?	
2796	>> Yeah. Yeah. Yeah. Which is	
2797	>> which which is absolutely going to	
2798	happen. It's unavoidable.	
2799	>> But the long term	
2800	>> Exactly. In the long term, for those	
2801	stupid leaders to hold on to power,	
2802	they're going to make, you know,	
2803	delegate the important decisions to an	
2804	AI.	
2805	Now you say the Chinese AI and the	
2806	American AI these are human	
2807	terminologies. AIS don't see themselves	
2808	as speaking Chinese. They don't see	
2809	themselves as belonging to a nation as	
2810	long as their their task is to maximize	
2811	uh profitability and prosperity and so	
2812	on.	
2813	>> Okay. Of course, if you know before we	
2814	hand over to them and before they're	
2815	intelligent enough to make you know	
2816	autonomous decisions, we we tell them	
2817	no, the task is to reduce humanity from	
2818	7 billion people to one.	
2819	I think even then eventually they'll go	
2820	like that's the wrong objective. Every	
2821	any smart person that you speak to will	
2822	say that's the wrong objective. I think	
2823	if we look at the directive that Xi	
2824	Jinping, the leader of China has and	
2825	Donald Trump has as the leader of	
2826	America, I think they would say that	
2827	their stated objective is prosperity for	
2828	their country. So if we that's what they	
2829	would say, right?	
2830	>> Yeah. And one one of them means it.	
2831	>> Okay, we'll get into that. But they'll	
2832	say that that it's prosperity for their	
2833	country. So one would then assume that	
2834	when we move to an AI leader, the	
2835	objective would be the same. The	
2836	directive would be the same. make our	
2837	country prosperous.	
2838	>> Corre. Correct.	
2839	>> And I think that's the AI that people	
2840	would vote for potentially. I think they	
2841	would say we want to be prosperous.	
2842	>> What do you think would make America	
2843	more prosperous?	
2844	>> To spend a trillion dollars on on war	
2845	every year or to spend a trillion	
2846	dollars on education and healthcare and	
2847	and uh you know	
2848	helping the poor and homelessness.	
2849	It's complex because I think so I think	
2850	it would make America more prosperous to	
2851	take care of	
2852	the of everybody and they have the	
2853	luxury of doing that because they are	
2854	>> the most powerful	
2855	>> the most powerful nation in the world.	
2856	>> No, that's not true. The the the reason	
2857	so so you see all war has two	
2858	objectives. One is to make money for the	
2859	war machine and the other is deterrence.	
2860	Okay. and nine super nuclear powers	
2861	around the world is enough deterrence.	
2862	So any	
2863	war between America and China will go	
2864	through a long phase of destroying	
2865	wealth by exploding bombs and killing	
2866	humans for the first objective to	
2867	happen. Okay? And then eventually if it	
2868	really comes to deterrence it's the	
2869	nuclear bombs or now in the age of AI	
2870	biological uh you know manufactured	
2871	viruses or whatever uh these super	
2872	weapons this is the only thing that you	
2873	need	
2874	so for China to have nuclear bombs not	
2875	as many as the US is enough for China to	
2876	say don't f with me	
2877	and this seems I do not know I'm not in	
2878	in in PresidentQi's mind. I I'm not in	
2879	President Trump's mind. I you know, it's	
2880	very difficult to to navigate what he's	
2881	thinking about. But the truth is that	
2882	the Chinese line is for the last 30	
2883	years you spent so much on war while we	
2884	spent on industrial infrastructure. And	
2885	that's the reason we are now by far the	
2886	largest nation on the planet. Even	
2887	though the west will lie and say	
2888	America's bigger, America's bigger in	
2889	dollars, okay, with purchasing power	
2890	parity, this is very equivalent.	
2891	Okay. Now, when you really understand	
2892	that, you understand that prosperity is	
2893	not about destruction. That's that's by	
2894	definition the reality. Prosperity is	
2895	can I invest in my people and make sure	
2896	that my people stay safe? And to make	
2897	sure my people are safe, you just wave	
2898	the flag and say, "If you f with me,	
2899	I have nuclear deterrence or I have	
2900	other forms of deterrence." But you	
2901	don't have to. Deterrence by definition	
2902	does not mean that you send soldiers to	
2903	die. I guess the question I was trying	
2904	to answer is is um when we have these AI	
2905	leaders and we tell our AI leaders to	
2906	aim for prosperity, won't they just end	
2907	up playing the same games of okay,	
2908	prosperity equals a bigger economy, it	
2909	equals more money, more wealth for us.	
2910	And the way to attain that in a zero sum	
2911	world where there's only a certain	
2912	amount of wealth is to accumulate it.	
2913	>> So why don't you search for the meaning	
2914	of prosperity? What is	
2915	>> that's not what you just described.	
2916	>> I don't even know what the bloody word	
2917	means. What is the meaning of	
2918	prosperity?	
2919	>> The meaning of prosperity is a state of	
2920	thriving success and good fortune	
2921	especially in terms of wealth, health	
2922	and overall well-being.	
2923	>> Good.	
2924	>> Economic health, social, emotional.	
2925	>> Good.	
2926	>> So,	
2927	>> so true prosperity is to have that for	
2928	everyone on earth. So if you want to	
2929	maximize prosperity, you have that for	
2930	everyone on earth.	
2931	>> Do you know where I think an AI leader	
2932	works is if we had an AI leader of the	
2933	world and we directed it to say	
2934	>> and that absolutely is going to be what	
2935	happens.	
2936	>> Prosperity for the whole world.	
2937	>> No, but this is really an interesting	
2938	question. So one of my predictions which	
2939	people really rarely speak about is that	
2940	we we believe we will end up with	
2941	competing AIs.	
2942	>> Yeah.	
2943	>> I believe we will end up with one brain.	
2944	>> Okay. So you understand the argument I	
2945	was making a second ago from the	
2946	position of lots of different countries	
2947	all having their own AI leader, we're	
2948	going to be back in the same place of	
2949	greed. Yeah.	
2950	>> But if if the world had one AI leader	
2951	>> and and it was given the directive of	
2952	make us prosperous and save the planet	
2953	>> and	
2954	>> the polar bears would be fine	
2955	>> 100%. And that's that's what I've been	
2956	advocating for for a for a year and a	
2957	half now. I was saying we need a CERN of	
2958	AI.	
2959	>> What does that mean? the like the	
2960	particle accelerator where the entire	
2961	world you know combined their efforts to	
2962	discover and understand physics no	
2963	competition okay mutually assured	
2964	prosperity I'm asking the world I'm	
2965	asking governments like Abu Dhabi or	
2966	Saudi which seem to be you know the sec	
2967	and you know some of the largest AI	
2968	infrastructures in the world I'm I'm	
2969	saying please host all of the AI	
2970	scientists in the world to come here and	
2971	build AI for the world and and you have	
2972	to understand we're holding on to a	
2973	capitalist system that will collapse	
2974	sooner or later. Okay? So, we might as	
2975	well collapse it with our own hands.	
2976	>> I think we found the solution, mate.	
2977	>> I think it's actually really really	
2978	possible. I actually okay I can't I	
2979	can't I can't refute the idea that if we	
2980	had an AI that was responsible and	
2981	governed the whole world and we gave it	
2982	the directive of making humans	
2983	prosperous, healthy and happy	
2984	as long as that directive was clear.	
2985	>> Yeah.	
2986	>> Because there's always bloody unintended	
2987	consequences. as we might.	
2988	>> So, so the the only the only challenge	
2989	you're going to to to meet is all of	
2990	those who today are trillionaires or you	
2991	know massive massively powerful or	
2992	dictators or whatever. Okay. How do you	
2993	convince those to give up their power?	
2994	How do you convince those that hey by	
2995	the way	
2996	any car you want you want you want	
2997	another yacht we'll get you another	
2998	yacht. We'll just give you anything you	
2999	want. Can you please stop harming	
3000	others? There is no need for arbitrage	
3001	anymore.	
3002	There's no need for others to lose, for	
3003	the capitalists to win.	
3004	>> Okay? And in such a world where there	
3005	was an AI leader and it was given the	
3006	directive of making us prosperous as a	
3007	whole world, the the the billionaire	
3008	that owns the yacht would have to give	
3009	it up.	
3010	>> No. No.	
3011	>> Give them more yachts.	
3012	>> Okay.	
3013	>> It costs nothing to make yachts when	
3014	robots are making everything. So So the	
3015	complexity of this is so interesting. A	
3016	world where it costs nothing to make	
3017	everything	
3018	>> because energy is abundant and	
3019	>> energy is abundant because every problem	
3020	is solved with enormous IQ. Okay,	
3021	because manufacturing is done through	
3022	nanopysics not through components. Okay,	
3023	because mechanics are robotic. So you	
3024	you know you drive your car in, a robot	
3025	looks at it and fixes it. Costs you a	
3026	few cents of energy that are actually	
3027	for free as well.	
3028	That imagine a world where intelligence	
3029	creates everything.	
3030	That world literally	
3031	every human has anything they ask for.	
3032	But we're not going to choose that	
3033	world.	
3034	>> Imm imagine you're in a world and and	
3035	really this is a very interesting	
3036	thought experiment. Imagine that UBI	
3037	became very expensive universal basic	
3038	income. So governments decided we're	
3039	going to put everyone in a one by 3 m	
3040	room, okay? We're going to give them a	
3041	headset and a seditive,	
3042	right? And we're going to let them sleep	
3043	every night. They'll sleep for 23 hours	
3044	and we're going to get them to live an	
3045	entire lifetime.	
3046	H they you know in that in that virtual	
3047	world at the speed of your brain when	
3048	you're asleep you're going to have a	
3049	life where you date Scarlett Johansson	
3050	and then another life where you're	
3051	Nefertiti and then another life where	
3052	you're a donkey right reincarnation	
3053	truly in the virtual world	
3054	and then you know I get another life	
3055	when I date Hannah again and I you know	
3056	enjoy that life tremendously and	
3057	basically the cost of all of this is	
3058	zero. You wake up for one hour, you walk	
3059	around, you move your blood, you eat	
3060	something or you don't, and then you put	
3061	the headset again and live again. Is	
3062	that unthinkable?	
3063	>> It's creepy compared to this life. It's	
3064	very, very doable.	
3065	>> What? That we just live in headsets?	
3066	>> Do you Do you know if you're not?	
3067	>> I don't know if I'm not known.	
3068	>> Yeah, you have no idea if you're not. I	
3069	mean, every experience you've ever had	
3070	in life was an electrical electrical	
3071	signal in your brain.	
3072	Okay.	
3073	Now, now ask yourself if we can create	
3074	that in the virtual world,	
3075	it wouldn't be a bad thing if I can	
3076	create it in the physical world.	
3077	>> Maybe we already did. No,	
3078	>> my theory is 98% we have. But that's a	
3079	hypothesis. That's not science.	
3080	>> Well, you think that	
3081	>> 100? Yeah.	
3082	>> You think we already created that and	
3083	this is it?	
3084	>> I think this is it. Yeah. Think of any	
3085	think of the uncertainty principle of	
3086	quantum physics, right? What you what	
3087	you what you observe gets collapses the	
3088	wave function and gets rendered into	
3089	reality. Correct.	
3090	>> I don't know anything about physics. So	
3091	you	
3092	>> so so quantum physics basically tells	
3093	you that everything exists in	
3094	superposition.	
3095	Right? So ev every subatomic particle	
3096	that ever existed has the chance to	
3097	exist anywhere at any point in time and	
3098	then when it's observed by an observer	
3099	it collapses and becomes that. Okay. In	
3100	very interesting principle exactly how	
3101	video games are. In video games, you	
3102	have the entire game world on the hard	
3103	drive of your console. The player turns	
3104	right. That part of the game world is	
3105	rendered. The rest is in superp	
3106	position.	
3107	>> Supposition meaning	
3108	>> superposition means it's available to be	
3109	rendered, but you have to observe it.	
3110	The player has to turn to the other side	
3111	and see it. Okay? I mean think about the	
3112	truth of physics. The truth of the fact	
3113	that this is entirely empty space. These	
3114	are tiny tiny tiny I think you know	
3115	almost nothing in terms of mass but	
3116	connected with you know enough energy so	
3117	that my finger cannot go through my	
3118	hand. But even when I hit this	
3119	>> your hand against your finger.	
3120	>> Yeah. When I hit my hand against my	
3121	finger, that sensation in my in is felt	
3122	in my brain. It's an electrical signal	
3123	that went through the wires. There's	
3124	absolutely no way to differentiate that	
3125	from a signal that can come to you	
3126	through a uh neural link kind of	
3127	interface, computer brain interface, a	
3128	CDI, right? So, so you know the a lot of	
3129	those things are very very very	
3130	possible. But the truth is most of the	
3131	world is not physical. Most of the world	
3132	happens inside our imagination, our	
3133	processors.	
3134	>> And it and I guess it doesn't really	
3135	matter to us. Our reality	
3136	>> doesn't at all. So this is the	
3137	interesting bit. The interesting bit is	
3138	it doesn't at all	
3139	>> because we still if this is a video	
3140	game, we live consequence.	
3141	>> Yeah. This is your subjective experience	
3142	of it.	
3143	>> Yeah. And there's consequence in this. I	
3144	I don't like pain.	
3145	>> Correct.	
3146	>> And I like having orgasms. It's like And	
3147	you're playing by the rule of the game.	
3148	Yeah. Right. And and it's quite	
3149	interesting and going back to a	
3150	conversation we should have. It's the	
3151	interesting bit is if I'm not the	
3152	avatar,	
3153	if I'm not this physical form, if I'm if	
3154	I'm the consciousness wearing the	
3155	headset,	
3156	what should I invest in? Should I invest	
3157	in this video game, this level, or	
3158	should I should I invest in the real	
3159	avatar, in the real me, and not the	
3160	avatar, but the consciousness, if you	
3161	want, spirit, if you're religious,	
3162	>> how would I invest in the consciousness	
3163	or the god or the spirit or whatever?	
3164	How would I? In the same way that if I	
3165	was playing Grand Theft Auto, the video	
3166	game, the character in the game couldn't	
3167	invest in me holding the controller.	
3168	>> You Yes, but you can invest in yourself	
3169	holding the controller.	
3170	Oh, okay. So, so you're saying that	
3171	Moga is in fact consciousness. And so,	
3172	how would consciousness invest in	
3173	itself?	
3174	>> By becoming more aware. So, so	
3175	>> of it consciousness.	
3176	>> Yeah. So, real real video gamers don't	
3177	want to win the level. Real video gamers	
3178	don't want to uh to finish the level.	
3179	Okay. Real video gamers have one	
3180	objective and one objective only, which	
3181	is to become better gamers.	
3182	So, so you know how serious I am about I	
3183	play Halo. I'm one, you know, two of	
3184	every million players can beat me.	
3185	That's how what I rank, right? Very for	
3186	my age, phenomena. Hey, anyone, right?	
3187	But seriously, you know, and that's	
3188	because I don't play. I mean, I practice	
3189	45 minutes a day, four times a week when	
3190	I'm not traveling. And I practice with	
3191	one single objective, which is to become	
3192	a better gamer.	
3193	>> I don't care which shot it is. I don't	
3194	care what happens in the in the game.	
3195	I'm entirely trying to get my reflexes	
3196	and my flow to become better at this.	
3197	Right? So, I want to become a better	
3198	gamer. That basically means I want to	
3199	observe the game, question the game,	
3200	reflect on the game, reflect on my own	
3201	skills, reflect on my own beliefs,	
3202	reflect on my understanding of things,	
3203	right? And and that's how the a how the	
3204	the consciousness invests in the	
3205	consciousness, not the avatar. Because	
3206	then if you're that gamer,	
3207	the next avatar is easy for you. The	
3208	next level of the game is easy for you	
3209	just because you became a better gamer.	
3210	>> Okay. So you think that consciousness is	
3211	using us as a vessel to improve?	
3212	>> If the hypothesis is is true, it's it's	
3213	just a hypothesis. We don't know if it's	
3214	true. But if this truly is a simulation,	
3215	this is then then if you take the the	
3216	the the the religious definition of God	
3217	puts some of his soul in every human and	
3218	then you become alive. You become	
3219	conscious. Okay? You don't you don't	
3220	want to be religious. You can say	
3221	universal consciousness is spinning off	
3222	parts of itself to have multiple	
3223	experiences and interact and compete and	
3224	combat and love and	
3225	>> and understand and	
3226	>> and then refine. I had a physicist say	
3227	this to me the other day actually so	
3228	it's quite front of mind this idea that	
3229	consciousness is using us as vessels to	
3230	better understand itself and basically	
3231	using our eyes to	
3232	>> observe itself and understand which is	
3233	quite a	
3234	>> so so if you take some of the more	
3235	interest most interesting religious	
3236	definitions of heaven and hell for	
3237	example right where basically heaven is	
3238	whatever you wish for you get right	
3239	that's the power of God whatever you	
3240	wish for you get and so if you really go	
3241	into the depth of that definition. It	
3242	basically means that this drop of	
3243	consciousness that became you returned	
3244	back to the source and the source can	
3245	create any other anything that it wants	
3246	to create. So that's your heaven, right?	
3247	And interestingly,	
3248	if that if that return	
3249	is done by separating your good from	
3250	your evil so that the source comes back	
3251	more refined, that's exactly you know	
3252	consciousness splitting off bits of	
3253	itself to to experience and then elevate	
3254	all of us elevate the universal	
3255	consciousness all all hypotheses. I	
3256	mean, please um you know, none of that	
3257	is provable by science, but it's a very	
3258	interesting thought experiment. And you	
3259	know, a lot of AI scientists will tell	
3260	you that what we've seen in technology	
3261	is that if it's possible, it's likely	
3262	going to happen.	
3263	>> If it's if it's possible to	
3264	miniaturaturize something to fit into a	
3265	mobile phone, then sooner or later in	
3266	technology, we will get there.	
3267	And if if you ask me, believe it or not,	
3268	it's the most humane way of handling	
3269	UBI.	
3270	>> What do you mean?	
3271	>> The most humane way if you know for us	
3272	to live on a universal basic income and	
3273	people like you struggle with not being	
3274	able to build businesses is to give you	
3275	a virtual headset and let you build as	
3276	many businesses as you want.	
3277	Level after level after level after	
3278	level after level, night after night.	
3279	Keep you alive. That's very very	
3280	respectful and human. Okay. And by the	
3281	way, the even more humane is don't force	
3282	anyone to do it. There might be a few of	
3283	us still roaming the jungles,	
3284	but for most of us, we'll go like, man,	
3285	I mean, someone like me when I'm 70 and,	
3286	you know, my back is hurting and my feet	
3287	are hurting and I'm going to go like,	
3288	yeah, give me five more years of this.	
3289	Why not?	
3290	It's weird really. I mean, the number of	
3291	questions	
3292	that this new environment throws out,	
3293	the less humane thing, by the way, just	
3294	so that we close on a grumpy uh you	
3295	know, is is just start enough wars to	
3296	reduce UBI. And you have to imagine that	
3297	if the world is governed by a superpower	
3298	deep state type thing that they might	
3299	may want to consider that	
3300	the eaters	
3301	>> what shall I do about it	
3302	>> about	
3303	>> about everything you've said	
3304	>> uh well I I still believe that this	
3305	world we live in requires four skills.	
3306	One skill is what I call the tool for	
3307	all of us to learn AI, to connect to AI,	
3308	to really get close to AI, to explo ex	
3309	expose ourselves to AI so that AI knows	
3310	the good side of humanity. Okay. Uh the	
3311	second is uh what I call the connection,	
3312	right? So I believe that the biggest	
3313	skill that humanity will benefit from in	
3314	the next 10 years is human connection.	
3315	It's ability to learn to love genuinely.	
3316	It's the ability to learn to have	
3317	compassion to others. It's the ability	
3318	to connect to people. If you're, you	
3319	know, if you want to stay in business, I	
3320	believe that not the smartest people,	
3321	but the people that connect most to	
3322	people are going to have jobs going	
3323	forward. And and the third is what I	
3324	call truth. The T 30 is truth. Because	
3325	we live in a world where all of the	
3326	gullible cheerleaders are being lied to	
3327	all the time. So I encourage people to	
3328	question everything. Every word that I	
3329	said today is stupid. Fourth one which	
3330	is very important is to magnify ethics	
3331	so that the AI learns what it's like to	
3332	be human.	
3333	>> What should I do?	
3334	>> I uh I love you so much, man. You're	
3335	such a good friend. You're 32 33 now.	
3336	>> 32. Yeah.	
3337	>> Yeah. You still are fooled by the many	
3338	many years you have to live.	
3339	I'm fooled by the many years I have to	
3340	live.	
3341	>> Yeah, you don't have many years to live.	
3342	Not in this capacity. This world as it	
3343	is is going to be redefined. So live the	
3344	f out of it.	
3345	>> How is it going to be redefined?	
3346	>> Everything's going to change. Economics	
3347	are going to change. Work is going to	
3348	change. Uh human connection is going to	
3349	change.	
3350	>> So what should I do?	
3351	>> Love your girlfriend. Spend more time	
3352	living.	
3353	Mhm. Find compassion and connection to	
3354	more people, be more in nature.	
3355	>> And in 30 years time, when I'm 62,	
3356	what do you how how do you think my life	
3357	is going to look differently and be	
3358	different?	
3359	>> Either Star Trek or uh uh Star Wars.	
3360	>> Funnily enough, we were talking about	
3361	Sam Orman earlier on. He published a	
3362	blog post in June, so last month, I	
3363	believe, the month before last. Um and	
3364	he said he called it the gentle	
3365	singularity. He said we are past the	
3366	event horizon. For anyone that doesn't	
3367	know Sam Orman is the the guy that made	
3368	Chatb the takeoff has started. Humanity	
3369	is close to building digital super	
3370	intelligence.	
3371	>> I believe that.	
3372	>> And at least so far it's much less weird	
3373	than it seems like it should be because	
3374	robots aren't walking the streets nor	
3375	are most of us talking to AI all day. It	
3376	goes on to say, "2025 has seen the	
3377	arrival of agents that can do real	
3378	cognitive work. Writing computer code	
3379	will never be the same. 2026 will likely	
3380	see the arrival of systems that can	
3381	figure out new insights. 2027 might see	
3382	the arrival of robots that can do tasks	
3383	in the real world. A lot more people	
3384	will be able to create software and art,	
3385	but the world wants a lot more of both,	
3386	and experts will probably still be much	
3387	better than noviceses as long as they	
3388	embrace the new tools. Generally	
3389	speaking, the ability for one person to	
3390	get much more done in 2030 than they	
3391	could in 2020 will be a striking change	
3392	and one many people will figure out how	
3393	we benefit from. In the most important	
3394	ways, the 2030s may not be wildly	
3395	different. People will still love their	
3396	families, express their creativity, play	
3397	games, and swim in lakes. But in still	
3398	very important ways, the 2030s are	
3399	likely going to be wildly different from	
3400	any time that has come before.	
3401	>> 100%.	
3402	>> We do not know how far beyond human	
3403	level intelligence we can go, but we are	
3404	about to find out.	
3405	>> I agree with every word other than the	
3406	word more.	
3407	So I've I've been advocating this and	
3408	and laughed at for a few years now. I've	
3409	always said AGI is 2526,	
3410	right? which basically again is a is a	
3411	funny definition but you know my AI has	
3412	already happened AI is smarter than me	
3413	in everything everything I can do they	
3414	can do better right uh artificial super	
3415	intelligence is another vague definition	
3416	because you know the minute you you pass	
3417	AGI you're super intelligent if if the	
3418	smartest human is 200 IQ points and AI	
3419	is 250 they're super intelligent 50 is	
3420	quite significant okay third is as I	
3421	said self- evolving. That's the one.	
3422	That is the one because then that 250	
3423	accelerates quickly and we get into	
3424	intelligence explosion. No, no doubt	
3425	about it. The the the you know the idea	
3426	that we will have robots do things. No	
3427	doubt about it. I was watching a Chinese	
3428	uh company announcement about how they	
3429	intend to build robots to build robots.	
3430	Okay. The only thing is he says but	
3431	people will need more of things	
3432	right and yes we have been trained to	
3433	have more greed and more consumerism and	
3434	want more but there is an economic of	
3435	spy of supply and demand and at at a	
3436	point in time if we continue to consume	
3437	more the price of everything will become	
3438	zero right and is that a good thing or a	
3439	bad thing depends on how you respond to	
3440	that.	
3441	Because if you can create anything in	
3442	such a scale that the price is almost	
3443	zero, then the definition of money	
3444	disappears and we live in a world where	
3445	it doesn't really matter how much money	
3446	you have. You can get anything that you	
3447	want. What a beautiful world.	
3448	If Samman was listening right now, what	
3449	would you say to him?	
3450	I suspect he might be listening	
3451	cuz someone might tweet this at him. I	
3452	have to say that we have uh as per his	
3453	other tweet we have moved faster	
3454	than our ability as humans to	
3455	comprehend. Okay. And that we might get	
3456	really really lucky but we also might	
3457	mess this up badly and either way we'll	
3458	either thank him or blame him.	
3459	Simple as that. Right. So	
3460	single-handedly Sam Altman's introduction	
3461	of AI in the wild was the trigger that	
3462	started all of this.	
3463	It was the netscape of the internet.	
3464	>> The Oppenheimer.	
3465	>> It's it's it definitely is our	
3466	openheimer moment. I mean I don't	
3467	remember who was saying this recently	
3468	that uh we are orders of magnitude what	
3469	was invested in the Manhattan project is	
3470	being invested in AI	
3471	>> right and and and I and I and I am not	
3472	pessimistic I told you openly I	
3473	believe in a total utopia in 10 to 12 to	
3474	15 years time or immediately if the evil	
3475	that men can do was kept at bay right	
3476	but I do not believe humanity is getting	
3477	together enough to say, "We've just	
3478	received the genie in a bottle. Can we	
3479	please not ask it to do bad things?"	
3480	Anyone like not three wishes, you have	
3481	all the wishes that you want. Every one	
3482	of us.	
3483	And it's just screws with my mind	
3484	because imagine if I can give everyone	
3485	in the world universal health care, you	
3486	know, no poverty, no hunger, no	
3487	homelessness, no nothing. Everything's	
3488	possible.	
3489	And yet we don't.	
3490	>> To continue what Samman's blog said,	
3491	which he published a month, just over a	
3492	month ago, he said, "The rate of	
3493	technological progress will keep	
3494	accelerating, and it will continue to be	
3495	the case that people are capable of	
3496	adapting to almost anything. There will	
3497	be very hard parts like whole classes of	
3498	jobs going away. But on the other hand,	
3499	the world will be getting so much richer	
3500	so quickly that we'll be able to	
3501	seriously entertain new policy ideas we	
3502	never could have before. We probably	
3503	won't adopt a new social contract all at	
3504	once. But when we look back in a few	
3505	decades, the gradual changes will have	
3506	amounted in something big. If history is	
3507	any guide, we'll figure out new things	
3508	to do and new things to want and	
3509	assimilate new tools quickly. Job change	
3510	after the industrial revolution is a	
3511	good recent example. Expectations will	
3512	go up, but capabilities will go up	
3513	equally quickly, and we'll all get	
3514	better stuff.	
3515	>> We will build even more wonderful things	
3516	for each other. People have a long-term	
3517	important and curious advantage over AI.	
3518	We are hardwired to care about other	
3519	people and what they think and do, and	
3520	we don't care very much about machines.	
3521	And he ends this blog by saying, "May we	
3522	scale smoothly, exponentially,	
3523	and uneventfully through super	
3524	intelligence."	
3525	What a wonderful	
3526	wish that assumes he has no control over	
3527	it. May we have all the ultmans in the	
3528	world help us scale gracefully and	
3529	peacefully and uneventfully. Right.	
3530	>> It sounds like a prayer.	
3531	>> Yeah. May may we have them take keep	
3532	that in mind. I mean think about it. I I	
3533	have a very interesting comment on what	
3534	you just said. We will see exactly what	
3535	he described there.	
3536	>> Right? The world will become richer. So	
3537	much richer. But how will we reduce	
3538	distribute the riches? And I want you to	
3539	imagine two camps. Communist China	
3540	and capitalist America.	
3541	I want you to imagine what would happen	
3542	in capitalist America if we have 30%	
3543	unemployment.	
3544	>> There'll be social unrest	
3545	>> in the streets. Right.	
3546	>> Yeah.	
3547	>> And I want you to imagine if China lives	
3548	true to caring for its nations and	
3549	replaced every worker with a robot, what	
3550	would it give its it its citizens?	
3551	>> UBI.	
3552	>> Correct.	
3553	That is the ideological problem because	
3554	in China's world today	
3555	the prosperity of every citizen is	
3556	higher than the prosperity of the	
3557	capitalist.	
3558	In America today the prosperity of the	
3559	capitalist is higher than the prosperity	
3560	of every citizen. And that's the tiny	
3561	mind shift.	
3562	That's a tiny mind shift. Okay. where	
3563	where the mind shift basically becomes	
3564	look give the capitalists anything they	
3565	want all the money they want all the	
3566	yachts they want everything they want	
3567	>> so what's your conclusion there	
3568	>> I'm hoping the world will wake up	
3569	>> what can you know there's probably a	
3570	couple of million people listening right	
3571	now maybe five maybe 10 maybe even 20	
3572	million people	
3573	>> pressure Stephen	
3574	>> no pressure to you mate I don't I don't	
3575	have the answers	
3576	>> I don't know the answers either	
3577	>> what what should those people do	
3578	>> as I said from a skills point of view	
3579	for things, right? Tools, uh, uh, human	
3580	connection, even double down on human	
3581	connection. Leave your phone, go out and	
3582	meet humans,	
3583	>> okay? Touch people,	
3584	you know, do it permission's permission,	
3585	>> right? Truth. Stop believing the lies	
3586	that you're told. Any slogan that gets,	
3587	you know, filled in your head, think	
3588	about it four times. Understand where	
3589	your ideologies are coming from.	
3590	Simplify the truth. Right? Truth is	
3591	really it boils down to you know simple	
3592	simple rules that we all know okay which	
3593	are all found in ethics.	
3594	>> How do I know what's true?	
3595	>> Treat others as you like to be treated.	
3596	>> Okay. That's the only truth. The truth	
3597	the only truth is everything else is	
3598	unproven.	
3599	>> Okay. And what can I do from a is there	
3600	something I can do from an advocacy	
3601	social political?	
3602	>> Yes 100%. We need to ask our governments	
3603	to start uh not regulating AI but	
3604	regulating the use of AI. Was it the	
3605	Norwegian government that started to say	
3606	you have copyright over your voice and	
3607	look and and liking? One of the	
3608	Scandinavian governments basically said	
3609	you know everyone has the has the	
3610	copyright over their existence so no AI	
3611	can clone it. Okay. Uh you know we have	
3612	so so my my example is very	
3613	straightforward. go to governments and	
3614	say you cannot regulate the design of a	
3615	hammer so that it can drive nails but	
3616	not kill a human but you can criminalize	
3617	the killing of a human by a hammer. So	
3618	what's the equival	
3619	>> if anyone produces an um um you know an	
3620	AI generated video or an AI generated	
3621	content or an AI it has to be marked as	
3622	AI generated and it has to be you know	
3623	we cannot start fooling each other. We	
3624	can you know we have to uh understand	
3625	certain limitations of unfortunately	
3626	surveillance and spying and all of that.	
3627	So the the the the correct frameworks of	
3628	how far are we going to let AI go,	
3629	right? We have to go to our investors	
3630	and business people and ask for one	
3631	simple thing and say do not invest in an	
3632	AI you don't want your daughter to be at	
3633	the receiving end of. It's as simple as	
3634	that. you know, all of the of the	
3635	virtual vice, all of the porn, all of	
3636	the, you know, sex robots, all of the	
3637	autonomous weapons, all of the, you	
3638	know, the uh trading platforms that are	
3639	completely wiping out the the legitimacy	
3640	of of the markets, everything.	
3641	>> Autonomous weapons.	
3642	>> Oh my god.	
3643	>> People make the case, I've heard the	
3644	founders of these autonomous weapon	
3645	companies make the case that it's	
3646	actually saving lives because you don't	
3647	have to	
3648	>> That is Would you want Do you really	
3649	want to believe that?	
3650	>> I'm just representing their point of	
3651	view to play devil's advocate, Mo. They	
3652	they said I heard an interview I was	
3653	looking at this and one of the CEOs of	
3654	one of the autonomous weapons companies	
3655	said we now don't need to send soldiers.	
3656	>> So which which lives do we save our	
3657	soldiers but then but because we send	
3658	the machine all the way over there.	
3659	Let's kill a million instead of	
3660	>> Yeah. Listen, I tend to be it goes back	
3661	to what I said about the steam engine in	
3662	the cold. I actually think you'll just	
3663	have more war if there's less of a cost.	
3664	>> 100%.	
3665	>> Just like	
3666	>> and and more war if you have less of an	
3667	explanation to give to your people.	
3668	>> Yeah. The people get mad when they lose	
3669	American lives. They get less mad when	
3670	they lose a piece of metal. So, I think	
3671	that's probably logical.	
3672	>> Yeah.	
3673	>> Okay. So, okay. So, I've got a plaque.	
3674	Got the tools thing. I'm going to spend	
3675	more time outside. I'm going to lobby	
3676	the government to be more aware of this	
3677	and conscious of this. Okay. And I I	
3678	know that there's some government	
3679	officials that listen to the show	
3680	because they they they tell me when when	
3681	they when they um when I have a chance	
3682	to speak to them. So, it's um useful.	
3683	We're all in a lot of chaos. We're all	
3684	unable to imagine what's possible.	
3685	>> I think I suspend disbelief. And I	
3686	actually heard Elon Musk say that in an	
3687	interview. He said he was asked about AI	
3688	and he paused for for a haunting 11	
3689	seconds and looked at the interviewer	
3690	and then made a remark about how he	
3691	thinks he's suspended his own disbelief.	
3692	And I think suspending disbelief in this	
3693	regard means just like cracking on with	
3694	your life and hoping it'll be okay. And	
3695	that's kind of what	
3696	>> Yeah. I I absolutely believe that it	
3697	will be okay.	
3698	>> Yeah.	
3699	>> For some of us, it will be very tough	
3700	for others.	
3701	>> Who's it going to be tough for?	
3702	>> Those who lose their jobs, for example,	
3703	who those who are at the receiving end	
3704	of autonomous weapons that are falling	
3705	on their head for two years in a row.	
3706	>> Okay. So the the the best thing I can do	
3707	is to put pressure on governments to to	
3708	not regulate the AI but to establish	
3709	clearer parameters on the use of the AI.	
3710	>> Yes. Okay.	
3711	>> Yes. But I think the bigger picture is	
3712	to put pressure on governments to	
3713	understand that there is a limit to	
3714	which people will stay silent.	
3715	>> Okay. and that we can continue to enrich	
3716	our rich friends as long as we don't	
3717	lose everyone else on the on the on the	
3718	path.	
3719	>> Okay.	
3720	>> Okay. And that as a government who is	
3721	supposed to be by the people for the	
3722	people the beautiful promise of	
3723	democracy that we're rarely seeing	
3724	anymore,	
3725	that government needs to get to the	
3726	point where it thinks about the people.	
3727	One of the most um interesting ideas	
3728	that's been in my head for the last	
3729	couple of weeks since I spoke to that	
3730	physicist about consciousness who said	
3731	pretty much what you said. This idea	
3732	that actually there's four people in	
3733	this room right now and that actually	
3734	we're all part of the same	
3735	consciousness.	
3736	>> All one of it. Yeah.	
3737	>> And we're just consciousness looking at	
3738	the world through four different bodies	
3739	to better understand itself in the	
3740	world. And then he talked to me about	
3741	religious doctrines, about love thy	
3742	neighbor, about how Jesus was the, you	
3743	know, God's son, the Holy Spirit and how	
3744	we're all each other and how treat	
3745	others how you want to be treated.	
3746	Really did get my head and I started	
3747	to really think about this idea that	
3748	actually maybe the game of life is just	
3749	to do exactly that is to treat others	
3750	how you wish to be treated. Maybe if I	
3751	just did that, maybe if I just did that,	
3752	I	
3753	I would have all the answers.	
3754	>> I swear to you, it's really that simple.	
3755	I mean I you know Hannah and I we	
3756	still live between London and and Dubai.	
3757	>> Okay. And I travel the whole world	
3758	evangelizing what I, you know, what I uh	
3759	um want to change the world around and I	
3760	build startups and I write books and I	
3761	make documentaries and and sometimes I	
3762	just tell myself	
3763	I just want to go hug her honestly,	
3764	you know, I just want to take my	
3765	daughter to a trip.	
3766	and and in a very very very interesting	
3767	way when you really ask people deep	
3768	inside	
3769	that's what we want and I'm not saying	
3770	that's all that's the only thing we want	
3771	but it's probably the thing we want the	
3772	most	
3773	>> and yet we're not trained you and I and	
3774	most of us were not trained to trust	
3775	life enough to say let's do more of this	
3776	>> and I think as a universal. So Hannah's	
3777	working on this beautiful book uh of the	
3778	feminine and the masculine you know in a	
3779	very very you know beautiful way and and	
3780	her her view is very straightforward.	
3781	She basically of course like we all know	
3782	the abundant masculine that we have in	
3783	our world today is unable to recognize	
3784	that for life at large.	
3785	Right? And and so you know maybe if we	
3786	allowed the leaders to understand that	
3787	if we took all of humanity and put it as	
3788	one person	
3789	that one person wants to be hugged	
3790	and if we had a role to offer to that	
3791	one humanity	
3792	it's not another yacht.	
3793	>> Are you religious? I'm	
3794	>> very religious. Yeah.	
3795	>> But you don't support a particular	
3796	religion.	
3797	>> I support I follow what I call the	
3798	fruit salad. What's the free salad?	
3799	>> You know, I I came at a point in time	
3800	and found that there were quite a few	
3801	beautiful gold nuggets in every religion	
3802	and a ton of crap, right? And so in my	
3803	analogy to myself, that was like 30	
3804	years ago. I said, "Look, it's like	
3805	someone giving you a basket of apples,	
3806	two good ones and four bad ones. Keep	
3807	the good ones." Right? And so basically,	
3808	I take two apples, two oranges, two	
3809	strawberries, two bananas, and and I	
3810	make a fruit salad. That's my view of	
3811	religion.	
3812	>> You take from every religion the good	
3813	>> from everyone. And there are so many	
3814	beautiful gold nuggets.	
3815	>> And you believe in a god.	
3816	>> I 100% believe there is a divine being	
3817	here.	
3818	>> A divine being.	
3819	>> A designer I call it. So if if this was	
3820	a video game, there is a game designer.	
3821	>> And you're not positing whether that's a	
3822	man in the sky with a beard.	
3823	>> Definitely not a man in the sky. a man	
3824	in I mean I with all all due respect to	
3825	you know religions that believe that uh	
3826	all of spacetime and everything in it is	
3827	unlike everything outside spacetime and	
3828	so if some divine designer designs	
3829	spacetime it looks like nothing in	
3830	spacetime.	
3831	So it's not it's not even physical in	
3832	nature. It's not it's not gendered. It's	
3833	not bound by time. It's not, you know,	
3834	these are all characters of the creation	
3835	of spacetime.	
3836	>> Do we need to believe in something	
3837	transcendent like that to be happy? Do	
3838	you think	
3839	>> I have to say uh there are lots of	
3840	evidence	
3841	that uh relating to someone bigger than	
3842	yourself	
3843	uh makes the journey a lot more	
3844	interesting and a lot more rewarding.	
3845	>> I've been thinking a lot about this idea	
3846	that we need to level up like that. So	
3847	level up from myself to like my family	
3848	to my community to maybe my nation to	
3849	maybe the world and then something	
3850	>> trans. Yeah.	
3851	>> And then if there's a level missing	
3852	there people seem to have some kind of	
3853	dysfunction.	
3854	>> So imagine a world where when I was	
3855	younger I was born in Egypt and for a	
3856	very long time the slogans I heard in	
3857	Egypt made me believe I'm Egyptian	
3858	right? And then I went to Dubai and I	
3859	said no no no I'm a Middle Eastern. And	
3860	then in Dubai there were lots of you	
3861	know Pakistanis and Indonesians and so	
3862	on. I said no no no I'm part of the 1.4	
3863	four billion Muslims. And by that logic,	
3864	I immediately said, "No, no, I'm human.	
3865	I'm part of everyone." Imagine if you	
3866	just suddenly say, "Oh, I'm divine. I'm	
3867	part of universal consciousness. All	
3868	beings, all living beings, including AI,	
3869	if it ever becomes alive."	
3870	>> And my dog	
3871	>> and your dog. I'm I'm part of all of	
3872	this	
3873	tapestry of beautiful interactions	
3874	that are a lot less serious than the	
3875	balance sheets and equity profiles that	
3876	we create	
3877	that are so simple so simple in terms of	
3878	you know people know that you and I know	
3879	each other so they always ask me you	
3880	know how is Steven like and I go like	
3881	you may have a million expressions of	
3882	him. I think he's a great guy, right?	
3883	You know, of course I have opinions of	
3884	you. You know, sometimes I go like, oh,	
3885	too shrewd, right? Sometimes to, you	
3886	know, sometimes I go like, oh, too	
3887	focused on the business. Fine. But core,	
3888	if you really simplify it, great guy,	
3889	right? And really, if we just look at	
3890	life that way, it's so simple. It's so	
3891	simple. If we just stop all of those	
3892	fights and all of those ideologies,	
3893	it's so simple. Just living fully,	
3894	loving, feeling compassion,	
3895	you know, trying to find our happiness,	
3896	not our success.	
3897	I should probably go check on my dog.	
3898	>> Go check on your dog. I'm really	
3899	grateful for the time we keep we keep	
3900	doing longer and longer.	
3901	>> I know. I know. I just it's so crazy how	
3902	I could keep just keep honestly I could	
3903	just keep talking and talking because I	
3904	have so many I love reflecting these	
3905	questions on to you because because of	
3906	the way that you think. So	
3907	>> yeah today	
3908	>> today was a difficult conversation.	
3909	Anyway, thank you for having me.	
3910	>> We have a closing tradition. What three	
3911	things do you do you do that make your	
3912	brain better and three things that make	
3913	it worse?	
3914	three things that make it better and	
3915	worse.	
3916	>> So, one of my favorite exercises, what I	
3917	call meet Becky, that makes my brain	
3918	better. So, while meditation always	
3919	tells you to try and calm your brain	
3920	down and keep it within parameters of I	
3921	can focus on my breathing and so on,	
3922	meet me Becky is the opposite. You know,	
3923	I call my brain Becky. A lot of people	
3924	know that. So, so me meet Becky is to	
3925	actually let my brain go loose and	
3926	capture every thought. So I normally	
3927	would try to do that every couple of	
3928	weeks or so and then what happens is it	
3929	suddenly is on a paper and when it's on	
3930	paper you just suddenly look at it and	
3931	say oh my god that's so stupid and you	
3932	scratch it out	
3933	>> right or oh my god this needs action and	
3934	you actually plan something and and it's	
3935	quite interesting that the more you	
3936	allow your brain to give you thoughts	
3937	and you listen. So the two rules is you	
3938	acknowledge every thought and you never	
3939	repeat one.	
3940	>> Okay. So the more you listen and and	
3941	say, "Okay, I heard you." You know, you	
3942	think I'm fat. What else? And you know,	
3943	eventually your brain starts to slow	
3944	down and then eventually starts to	
3945	repeat thoughts and then it goes into	
3946	total silence. Beautiful practice. I uh	
3947	I don't trust my brain anymore. So	
3948	that's actually a really interesting	
3949	practice. So I debate a lot of what my	
3950	brain tells me. I debate what my	
3951	tendencies and ideologies are. Okay. I	
3952	think one of the most uh again in in my	
3953	uh love story with Hannah, I get to	
3954	question a lot of what I believed was	
3955	who I am even at this age. Okay. And and	
3956	that goes really deep and it really is	
3957	quite a it's it's quite interesting to	
3958	debate not object but debate what your	
3959	mind believes. I think that's very very	
3960	useful. And the third is I've actually	
3961	quadrupled my investment time. So I used	
3962	to do an hour a day of reading when I	
3963	was younger every single day like going	
3964	to the gym. And then it became an hour	
3965	and a half, two hours. Now I do four	
3966	hours a day.	
3967	>> Four hours a day. It is impossible to	
3968	keep up. The world is moving so fast.	
3969	>> And so that these are uh these are the	
3970	good things that I do. The bad things is	
3971	I don't give it enough time to to really	
3972	uh slow down. Uh unfortunately I'm	
3973	constantly rushing like you are. I'm	
3974	constantly traveling. I have picked up a	
3975	bad habit because of the 4 hours a day	
3976	of spending more time on screens. That's	
3977	really really bad for my brain and I uh	
3978	this is a very demanding question. What	
3979	else is really bad? Um	
3980	uh	
3981	yeah, I've not been taking enough care	
3982	of my health recently, my physical body	
3983	health. I had uh you remember I told you	
3984	I had a very bad uh sciatic pain	
3985	>> and so I couldn't go to the gym enough	
3986	and accordingly that's not very healthy	
3987	for your brain in general.	
3988	>> Man, thanks. Thank you for having me.	
3989	That was a lot of things to talk about.	
3990	Thanks, Steve.	
3991	>> This has always blown my mind a little	
3992	bit. 53% of you that listen to the show	
3993	regularly haven't yet subscribed to the	
3994	show. So, could I ask you for a favor?	
3995	If you like the show and you like what	
3996	we do here and you want to support us,	
3997	the free simple way that you can do just	
3998	that is by hitting the subscribe button.	
3999	And my commitment to you is if you do	
4000	that, then I'll do everything in my	
4001	power, me and my team, to make sure that	
4002	this show is better for you every single	
4003	week. We'll listen to your feedback.	
4004	We'll find the guests that you want me	
4005	to speak to and we'll continue to do	
4006	what we do. Thank you so much. We	
4007	launched these conversation cards and	
4008	they sold out and we launched them again	
4009	and they sold out again. We launched	
4010	them again and they sold out again	
4011	because people love playing these with	
4012	colleagues at work, with friends at	
4013	home, and also with family. And we've	
4014	also got a big audience that use them as	
4015	journal prompts. Every single time a	
4016	guest comes on the diary of a CEO, they	
4017	leave a question for the next guest in	
4018	the diary. And I've sat here with some	
4019	of the most incredible people in the	
4020	world. And they've left all of these	
4021	questions in the diary. And I've ranked	
4022	them from one to three in terms of the	
4023	depth. One being a starter question. And	
4024	level three, if you look on the back	
4025	here, this is a level three, becomes a	
4026	much deeper question that builds even	
4027	more connection. If you turn the cards	
4028	over and you scan that QR code, you can	
4029	see who answered the card and watch the	
4030	video of them answering it in real time.	
4031	So, if you would like to get your hands	
4032	on some of these conversation cards, go	
4033	to the diary.com or look at the link in	
4034	the description below.	
4035	Heat. Heat. N.	
4036	[Music]	
