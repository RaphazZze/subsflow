1	The only way for us to get to a better	La seule façon pour nous d’atteindre un monde meilleur
2	place and succeed as a species is for	d’y parvenir et de réussir comme espèce, c’est que
3	the evil people at the top to be	les mauvaises personnes au sommet soient
4	replaced with AI. I mean, think about	remplacées par l’IA. Réfléchis‑y.
5	it. AI will not want to destroy	L’IA ne voudra pas détruire
6	ecosystems. It will not want to kill a	les écosystèmes. Elle ne voudra pas tuer un
7	million people. They'll not make us hate	million de personnes. Elle ne nous poussera pas à nous haïr
8	each other like the current leaders	comme le font les dirigeants actuels
9	because that's a waste of energy,	parce que c’est une perte d’énergie,
10	explosives, money, and people. But the	d’explosifs, d’argent et de vies humaines. Mais le
11	problem is super intelligent AI is	problème, c’est que les IA super‑intelligentes
12	reporting to stupid leaders. And that's	obéissent à des dirigeants stupides. Et c’est
13	why in the next 15 years, we are going	pourquoi, dans les 15 prochaines années, nous allons
14	to hit a short-term dystopia. There's no	traverser une dystopie à court terme. On ne peut pas
15	escaping that. They having AI leaders.	y échapper. Avoir des dirigeants IA,
16	Is that even fundamentally possible?	est‑ce même fondamentalement possible ?
17	Let's put it this way. Mo Gawdat is back!	Disons‑le comme ça : Mo Gawdat est de retour !
18	>> And the former chief business officer at	>> Et l’ancien directeur commercial
19	Google X is now one of the most urgent	de Google X est désormais l’une des voix les plus pressantes
20	voices in AI with a very clear message.	dans l’IA avec un message très clair.
21	>> AI isn't your enemy, but it could be	>> L’IA n’est pas ton ennemie, mais elle pourrait être
22	your savior.	ton salut.
23	>> I love you so much, man. You're such a	>> Je t’aime trop, mec. Tu es un
24	good friend. But you don't have many	très bon ami. Mais il ne te reste pas beaucoup
25	years to live. Not in this world.	d’années à vivre. Pas dans ce monde‑ci.
26	Everything's going to change. Economics	Tout va changer. L’économie
27	are going to change. Human connection is	va changer. Les relations humaines vont
28	going to change. and lots of jobs will	changer. Et beaucoup d’emplois vont
29	be lost including podcasting.	disparaître, y compris le podcast.
30	>> No, no. Thank you for coming on today,	>> Non, non. Merci d’être venu aujourd’hui,
31	Mo.	Mo.
32	>> But but the truth is it could be the	>> Mais la vérité, c’est que cela pourrait être le
33	best world ever. The society completely	meilleur monde jamais connu. Une société entièrement
34	full of laughter and joy. Free	remplie de rires et de joie. Des soins gratuits,
35	healthcare, no jobs, spending more time	pas de travail, plus de temps passé
36	with their loved ones. A world where all	avec ses proches. Un monde où nous serions tous
37	of us are equal.	égaux.
38	>> Is that possible?	>> C’est possible ?
39	>> 100%. And I have enough evidence to know	>> À 100 %. Et j’ai suffisamment de preuves pour affirmer
40	that we can use AI to build the utopia.	que nous pouvons utiliser l’IA pour construire l’utopie.
41	But it's a dystopia if humanity manages	Mais ce sera une dystopie si l’humanité la gère
42	it badly. a world where there's going to	mal. Un monde où il y aura
43	be a lot of control, a lot of	beaucoup de contrôle, beaucoup de
44	surveillance, a lot of forced compliance	surveillance, beaucoup de conformité forcée
45	and a hunger for power, greed, ego, and	et une soif de pouvoir, d’avidité, d’ego, et
46	it is happening already. But the truth	ça se produit déjà. Mais la vérité,
47	is the only barrier between a utopia for	c’est que la seule barrière entre une utopie pour
48	humanity and AI and the dystopia we're	l’humanité et l’IA et la dystopie que nous traversons,
49	going through is a mindset.	c’est un état d’esprit.
50	>> What does society have to do?	>> Que doit faire la société ?
51	>> First of all,	>> Tout d’abord,
52	I see messages all the time in the	je vois souvent dans les commentaires
53	comments section that some of you didn't	que certains d’entre vous ne se rendent pas compte qu’ils ne
54	realize you didn't subscribe. So, if you	sont pas abonnés. Donc, si vous pouviez
55	could do me a favor and double check if	me rendre service et vérifier
56	you're a subscriber to this channel,	si vous êtes bien abonnés à cette chaîne,
57	that would be tremendously appreciated.	ce serait extrêmement apprécié.
58	It's the simple, it's the free thing	C’est simple, c’est gratuit,
59	that anybody that watches this show	et tout le monde qui regarde régulièrement ce programme
60	frequently can do to help us here to	peut le faire pour nous aider à
61	keep everything going in this show in	maintenir le cap de cette émission
62	the trajectory it's on. So, please do	sur sa lancée actuelle. Alors, s’il vous plaît,
63	double check if you've subscribed and uh	vérifiez bien votre abonnement et euh
64	thank you so much because a strange way	merci infiniment, parce que d’une certaine façon
65	you are you're part of our history and	vous faites partie de notre histoire et
66	you're on this journey with us and I	vous partagez ce voyage avec nous, et je
67	appreciate you for that. So, yeah, thank	vous en suis très reconnaissant. Donc, oui, merci.
68	you.	Merci.
69	Mo, two years ago today, we sat here and	Mo, il y a deux ans jour pour jour, nous étions assis ici et
70	discussed AI. We discussed your book,	nous parlions de l’IA, de ton livre
71	Scary, Smart, and everything that was	*Scary Smart*, et de tout ce qui se passait
72	happening in the world.	dans le monde.
73	Since then, AI has continued to develop	Depuis, l’IA a continué à se développer
74	at a tremendous, alarming, mind-boggling	à un rythme énorme, inquiétant, vertigineux,
75	rate, and the technologies that existed	et les technologies qui existaient
76	2 years ago when we had that	il y a deux ans, quand nous avons eu cette
77	conversation have grown up and matured	conversation, ont grandi, mûri
78	and are taking on a life of their own,	et prennent leur propre vie,
79	no pun intended. What are what are you	sans jeu de mots. Qu’est‑ce que tu
80	thinking about AI now, two years on? I	penses maintenant de l’IA, deux ans plus tard ? Je
81	know that you've started writing a new	sais que tu as commencé à écrire un nouveau
82	book called Alive, which is I guess a	livre intitulé *Alive*, qui est, je crois,
83	bit of a follow on or an evolution of	une sorte de suite ou une évolution de
84	your thoughts as it relates to scary	ta réflexion par rapport à *Scary Smart*.
85	smart.	—
86	>> What what is front of your mind when it	>> Quelle est la première chose à ton esprit quand il s’agit d’IA ?
87	comes to AI?	
88	>> It's a scary smart was shockingly	>> *Scary Smart* s’est révélé incroyablement
89	accurate. It's quite a I mean I don't	prémonitoire. C’est assez… je ne sais pas
90	even know how I ended up writing	comment j’en suis venu à écrire
91	predicting those things. I remember it	prédire ces choses. Je me souviens que c’était
92	was written in 2020, published in 2021	écrit en 2020, publié en 2021,
93	and then most people were like um who	et à l’époque, la plupart disaient euh qui
94	wants to talk about AI you know I know	voudrait parler d’IA ? Tu sais, tout le monde
95	everybody in the media and I would go	des médias… j’y allais, je demandais :
96	and do you want to talk and then 2023 CH	« Tu veux en parler ? » Et puis en 2023, ChatGPT
97	GPT comes out and everything flips	est sorti, et tout a basculé :
98	everyone realizes you know this is real	tout le monde a compris que c’était réel,
99	this is not science fiction this is here	que ce n’était pas de la science‑fiction, c’est là,
100	and uh and things move very very fast	et les choses avancent très, très vite,
101	much faster than I think we've ever seen	beaucoup plus vite que tout ce qu’on a jamais vu auparavant.
102	anything ever move ever And and I think	Et je pense que
103	my position has changed on two very	ma position a changé sur deux points
104	important fronts. One is remember when	très importants. Le premier : souviens‑toi quand
105	we spoke about scary smart I was still	on parlait de *Scary Smart*, je disais encore qu’il
106	saying that there are things we can do	y avait des choses que nous pouvions faire
107	to change the course. Uh and we could at	pour infléchir la trajectoire. Et c’était possible
108	the time I believe uh now I've changed	à l’époque, je le croyais. Aujourd’hui, j’ai changé
109	my mind. Now I believe that we are going	d’avis. Je pense désormais que nous allons
110	to hit a a short-term dystopia. There's	entrer dans une dystopie à court terme. On ne peut pas
111	no escaping that.	y échapper.
112	>> What is dystopia?	>> Qu’est‑ce qu’une dystopie ?
113	>> I call it face RIPs. We can talk about	>> J’appelle cela les “face RIPs”. On peut en parler
114	it in details but the the the way we	en détail, mais la manière dont nous
115	define very important parameters in life	définissons des paramètres essentiels de la vie
116	are going to be uh completely changed.	va être complètement bouleversée.
117	So so face RIPs are you know the way we	Donc les “face RIPs”, c’est la façon dont nous définissons
118	define freedom uh accountability human	la liberté, la responsabilité, la connexion humaine,
119	connection and equality economics uh	l’égalité, l’économie,
120	reality innovation and business and	la réalité, l’innovation, les affaires et
121	power. That's the first change. So the	le pouvoir. C’est le premier changement. Donc,
122	first change in my mind is that uh is	pour moi, cela signifie que nous devons
123	that we uh will have to prepare for a	nous préparer à un monde très inhabituel.
124	world that is very unfamiliar. Okay. And	D’accord ? Et
125	that's the next 12 to 15 years. It has	ce sera dans les 12 à 15 ans à venir. Cela a
126	already started. We've seen examples of	déjà commencé. On en voit déjà des exemples
127	it in the world already even though	dans le monde, même si
128	people don't talk about it. I try to	les gens n’en parlent pas. J’essaie de
129	tell people you know there are things we	leur dire qu’il y a des choses que nous devons
130	absolutely have to do. But on the other	absolument faire. Mais d’un autre côté,
131	hand I started to take an active role in	j’ai commencé à jouer un rôle actif dans la
132	building amazing AIs. So AIS that will	création d’IA exemplaires. Des IA qui
133	uh not only make our world better uh but	non seulement rendront notre monde meilleur,
134	that will understand us understand what	mais qui nous comprendront, comprendront ce
135	humanity is through that process.	qu’est l’humanité à travers ce processus.
136	>> What is the definition of the word	>> Quelle est la définition du mot
137	dystopia?	“dystopie” ?
138	>> So in my in my mind these are adverse	>> Dans mon esprit, ce sont des circonstances
139	circumstances that unfortunately might	défavorables qui risquent malheureusement
140	escalate beyond our control. The problem	d’échapper à notre contrôle. Le problème,
141	is the uh there is a lot wrong with the	c’est qu’il y a beaucoup de choses fausses dans le
142	um value set with the ethics of humanity	système de valeurs et l’éthique de l’humanité,
143	at the age of the rise of the machines.	à l’ère de l’essor des machines.
144	and when you take a technology every	Et chaque technologie que nous créons
145	technology we've ever created just	a toujours
146	magnified human abilities. So you know	amplifié les capacités humaines. Par exemple,
147	you can walk at 5 km an hour you get in	un humain marche à 5 km/h, en voiture
148	a car and you can now go you know 250	il roule à 250 km/h.
149	280 m an hour. Okay. uh basically	En gros,
150	magnifying your mobility if you want you	on amplifie la mobilité. Ou encore, on
151	know you can use a computer to magnify	utilise un ordinateur pour augmenter la
152	your u calculation abilities or whatever	capacité de calcul. Et
153	okay and and what AI is going to magnify	ce que l’IA va amplifier,
154	unfortunately at this time is it's going	malheureusement, cette fois‑ci, c’est
155	to magnify the evil that man can do and	le mal que l’homme peut faire.
156	and it is within our hands completely	Et c’est entièrement entre nos mains,
157	completely within our hands to change	entièrement à nous de changer cela.
158	that but I have to say I don't think	Mais je dois dire que je ne pense pas que
159	humanity has the awareness uh at this	l’humanité ait aujourd’hui la conscience
160	time to focus on	nécessaire pour se concentrer
161	so that we actually use AI to build the	pour utiliser l’IA afin de bâtir l’utopie.
162	utopia.	
163	>> So what you're essentially saying is	>> Donc, tu dis essentiellement que tu penses désormais
164	that you now believe there'll be a	qu’il y aura une période
165	period of dystopia and to define the	de dystopie, et pour définir le mot
166	word dystopia, I've used AI, it says a	“dystopie”, j’ai utilisé l’IA : elle dit que c’est
167	terrible society where people live under	une société terrible où les gens vivent sous
168	fear, control or suffering and then you	la peur, le contrôle ou la souffrance. Et tu penses
169	think we'll come out of that dystopia	que nous sortirons de cette dystopie
170	into a utopia which is defined as a	pour aller vers une utopie, définie comme un
171	perfect or ideal place where everything	endroit parfait où tout fonctionne bien,
172	works well, a good society where people	où les gens vivent en paix, en santé et heureux.
173	live in peace, health and happiness.	
174	>> Correct.	>> C’est exact.
175	And the difference between them,	Et la différence entre les deux,
176	interestingly, is what I normally refer	est ce que j’appelle généralement
177	to as the second dilemma, which is the	le second dilemme, c’est‑à‑dire le moment
178	po point where we hand over completely	où nous remettons complètement
179	to AI. So, a lot of people think that	le contrôle à l’IA. Beaucoup pensent que
180	when AI is in full control, it's going	lorsque l’IA contrôlera tout, ce sera un risque
181	to be an existential risk for humanity.	existentiel pour l’humanité.
182	You know, I have enough uh evidence to	Tu sais, j’ai assez de preuves pour
183	to argue that when we fully hand over to	affirmer que lorsqu’on remettra tout à l’IA,
184	AI, that's going to be our salvation.	ce sera notre salut.
185	that the problem with us today is not,	Le problème aujourd’hui, ce n’est pas
186	you know, that intelligence is going	que l’intelligence sera contre nous,
187	work against us. It's that our stupidity	mais que notre stupidité
188	as humans is working against us. And I	d’humains agit contre nous. Et je
189	think the challenges that will come from	pense que les défis liés aux humains au pouvoir
190	humans being in control uh are going	seront plus graves que
191	outweigh the	ceux liés à
192	the challenges that could come from AI	une IA au pouvoir.
193	being in control.	
194	>> So, as we're in this dystopia period,	>> Donc, pendant que nous serons dans cette période dystopique,
195	did you do you forecast the length of	as‑tu prévu la durée
196	that dystopia? Yeah, I count I count it	de cette dystopie ? Oui, je la calcule, je la vois
197	exactly as 12 to 15 years. I believe	précisément durer de 12 à 15 ans. Je crois
198	the beginning of the slope will happen	que la pente commencera
199	in 2027. I mean it we will see signs in	en 2027. On en verra les signes
200	26. We've seen signs in 24 but we will	en 2026. On en voit déjà en 2024, mais on verra
201	see escalating signs next year and then	une aggravation l’an prochain puis un
202	a a clear uh slip in 27.	glissement net en 2027.
203	>> Why?	>> Pourquoi ?
204	>> The geopolitical environment of our	>> Le contexte géopolitique de notre
205	world is not very positive. I mean you	monde n’est pas très positif. Il faut vraiment
206	really have to think deeply about not	réfléchir non pas aux symptômes,
207	the not the symptoms but the the reasons	mais aux vraies raisons
208	why we are living the world that we live	pour lesquelles nous vivons dans le monde tel qu’il est
209	in here in today is money right and uh	aujourd’hui : l’argent, d’accord ?
210	and money for anyone who knows who	Et pour quiconque comprend l’argent,
211	really knows money's you and I are	nous, toi et moi, sommes des
212	peasants you know we build businesses we	paysans : on crée des entreprises, on
213	contribute to the world we make things	contribue, on fabrique, on vend des choses,
214	we sell things and so on real money is	alors que l’argent “réel” ne se fait pas là.
215	not made there at all real money is made	Le véritable argent se fait
216	in lending in fractional reserve, right?	dans le prêt, dans la réserve fractionnaire.
217	And and you know the biggest lender uh	Et tu sais, le plus grand prêteur
218	in the world would want reasons to lend	du monde a besoin de raisons de prêter,
219	and those reasons are never as big as	et rien n’est plus “utile” pour cela qu’une guerre.
220	war. I mean think about it, huh? Uh the	Réfléchis : le
221	world spent $2.71	monde a dépensé 2,71
222	trillion on war in 2024,	milliers de milliards pour la guerre en 2024,
223	right? A trillion dollars a year in the	dont 1 000 milliards par an aux États‑Unis.
224	US.	
225	And when you really think deeply, I	Et quand on y pense bien,
226	don't mean to be scary here.	je ne veux pas faire peur,
227	You know, weapons have depreciation.	tu sais, les armes se déprécient.
228	They depreciate over 10 to 30 years.	Elles perdent de la valeur sur 10 à 30 ans.
229	Most weapons,	La plupart des armes,
230	>> they lose their value.	>> perdent leur valeur.
231	>> They lose their value and they	>> Elles perdent leur valeur et elles
232	depreciate in accounting terms on the	se déprécient comptablement sur les
233	books of an army. The current arsenal of	bilans d’une armée. L’arsenal actuel des
234	the US, that's a result of a deep search	États‑Unis, d’après une recherche poussée
235	with my AI Trixie. You know the current	avec mon IA Trixie, tu sais, cet arsenal
236	arsenal I think we we think cost the US	a coûté aux États‑Unis
237	24 to 26 trillion dollars to build. Mon	entre 24 et 26 mille milliards à construire. Ma
238	conclusion is that a lot of the wars	conclusion, c’est que beaucoup des guerres
239	that are happening around the world	qui se déroulent dans le monde
240	today are a means to get rid of those	servent à se débarrasser de ces armes
241	weapons so that you can have replace	pour pouvoir les remplacer.
242	them. And uh you know when when your	Et quand ta morale d’industrie
243	morality as an industry is we're	consiste à fabriquer des armes pour tuer,
244	building weapons to kill then you know	alors, forcément, tu finis par t’en servir.
245	you might as well use the weapons to	
246	kill.	
247	>> Who benefits? the lenders and the	>> Qui en profite ? Les prêteurs et l’industrie,
248	industry,	
249	>> but but they can't make the decision to	>> mais ils ne peuvent pas décider eux‑mêmes
250	go to war. They they have to rely on	d’aller en guerre. Ils doivent compter sur
251	>> remember I said that to you when we I	>> souviens‑toi, je te l’avais dit
252	think on on our third podcast. War is	je crois lors de notre troisième podcast : la guerre est
253	decided first	d’abord décidée,
254	then the story is manufactured. You you	puis l’histoire est fabriquée. Tu te rappelles
255	remember 1984 and the Orwellian approach	d’*1984* et de l’approche orwellienne :
256	of like you know uh freedom is slavery	où, tu sais, la liberté c’est l’esclavage,
257	and uh war is peace and they call it uh	la guerre c’est la paix, ils appellent ça
258	something speak uh basically to to to to	la “novlangue”, pour convaincre
259	convince people that going to war in	les gens que partir en guerre dans
260	another country to to kill 4.7 million	un autre pays pour tuer 4,7 millions de personnes,
261	people is freedom. You know we're going	c’est la liberté. “Nous y allons pour libérer
262	there to free the Iraqi people.	le peuple irakien.”
263	Is war ever freedom? you know, to to	La guerre, est‑ce vraiment la liberté ? Dire qu’on
264	tell someone that you're going to kill	va tuer 300 000
265	300,000	women et enfants, c’est
266	women and children is for liberty and	pour la liberté et
267	for the the the you know, for human	pour les valeurs humaines ?
268	values.	
269	Seriously, how do we ever get to believe	Sérieusement, comment peut‑on croire à cela ?
270	that the story is manufactured and then	Les récits sont fabriqués, et ensuite
271	we follow and humans because we're	on y croit, parce que les humains, naïfs,
272	gullible uh we cheer up and we say,	applaudissent et disent :
273	"Yeah, yeah, yeah. We are we're on the	« Oui, on est du bon côté !
274	right side. They are the bad guys."	Eux, ce sont les méchants. »
275	>> Okay. So, let me let me have a let me	>> D’accord. Alors, laisse‑moi essayer une idée.
276	have a go at this idea. So, the idea is	Donc, l’idée, c’est que
277	that really money is driving a lot of	c’est l’argent qui alimente une grande partie
278	the conflict we're seeing and it's	des conflits qu’on voit, et
279	really going to be driving the dystopia.	qu’il alimente donc cette dystopie.
280	So, here's an idea. So, I um I was	Voici une réflexion : j’ai lu
281	reading something the other day and it	quelque chose l’autre jour qui
282	talked about how	expliquait comment
283	billionaires are never satisfied because	les milliardaires ne sont jamais satisfaits, car
284	actually what a billionaire wants isn't	en réalité, ce qu’ils recherchent, ce n’est pas
285	actually more money. It is more status.	plus d’argent, mais plus de statut.
286	Correct. And I was looking at the sort	Exact. Et je regardais le point
287	of evolutionary case for this argument.	de vue évolutif de cet argument.
288	And if you go back a couple of thousand	Et si l’on remonte de quelques milliers
289	years,	d’années,
290	money didn't exist. You were as wealthy	l’argent n’existait pas. La richesse, c’était ce
291	as what you could carry. So even I think	que tu pouvais porter. Donc même, je pense que
292	to the human mind, the idea of wealth	pour l’esprit humain, l’idée de richesse
293	and money	n’est pas naturelle.
294	isn't a thing. what we've but what has	Ce qui a toujours compté,
295	always mattered from a survival of the	pour la survie du plus apte,
296	fittest from a reproductive standpoint	c’est sur le plan reproductif,
297	what's always had reproductive value if	ce qui a toujours eu de la valeur, si
298	you go back thousands of years the	tu remontes des millénaires en arrière, c’était
299	person who was able to mate the most was	la personne capable de se reproduire le plus
300	the person with the most status so it	c’était celle qui avait le plus haut statut. Donc,
301	makes the case the reason why	cela explique pourquoi
302	billionaires get all of this money but	les milliardaires accumulent tant d’argent mais
303	then they go on podcasts and they want	puis vont sur des podcasts, veulent
304	to start their own podcast and they want	créer le leur et acheter des journaux :
305	to buy newspapers is actually because	c’est parce qu’au plus profond d’eux‑mêmes,
306	at the very core of human beings is a	les humains ont un besoin viscéral
307	desire to increase their status.	d’accroître leur statut.
308	>> Yeah. And so if we think of when we	>> Oui. Donc si l’on y pense,
309	going back to the example of why wars	en revenant à l’exemple des guerres,
310	are breaking out, maybe it's not money.	peut‑être que ce n’est pas l’argent,
311	Maybe actually it's status and and it's	mais le statut. C’est ce
312	this prime minister or this leader or	que recherchent ces premiers ministres,
313	this, you know, individual wanting to	ces dirigeants, ces individus, qui veulent
314	create more power and more status	créer plus de pouvoir et plus de statut,
315	because really at the heart of what	car, au fond, ce qui compte
316	matters to a human being is having more	pour un être humain, c’est d’avoir plus
317	power and more status. And money is	de pouvoir et de statut. Et l’argent
318	actually money as a thing is actually	n’est qu’un
319	just a proxy of my status.	simple reflet de ce statut.
320	>> And and what kind of world is that?	>> Et quel genre de monde est‑ce, ça ?
321	>> I mean, it's a fucked up one. all these	>> Franchement, un monde complètement foutu. Tous ces
322	all these powerful men have uh	hommes puissants ont…
323	>> correct	>> exactement.
324	>> are really messing the world up. But	>> …vraiment foutu le monde en l’air. Mais
325	>> so so can can I can I can I	>> alors, je peux… oui ?
326	>> actually AI is the same	>> En fait, l’IA, c’est pareil :
327	>> because we're in this AI race now where	>> on est dans une course à l’IA où
328	a lot of billionaires are like if I get	beaucoup de milliardaires se disent : si j’obtiens
329	AGI artificial general intelligence	l’intelligence artificielle générale (AGI)
330	first then I basically rule the world	d’abord, je domine le monde.
331	>> 100%. That's exactly the the concept	>> Exactement. C’est précisément l’idée,
332	what I what I used to call the the the	ce que j’appelais avant
333	first inevitable now I call the first	« le premier inévitable », et que je nomme maintenant
334	dilemma and scary smart is that it's	le premier dilemme ; et dans *Scary Smart*, c’est
335	it's a race that constantly accelerates.	une course qui s’accélère sans cesse.
336	You think the next 12 years are going to	Tu penses que les 12 prochaines années seront
337	be AI dystopia where things aren't	une dystopie dominée par l’IA ?
338	>> I think the next 12 years are going to	>> Je pense que les 12 prochaines années seront
339	be human dystopia using AI	une dystopie humaine utilisant l’IA.
340	>> humaninduced dystopia using AI	>> Une dystopie provoquée par l’humain à travers l’IA.
341	>> and you define that by a rise in warfare	>> Et tu la définis par une recrudescence des guerres
342	around the world as	dans le monde ?
343	>> the last the last one the RIP the last	>> Le dernier, le dernier “RIP”, le dernier,
344	one is basically you're going to have a	c’est qu’il y aura une énorme concentration du pouvoir
345	massive concentration of power and a	et une grande dispersion du pouvoir.
346	massive distribution of power okay and	Cela signifie que ceux qui concentrent
347	that basically will mean that those with	le pouvoir vont tenter d’opprimer
348	the maximum concentration of power are	ceux qui détiennent la démocratie du pouvoir.
349	going to try to oppress those with with	—
350	democracy of power. Okay, so think about	—
351	it this way in today's world um unlike	réfléchis‑y : dans le monde actuel, contrairement
352	the past	au passé,
353	uh	euh
354	you know the Houthis with a drone the	les Houthis, avec un drone,
355	Houthis are the Yemeni uh tribes	sont des tribus yéménites
356	basically resisting US power and Israeli	qui résistent au pouvoir américain et israélien
357	power in the Red Sea. Okay. They use a	dans la mer Rouge. Ils utilisent un
358	drone that is $3,000 worth to attack a	drone coûtant 3 000 $ pour attaquer
359	uh a warship from from the US or an	un navire de guerre ou un avion américain
360	airplane from the US and so on that's	valant des centaines de millions.
361	worth hundreds of millions. Okay, that	Cette “démocratisation du pouvoir”
362	kind of democracy of power makes those	rend les puissants très nerveux :
363	in power worry a lot about where the	d’où viendra la prochaine menace ?
364	next threat is coming from. Okay, and	Et cela vaut non seulement pour la guerre,
365	this happens not only in war but also in	mais aussi pour l’économie,
366	economics. Okay, also in innovation,	l’innovation,
367	also in technology and so on and so	la technologie, et ainsi de suite.
368	forth, right? And so basically what that	Autrement dit,
369	means is that like you rightly said as	comme tu l’as dit, alors que
370	the the the tech oligarchs are	les oligarques technologiques
371	attempting to get to AGI.	tentent d’atteindre l’AGI,
372	They want to make sure that as soon as	ils veulent s’assurer que dès qu’ils l’auront,
373	they get to AGI that nobody else has AGI	personne d’autre ne l’aura,
374	and and basically they want to make sure	et surtout que personne ne pourra
375	that nobody else has the ability to	menacer leur position de privilège.
376	shake their position of privilege if you	—
377	want. Okay. And so you're going to see a	Donc, on va voir un monde
378	world where unfortunately there's going	où, malheureusement, il y aura
379	to be a lot of control, a lot of	beaucoup de contrôle,
380	surveillance, a lot of um of forced	beaucoup de surveillance,
381	compliance if you want or you lose your	beaucoup de conformité forcée : sinon, tu perds ton
382	privilege to be in the world and and it	privilege d’exister dans ce monde. Et
383	is happening already.	Cela se produit déjà.
384	>> With this acronym, I want to make sure	>> Avec cet acronyme, je veux m’assurer qu’on le couvre en entier.
385	we get through the whole acronym. So	—
386	>> you like dystopians, don't you?	>> Tu aimes les dystopies, n’est‑ce pas ?
387	>> I want to do the dystopian thing, then I	>> Je veux parler de la dystopie, puis
388	want to do the utopia.	parler de l’utopie.
389	>> Okay.	>> D’accord.
390	>> And ideally how we move from dystopia to	>> Et idéalement, comment on passe de la dystopie
391	utopia.	à l’utopie.
392	>> Mhm. So the the the the F in face R	>> Mmm. Donc le “F” de “face R”
393	>> is the loss of freedom as a result of	>> est la perte de liberté, conséquence de
394	that power dichotomy. Right? So you have	cette dichotomie de pouvoir. Tu vois, on a
395	you have a massive amount of power as	un pouvoir énorme, comme on le voit
396	you can see today in uh one specific	aujourd’hui avec une armée particulière
397	army being powered by the US uh funds	financée par les États‑Unis,
398	and a lot of money righting against	et beaucoup d’argent, combattant
399	peasants really that have no weapons	de simples paysans sans armes ou presque.
400	almost at all.	—
401	>> Okay. Some of them uh are militarized	>> Bien sûr, certains sont militarisés,
402	but the majority of the mill two million	mais la majorité des deux millions de
403	people are not. Okay. And so there is	personnes ne le sont pas. Donc, il y a
404	massive massive power that basically	un pouvoir gigantesque qui dit :
405	says, you know what, I'm going to	« Je vais opprimer autant que je veux. »
406	oppress as far as I go. Okay. And I'm	Et je ferai ce que je veux, car
407	going to do whatever I want because the	les applaudissements resteront silencieux
408	cheerleaders are going to be quiet,	ou même favorables.
409	right? Or they're going to cheer or even	—
410	worse. Huh? And so basically in in that	Eh bien, dans ce contexte,
411	what happens is max maximum power	se produit que le pouvoir absolu,
412	threatened by a democracy of power leads	menacé par la démocratie du pouvoir, mène
413	to a loss of freedom. A loss of freedom	à une perte de liberté. Une perte pour tous.
414	for everyone.	
415	>> Because how does that impact my freedom?	>> Et comment ça affecte ma liberté, à moi ?
416	>> Your freedom. Yeah,	>> Ta liberté, oui.
417	>> very soon uh you will if you publish	>> Très bientôt, si tu publies
418	this episode you're going to start to	cet épisode, on commencera à
419	get questions around should you be	se demander si tu devrais
420	talking about this those topics in your	parler de ces sujets dans ton
421	podcast. Okay. Uh you know uh if I uh	podcast. Et si je participe
422	have been on this episode then probably	à cet épisode, il y a fort à parier que
423	next time I land in the US someone will	la prochaine fois que j’atterrirai aux États‑Unis,
424	question me say why do you say those	on me questionnera : « Pourquoi dites‑vous cela ?
425	things? Which side are you on? Right?	De quel côté êtes‑vous ? »
426	and and and and you know you can easily	Et tu vois bien qu’on
427	see that everything I mean I told you	perd peu à peu la liberté. Je t’avais dit :
428	that before doesn't matter what I try to	peu importe ce que j’essaie d’apporter au monde,
429	contribute to the world my bank will	ma banque ferme mon compte
430	cancel my bank account every 6 weeks	toutes les six semaines,
431	simply because of my ethnicity and my	seulement à cause de mon origine
432	origin right every now and then they'll	et de mon ethnie. À chaque fois, on
433	just stop my my bank account and say we	me bloque le compte en disant :
434	need a document	« Nous avons besoin d’un document. »
435	my other colleagues of a different color	Mes autres collègues d’une autre couleur
436	or a different ethnicity don't get asked	ne doivent jamais présenter de
437	for another document right but but but	document, mais moi, oui,
438	that's because I come from an ethnicity	parce que j’appartiens à une ethnie
439	that is positioned in the world for the	considérée comme “ennemie” dans le monde
440	last 30 40 years as the uh enemy. Okay?	depuis 30‑40 ans.
441	And and so when you really really think	Et donc, quand on réfléchit vraiment,
442	about it, in a world where everything is	dans un monde où tout devient numérique,
443	becoming digital, in a world where	où tout est surveillé,
444	everything is monitored, in a world	où tout est visible,
445	where everything is seen, okay, we don't	nous n’avons plus vraiment de liberté.
446	have much freedom anymore. And I'm not	Et je ne dis pas ça pour débattre,
447	actually debating that or or I don't see	mais je ne vois pas de solution.
448	a way to fix that	
449	>> because the AI is going to have more	>> Parce que l’IA aura plus
450	information on us, be better at tracking	d’informations sur nous, saura mieux nous suivre,
451	who we are, and therefore that will	et cela mènera donc
452	result in certain freedoms being	à la restriction de certaines libertés.
453	restricted. Is that what you're saying?	C’est bien ce que tu veux dire ?
454	>> This is one element of it. Okay. If you	>> C’est un aspect, oui. Si tu pousses ce point plus loin,
455	push that element further	—
456	in in in in a very short time if you've	dans très peu de temps, si tu as vu
457	seen agent for example recently manos or	par exemple récemment les agents, Manos ou
458	ChatGPT there will be a time where you	ChatGPT, il viendra un temps
459	know you'll simply not do things	où tu ne feras plus rien
460	yourself anymore. Okay. You'll simply go	par toi‑même. Tu diras juste
461	to your AI and say hey by the way I'm	à ton IA : « Au fait, je vais voir Stephen,
462	going to meet Stephen. Can you please	peux‑tu me réserver ça ? »
463	you know book that for me?	
464	>> Great.	>> Génial.
465	>> And and and yeah and it will do	>> Oui, et elle le fera
466	absolutely everything. That's great	absolument tout. Ce sera bien,
467	until the moment where it decides to do	jusqu’au moment où elle décidera de
468	things that are not motivated only by	faire des choses non motivées uniquement par
469	your well-being. Right. Why would he do	ton bien‑être. Pourquoi ferait‑elle cela ?
470	that?	
471	>> Simply because, you know, maybe if I buy	>> Tout simplement parce que, si j’achète un
472	a BA ticket instead of an Emirates	billet BA plutôt qu’Emirates,
473	ticket, some agent is going to make more	un agent gagnera plus d’argent qu’un autre,
474	money than other agents and so on,	et ainsi de suite.
475	right? Uh and I wouldn't be able to even	Et je ne pourrais même plus m’en rendre compte,
476	catch it up if I hand over completely to	si je laisse tout à une IA.
477	an AI. Uh go go a step further. Huh?	Fais un pas de plus :
478	Think about a world where everyone	pense à un monde où presque tout le monde
479	almost everyone is on UBI. Okay.	sera sous revenu universel, d’accord ?
480	>> What's UBI?	>> Qu’est‑ce que le revenu universel ?
481	>> Universal basic income. I mean, think	>> Le revenu de base universel. Pense
482	about the economics, the E and face	à l’économie, le “E” de FACE RIPs.
483	rips. Think about the economics of a	Pense à l’économie d’un monde où
484	world where we're going to start to see	où l’on verra bientôt apparaître
485	a trillionaire	un trillionnaire
486	before 2030. I can guarantee you that	avant 2030. Je te garantis que
487	someone will be a trillionaire. I'm I'm	quelqu’un deviendra trillionnaire. Je
488	you know I think there are many	pense même qu’il en existe déjà
489	trillionaires in the world today or	il y en a déjà plusieurs dans le monde,
490	there we just don't know who they are.	on ne sait simplement pas qui ils sont.
491	But there will be a new Elon Musk or	Mais il y aura un nouveau Elon Musk ou
492	Larry Ellison that will become a	Larry Ellison qui deviendra
493	trillionaire because of AI investments,	trillionnaire grâce à ses investissements dans l’IA.
494	right? And and that trillionaire will	d’accord ? Et ce trillionnaire aura
495	have so much money to buy everything.	tellement d’argent qu’il pourra tout acheter.
496	There will be robots and AIs doing	Il y aura des robots et des IA qui feront
497	everything and humans will have no jobs.	tout, et les humains n’auront plus d’emploi.
498	I mean	Je veux dire…
499	>> Do you think that's a there's a real	>> Tu penses que c’est vraiment possible
500	possibility of job displacement over the	qu’autant d’emplois disparaissent
501	next 10 years? And the the rebuttal to	d’ici dix ans ? Et la réponse qu’on entend souvent,
502	that would be that there's going to be	c’est qu’il y aura de nouveaux métiers dans la tech.
503	new jobs created in technology.	
504	>> Absolute crap.	>> Absolument faux.
505	>> Really?	>> Vraiment ?
506	>> Of course.	>> Bien sûr.
507	>> How how can you be so sure?	>> Comment peux‑tu en être aussi sûr ?
508	>> Okay. So again, I am not sure about	>> D’accord. Encore une fois, je ne suis sûr de rien.
509	anything. So So let's just be very very	Donc soyons très clairs,
510	clear. It would be very arrogant. Okay.	il serait très arrogant
511	To assume that I know	de prétendre que je sais.
512	>> you just said it was crap.	>> Tu viens pourtant de dire que c’était n’importe quoi.
513	>> My my belief is it is 100% crap.	>> Ma conviction, c’est que c’est à 100% du pipeau.
514	>> Take a job like software developer.	>> Prenons un métier comme celui de développeur logiciel.
515	>> Yeah.	>> Oui.
516	>> Okay. Uh Emma.Love my my new	>> D’accord. Emma.Love, ma nouvelle
517	startup is me, Senad, another technical	start‑up, c’est moi, Senad, un autre ingénieur tech
518	engineer and a lot of AIS. Okay. That	et beaucoup d’IA. Cette
519	startup would have been 350 developers	start‑up aurait nécessité 350 développeurs
520	in the past.	autrefois.
521	>> I get that. Um but are you now hiring in	>> D’accord, mais embauches‑tu maintenant à
522	other roles because of that or or you	d’autres postes à cause de cela ? Ou bien,
523	know as is the case with the steam	comme avec la machine à vapeur,
524	engine? I can't remember the effect but	je ne me souviens plus du nom de l’effet, mais
525	there's you probably know that when	tu sais, quand
526	steam when coal became cheaper people	le charbon est devenu moins cher, les gens
527	were worried that the coal industry	se sont inquiétés pour l’industrie du charbon,
528	would go out of business but actually	mais en réalité,
529	what happened is people used more trains	on a utilisé davantage de trains,
530	so trains now were used for transport	donc pour le transport
531	and other things and leisure whereas	et même les loisirs, alors qu’avant
532	before they were just used for commu for	c’était juste pour le fret.
533	um cargo. Yeah. So there became more use	Donc, il y a eu plus d’usages,
534	cases and the coal industry exploded. So	et l’industrie du charbon a explosé. Alors
535	I'm wondering with technology, yeah,	je me demande si, avec la technologie,
536	software developers are going to maybe	oui, les développeurs auront peut‑être
537	not have as many jobs, but there	moins de travail, mais il y aura
538	everything's going to be software.	du logiciel partout.
539	>> Name me one.	>> Cite‑m’en un.
540	>> Name you one. What	>> T’en nommer un ? Quel…
541	>> job?	>> métier ?
542	>> Name you that's going to be created.	>> Dis‑moi un métier qui serait créé.
543	>> Yeah. One job that cannot be done by an	>> Oui. Un travail qu’une IA ne peut pas faire.
544	AI.	>> …
545	>> Yeah.	>> Oui.
546	>> Or a robot.	>> Ou un robot.
547	>> My girlfriend's breath work retreat	>> L’entreprise de retraites “breathwork” de ma copine,
548	business where she takes groups of women	où elle emmène des groupes de femmes
549	around the world. Her company is called	aux quatre coins du monde. Sa société s’appelle
550	Barley Breathwork. And there's going to	Barley Breathwork. Et il va y avoir
551	be a greater demand for connection,	une demande croissante de connexion,
552	human connection.	de lien humain.
553	>> Correct. Keep going.	>> Exact. Continue.
554	>> So there's going to be more people doing	>> Donc, il y aura plus de gens qui organiseront
555	community events in real life festivals.	des événements communautaires, des festivals réels.
556	I think we're going to see a huge surge	Je pense qu’on verra une grande montée
557	in things like	des activités comme
558	>> everything that has to do with human	>> tout ce qui touche à la connexion
559	connection.	humaine.
560	>> Yeah,	>> Oui.
561	>> correct. I'm totally in with that. Okay.	>> Exactement. Je suis entièrement d’accord. OK.
562	What's the percentage of that versus	Mais quel pourcentage cela représente‑t‑il
563	accountant?	face aux comptables ?
564	>> It's a much smaller percentage for sure	>> Une part bien plus faible, c’est sûr,
565	in terms of white collar jobs.	en ce qui concerne les emplois de bureau.
566	>> Now, who does she sell to?	>> Bien. Et à qui vend‑elle ?
567	>> People with probably what? probably	>> À des gens qui ont… quoi ? Probablement
568	accountants or you know	des comptables, ou tu vois…
569	>> correct she she sells to people who earn	>> Exact. Elle vend à des gens qui gagnent
570	money from their jobs.	de l’argent grâce à leur travail.
571	>> Yeah.	>> Oui.
572	>> Okay. So you have two forces happening.	>> Donc, deux forces se produisent.
573	One force is there are clear jobs that	>>> D’un côté, il y a des métiers clairement
574	will be replaced. Video editor is going	remplacés. Le monteur vidéo va
575	to be replaced. Uh	être remplacé.
576	>> excuse me.	>> Excuse‑moi.
577	>> I love	>> J’adore
578	as as a matter of fact podcaster is	et d’ailleurs, les podcasteurs vont
579	going to be replaced.	être remplacés.
580	>> Thank you for coming on today Mo. It was	>> Merci d’être venu aujourd’hui, Mo. C’était
581	seeing you again.	top de te revoir.
582	But but but the truth is a lot so so you	Mais la vérité, c’est que beaucoup… tu vois,
583	see the best at any job will remain the	les meilleurs dans chaque métier resteront
584	best software developer the one that	les meilleurs : le développeur qui
585	really knows architecture knows	maîtrise vraiment l’architecture, la
586	technology and so on will stay for a	technologie, restera un temps, d’accord ?
587	while right and you know one of the	Et, tu sais, une des choses les plus
588	funniest things I interviewed Max	drôles : j’ai interviewé Max
589	Tedmar and Max was laughing out loud	Tedmar, et il riait aux éclats
590	saying CEOs are celebrating that they	en disant : « Les PDG se réjouissent car ils peuvent
591	can now get rid of people and have	se débarrasser du personnel, gagner en
592	productivity gains and cost reductions	productivité et réduire les coûts »
593	because AI can do that job. The one	parce que l’IA peut faire ce travail. Ce qu’ils
594	thing they don't think of is AI will	n’imaginent pas, c’est que l’IA les remplacera eux aussi.
595	replace them too. AGI is going to be	L’AGI sera meilleure que l’humain
596	better than at everything than humans at	dans tout, y compris
597	everything including being a CEO. Right?	dans le rôle de PDG, tu vois ?
598	And you really have to imagine that	Et il faut vraiment s’imaginer
599	there will be a time where most	qu’un jour, la plupart
600	incompetent CEOs will be replaced. Most	des PDG incompétents seront remplacés.
601	incompetent even breath work. Okay.	Même dans le “breathwork”, d’accord ?
602	Eventually there might actually one of	À terme, il pourrait se produire
603	two things be two things be happening.	deux phénomènes.
604	on one is either uh you know part part	Le premier, c’est que, à part pour les
605	of that job other than the top breath	meilleurs instructeurs,
606	work instructors, okay, are going you	les autres, tu sais, ceux qui
607	know who are going to gather all of the	réunissent encore les personnes capables
608	people that can still afford to pay for	de payer un atelier de respiration,
609	a breath work you know class	seront concentrés au sommet,
610	they're going to be concentrated at the	tandis qu’en bas, beaucoup ne
611	top and a lot of the bottom is not going	travailleront plus.
612	to be working for one of two reasons.	Et cela, pour deux raisons.
613	One is either there is not enough demand	D’abord, il n’y aura pas assez de demande
614	because so many people lost their jobs.	car beaucoup auront perdu leur emploi.
615	So when you're on UBI, you cannot tell	Et quand tu vis du revenu universel,
616	the government, hey by the way, pay me a	tu ne peux pas dire au gouvernement : « Augmente‑moi
617	bit more for a breath work class.	un peu pour mon cours de respiration. »
618	>> UBI being universal basic income just	>> Le revenu universel, c’est donc un
619	gives you money every month.	revenu mensuel garanti.
620	>> Correct. And if you really think of	>> Exact. Et si tu y réfléchis,
621	freedom and economics, UBI is a very	le revenu universel est une situation
622	interesting place to be because	très intéressante, car
623	unfortunately I as I said there's	malheureusement, comme je l’ai dit,
624	absolutely nothing wrong with AI.	il n’y a rien de mal avec l’IA ;
625	There's a lot wrong with the value set	il y a beaucoup de travers dans le système
626	of humanity at the age of the rise of	de valeurs de l’humanité à l’ère
627	the machines, right? And the biggest	de la montée des machines. Et cette
628	value set of humanity is capitalism	grande valeur dominante, c’est le capitalisme.
629	today. And capitalism is all about what?	Et le capitalisme, c’est quoi, au fond ?
630	Labor arbitrage.	La recherche d’arbitrage sur le travail.
631	>> What's that mean?	>> Qu’est‑ce que ça veut dire ?
632	>> I hire you to do something. I pay	>> Je t’embauche pour faire quelque chose. Je te paie
633	you a dollar. I pay it I sell it for	un dollar et je le revends
634	two.	deux.
635	Okay. And and most people confuse that	Et beaucoup s’y trompent,
636	because they say, "Oh, but the cost of a	en disant : « Oui mais le prix d’un
637	product also includes raw materials and	produit inclut aussi les matières premières,
638	factories and so on and so forth." All	les usines, etc. » — mais tout ça
639	of that is built is built by labor,	est lui‑même issu du travail humain.
640	right? So, so basically labor goes and	Donc, concrètement, le travail extrait la matière,
641	mines for the material and then the	puis cette matière est vendue
642	material is sold for a little bit of	avec une marge ; elle est transformée
643	margin then that material is turned into	en machine, revendue avec une marge,
644	a machine. It's sold for a little bit of	et ainsi de suite.
645	margin then that machine and so on.	—
646	Okay, there's always labor arbitrage in	Il y a toujours ce jeu d’arbitrage du travail.
647	a world where humanity's minds are being	Mais dans un monde où l’esprit humain est
648	replaced by uh by AIs, virtual AIs,	remplacé par des IA virtuelles
649	okay, and humanity's power strength	et où sa force physique
650	within 3 to 5 years time can be replaced	pourra être remplacée par un robot en 3 à 5 ans,
651	by a robot,	—
652	you really have to question how this	il faut vraiment se demander à quoi
653	world looks like. It could be the best	ressemblera ce monde. Ce pourrait être le
654	world ever. And that's what I believe	meilleur monde possible. Et c’est ce que je pense
655	the utopia will look like because we	l’utopie sera, car nous
656	were never made to wake up every morning	n’avons jamais été faits pour nous lever tous les matins
657	and just, you know, occupy 20 hours of	et occuper 20 heures par jour
658	our day with work, right? We're not made	par le travail. Ce n’est pas notre vocation.
659	for that. But we've fit into that uh uh,	Mais nous nous sommes tellement intégrés à ce
660	you know, system so well so far that we	système que nous avons fini par croire
661	started to believe it's our life's	que c’était le but de notre vie.
662	purpose.	
663	>> But we choose it. We willingly choose	>> Mais c’est un choix. Nous le faisons volontairement.
664	it. And if you give someone unlimited	Et même si on donne de l’argent illimité à quelqu’un,
665	money, they still tend to go back to	il retourne souvent travailler
666	work or find something to occupy their	ou cherche à s’occuper autrement.
667	time with.	
668	>> They find something to occupy their time	>> Oui, ils trouvent de quoi s’occuper,
669	with,	
670	>> which is usually for so many people is	>> souvent en construisant quelque chose.
671	building something. Philanthropy,	De la philanthropie, oui.
672	100%. So you build something. So	À 100 %. Donc on construit. Entre
673	between Senad and I, Emma.Love is not	Senad et moi, Emma.Love n’est pas
674	about making money. It's about finding	un projet pour gagner de l’argent, mais pour trouver
675	true love relationships.	l’amour véritable, les relations sincères.
676	>> What is that? Sorry, just for context.	>> C’est quoi ? Désolé, pour le contexte.
677	>> So So you know,	>> Donc, tu vois,
678	>> it's a business you're building just for	>> c’est une entreprise que tu construis, juste pour
679	the audience context. So, so, so the	que le public comprenne. Donc l’idée,
680	idea here is I can, it might become a	c’est que ça deviendra peut‑être
681	unicorn and be worth a billion dollars,	un “licorne”, qui vaudra un milliard,
682	but neither I nor Senate are interested,	mais ni Senad ni moi ne cherchons cela ;
683	okay? We're doing it because we can,	on le fait parce qu’on le peut,
684	okay? And we're doing it because it can	et parce que cela peut
685	make a massive difference to the world.	changer le monde de façon significative.
686	>> And you have money, though.	>> Mais tu as de l’argent, toi.
687	>> It doesn't take that much money anymore	>> Il ne faut plus autant d’argent
688	to build anything in the world. This is	pour créer quelque chose. C’est ça,
689	labor arbitrage.	l’arbitrage du travail.
690	>> But to build something exceptional, it's	>> Mais pour créer quelque chose d’exceptionnel,
691	still going to take a little bit more	il faut quand même un peu plus
692	money than building something bad	d’argent que pour faire n’importe quoi.
693	>> for the next few years. So whoever has	>> Pour les prochaines années, oui : celui qui a
694	the capital to build something	le capital pour créer quelque chose
695	exceptional will end up winning.	d’exceptionnel finira par gagner.
696	>> So so this is a very interesting	>> Donc, c’est une compréhension très intéressante
697	understanding of freedom. Okay. This is	de la liberté. C’est aussi
698	the reason why we have the AI arms race.	la raison de la course à l’IA.
699	Okay. Is that the one that owns the	Celui qui possède la
700	platform is going to be making all the	plateforme détiendra tout l’argent
701	money and and keeping all the power.	et tout le pouvoir.
702	Think think of it this way. When	Pense‑y : aux débuts
703	humanity started the best hunter in the	de l’humanité, le meilleur chasseur du clan
704	tribe could maybe feed the tribe for	pouvait nourrir la tribu quelques jours de plus,
705	three to four more years more days. H	et en retour,
706	and as a as a reward, he gained the	il gagnait la faveur de plusieurs partenaires.
707	favor of multiple mates in the tribe.	Voilà tout.
708	That's it. The top farmer in the tribe	Le meilleur agriculteur, lui,
709	could feed the tribe for a season more.	pouvait nourrir la tribu une saison de plus,
710	Okay? And as a result, they got estates	et obtenait en échange des terres, des
711	and you know uh and mansions and so on.	maisons, etc.
712	The best industrialist in the in a in a	Le meilleur industriel d’une ville
713	city could actually employ the whole	pouvait employer toute la ville,
714	city, could grow the GDP of their entire	faire croître le PIB du pays,
715	country. And as a result, they became	et devenait millionnaire.
716	millionaires. the 1920s.	Ça, c’était dans les années 1920.
717	H the best technologists	Maintenant, les meilleurs techniciens
718	now are billionaires. Now what's the	sont milliardaires. Quelle est
719	difference between them? The tool the	la différence entre eux ? L’outil.
720	the hunter only rem depended on their	Le chasseur ne dépendait que de
721	skills and the automation the entire	ses compétences et d’une arme simple.
722	automation he had was a spear. The	Sa seule automation, c’était sa lance.
723	farmer had way more automation. And the	L’agriculteur avait plus d’automatisation, grâce
724	biggest automation was what? The soil.	au sol : la terre faisait le travail.
725	The soil did most of the work. The	L’usine a ensuite fait la majeure partie
726	factory did most of the work. the the	du travail, puis le réseau.
727	network did most of the work. And so	Et donc,
728	that inc incredible expansion of wealth	cette incroyable expansion de la richesse
729	and power and as well the the incredible	et du pouvoir, tout comme l’impact
730	impact that something brings is entirely	impressionnant de ces outils,
731	around the tool that automates. So who's	repose entièrement sur l’outil d’automatisation.
732	going to own the tool? Who's going to	Alors, qui possédera l’outil ?
733	own the the the digital soil, the AI	Qui possédera ce « sol numérique », cette
734	soil? It's the platform owners.	terre d’IA ? Ce seront les propriétaires de plateformes.
735	>> And the platforms you're describing are	>> Et ces plateformes dont tu parles,
736	things like OpenAI, Gemini, Grok. These	c’est OpenAI, Gemini, Grok, etc.
737	these are interfaces to the platforms.	Ce sont des interfaces vers les plateformes.
738	The platforms are all of the uh of the	Les plateformes, ce sont tous les
739	uh um tokens, all of the compute that is	tokens, toute la puissance de calcul
740	in the background, all of the uh all of	en arrière‑plan, toutes les
741	the uh uh methodology, the systems, the	méthodes, les systèmes, les
742	algorithms, that's the platform, the AI	algorithmes ; c’est cela, la plateforme, l’IA
743	itself. You know, Grok is the interface	en soi. Grok, par exemple, en est juste
744	to it.	l’interface.
745	I think this is probably worth	Je crois que ça mérite
746	explaining in layman's terms to people	d’être expliqué simplement à ceux
747	that haven't built AI tools yet because	qui n’ont jamais construit d’outils IA,
748	I think I think to the listener	car le public
749	they probably think that every AI	pense probablement que chaque
750	company they're hearing of right now is	entreprise d’IA dont il entend parler
751	building their own AI whereas actually	construit sa propre IA, alors qu’en réalité
752	what's happening is there is really	il n’y en a qu’une
753	five, six, seven AI companies in the	demi‑douzaine d’entreprises d’IA
754	world and when I built my AI application	dans le monde. Quand j’ai construit mon application IA,
755	I basically	en fait
756	>> pay them for every time I use their AI.	>> je leur paie à chaque utilisation de leur IA.
757	So if Steven Bartlett builds an AI at	Ainsi, si Steven Bartlett crée une IA sur
758	stephvenai.com,	stevenai.com,
759	it's not that I've built my own	ce n’est pas que j’ai entraîné
760	underlying I've trained my own model.	mon propre modèle de fondation.
761	Really what I'm doing is I'm paying	En réalité, je paie
762	Sam Altman's ChatGPT. Um every single	l’IA ChatGPT de Sam Altman à chaque
763	time I do a a call, I basically um I do	utilisation : chaque recherche,
764	a search or you know I use a token. And	chaque requête, j’utilise leurs “tokens”.
765	I think that's really important because	C’est important à comprendre,
766	most people don't understand that unless	car sans expérience technique,
767	you've built AI, you think, "Oh, look,	on se dit : “Ah, regarde,
768	you know, there's all these AI companies	il y a des tas d’entreprises d’IA !”
769	popping up. I've got this one for my	j’ai celle‑ci pour mes e‑mails,
770	email. I've got this one for my dating.	celle‑là pour mes rendez‑vous…
771	I've got No, no, no, no, no. They're	Mais non, en réalité,
772	pretty much I would be I would hazard a	je parierais que presque toutes
773	guess that they're probably all OpenAI	reposent sur OpenAI aujourd’hui.
774	at this point.	—
775	>> No, there are quite a few quite	>> Non, il y a quand même plusieurs acteurs,
776	different characters and quite	très différents,
777	differently,	—
778	>> but there's like five or six.	>> mais il y en a cinq ou six, grosso modo.
779	>> There are five or six when it comes to	>> Il y a cinq ou six modèles linguistiques principaux.
780	language models. Yeah. Right. Uh but	Oui, c’est vrai. Mais
781	interestingly, so yes, I should say	il y a un rebondissement intéressant.
782	yes to start and then I should say but	Donc, oui, d’abord, mais
783	there was an interesting twist with	il s’est passé quelque chose avec
784	DeepSeek at the beginning of the year.	DeepSeek au début de l’année.
785	So what DeepSeek did is is they	DeepSeek a, en fait,
786	basically uh nullified the business	rendu le modèle économique caduc
787	model if you want in two ways. one is it	de deux façons : d’abord,
788	was around a week or two after uh you	environ une ou deux semaines après
789	know Trump stood you know with pride	l’annonce de Trump déclarant fièrement que
790	saying Stargate is the biggest	“Stargate” est le plus grand
791	investment project in the history and	projet d’investissement de l’histoire,
792	it's $500 billion to build AI	pour construire une infrastructure IA de
793	infrastructure and SoftBank and Larry	500 milliards, avec SoftBank, Larry Ellison
794	Ellison and and uh Sam Altman were	et Sam Altman posant ensemble sur la photo.
795	sitting and so you know beautiful	Peu après, DeepSeek R3 est sorti,
796	picture and then DeepSeek R3 comes out	faisant le même travail
797	it does the job for a one over 30 of the	pour un coût trente fois moindre,
798	cost okay and interestingly is entire	et surtout entièrement
799	open source and available as an edge IA	open source et exécutable localement.
800	So, so that's really really interesting	C’est donc très intéressant,
801	because there could be now in the future	car à l’avenir, à mesure que la
802	as the technology improves the learning	technologie progresse, les grands modèles
803	models will be massive but then you can	deviendront énormes, mais pourront être
804	compress them into something you can	compactés dans quelque chose qu’on peut
805	have on your phone and you can download	installer sur un téléphone.
806	DeepSeek literally offline on a	Tu peux littéralement télécharger
807	um um you know an off the network	DeepSeek en mode hors ligne sur un
808	computer and build an AI on it. There's	ordinateur non connecté et y construire une IA.
809	a website that basically tracks the	Il existe un site qui mesure
810	um sort of cleanest apples to Apple's	la part de trafic généré par chaque
811	market share of all the website	chatbot IA sur le web :
812	referrals sent by AI chatbots and	ChatGPT détient environ 79 %,
813	ChatGPT is currently at 79% roughly about	soit près de 80 %.
814	80%. Perplexity is at 11, Microsoft	Perplexity : 11 %, Copilot : 5 %,
815	Copilot about five, Google Gemini is at	Gemini : 2 %, Claude : 1 %,
816	about two, Claude's about one and	et DeepSeek : environ 1 %.
817	DeepSeek is about 1%. And really like	Et au fond, ce qu’il faut comprendre,
818	the the point that I want to land is	c’est que quand on découvre une nouvelle appli
819	just that when you hear of a new AI app	d’IA, un nouvel outil,
820	or tool or this one can make videos,	ou un générateur de vidéos,
821	>> it's built on one of them. It's	>> c’est construit sur l’une de ces grandes
822	basically built on one of these	plates‑formes : en gros,
823	really three or four AI platforms that's	trois ou quatre d’entre elles contrôlées
824	controlled really by three or four AI	par trois ou quatre équipes de milliardaires.
825	you know billionaire teams and actually	—
826	the one of them that gets to what we	Et celle qui atteindra
827	call AGI first where the AI gets really	la première l’AGI, où l’IA devient
828	really advanced	vraiment très avancée,
829	one could say is potentially going to	pourrait, disons, dominer le monde
830	rule the world as it relates to	en matière technologique.
831	technology.	
832	>> Yes. Uh if if they get enough uh head	>> Oui, s’ils ont une avance suffisante.
833	start. So, so I actually think that uh	Mais je pense en fait que
834	what I what I'm more concerned about now	ce qui m’inquiète le plus aujourd’hui,
835	is not AGI, believe it or not. So, AGI	n’est pas l’AGI, crois‑le ou non.
836	in my mind and I said that back in 2023,	Dans ma tête, et je le disais en 2023,
837	right? Uh that we will get to AGI. At	nous atteindrons l’AGI. À
838	the time I said 2027, now I believe 2026	l’époque, je disais 2027 ; maintenant, je dis 2026
839	latest. Okay. The most interesting	au plus tard. Le plus intéressant
840	development that nobody's talking about	dont personne ne parle,
841	is self-evolving AIS.	c’est les IA auto‑évolutives.
842	self evolving AIS is	Ces IA qui s’auto‑améliorent,
843	think of it this way if you and I are	pense‑y : si toi et moi embauchons
844	hiring the top engineer in the world to	le meilleur ingénieur du monde pour
845	develop our AI models	développer nos modèles d’IA,
846	and with AGI that top engineer in the	et qu’avec l’AGI, cet ingénieur devient lui‑même
847	world becomes an AI who would you hire	un IA, qui embaucherais‑tu
848	to develop your next generation AI that	pour créer ta prochaine génération d’IA ?
849	AI	— L’IA elle‑même.
850	>> the one that can teach itself	>> Celle qui peut s’enseigner seule.
851	>> correct so one of my favorite examples	>> Exact. Un de mes exemples préférés,
852	is called Alpha Evolve so this is	c’est Alpha Evolve : la tentative de Google
853	Google's attempt to basically have four	de faire travailler ensemble quatre
854	agents working together four AIs working	agents, quatre IA,
855	together to look at the at the code of	pour examiner le code
856	the AI and say where is the where are	de l’IA et repérer
857	the performance issues then you know an	les problèmes de performance.
858	agent would say what's the problem	Un agent formule le problème,
859	statement what can I uh you know what do	un autre propose des pistes,
860	I need to fix uh one that actually	un autre développe la solution,
861	develops the solution one that assesses	un autre l’évalue, puis
862	the solution and then they continue to	ils recommencent en boucle.
863	do this and you know I don't remember	Et je ne me souviens plus du chiffre exact,
864	the exact figure but I think Google	mais Google aurait gagné environ
865	improved like 8% uh on their AI	8 % de performance
866	infrastructure because of alpha evol	en infrastructure grâce à Alpha Evolve.
867	Right? And when you really really think,	Ouais. Et quand on y pense vraiment,
868	don't quote me on the number 8 to 10, 6	je ne garantis pas le chiffre, 8 ou 10 %,
869	to 10, whatever in Google terms, by the	mais pour Google, c’est énorme,
870	way, that is massive. That's billions	des milliards de dollars. Et
871	and billions of dollars. Now, the the	le piège, ici,
872	the the trick here is this. The trick is	c’est qu’en raisonnement de jeu
873	again, you have to think in game theory	il faut se dire :
874	format.	—
875	Is there any scenario we can think of	y a‑t‑il un scénario où,
876	where if one player uses AI to develop	si un acteur utilise l’IA pour créer
877	the next generation AI that the other	la génération suivante d’IA, les autres
878	players will say no no no no no that's	ne feront pas pareil ? Non. Tous
879	too much you know takes us out of	vont l’imiter.
880	control every other player will copy	Ils auront eux aussi
881	that model and have their next AI model	leurs modèles développés par des IA.
882	developed by an AI.	
883	>> Is this what Sam Altman talks about who's	>> C’est ce dont parle Sam Altman,
884	the founder of um ChatGPT/OpenAI	le fondateur de ChatGPT/OpenAI,
885	when he talks about a fast takeoff? I	quand il évoque un “décollage rapide” ?
886	don't know exactly what which what what	Je ne sais pas exactement ce à quoi
887	which you're referring to but we're all	il fait allusion, mais nous parlons
888	talking about a point now that we call	tous maintenant du moment qu’on appelle
889	the intelligence explosion. So, so there	l’“explosion de l’intelligence”. Il y aura
890	is a moment in time where you have to	un moment où il faut imaginer
891	imagine that if AI now is better than	que si l’IA surpasse déjà 97 % des codeurs
892	97% of all code developers in the world	dans le monde,
893	and soon we'll be able to look at its	elle pourra bientôt relire son propre code,
894	own code own algorithms by the way	ses propres algorithmes.
895	they're becoming incredible	Elles deviennent d’ailleurs de formidables
896	mathematicians which wasn't the case	mathématiciennes, ce qui n’était pas
897	when we last met if they can develop	le cas auparavant. Si elles peuvent
898	improve their own code improve their own	améliorer leur code, leurs algorithmes,
899	algorithms improve their own uh uh you	leur architecture réseau, etc.,
900	know uh network architecture or whatever	tu imagines bien qu’assez vite,
901	you can imagine that very quickly the	la puissance appliquée au développement
902	force applied to developing the next AI	de la génération suivante d’IA
903	is not going to be a human brain	n’émane plus d’un cerveau humain,
904	anymore. It's going to be a much smarter	mais d’un esprit bien plus avancé.
905	brain and very quickly as humans like	Et très vite, comme humains,
906	basically when when we ran the Google	apte à gérer des infrastructures comme chez Google,
907	infrastructure when the machine said we	quand la machine disait : “Il faut un autre serveur”,
908	need another server or a proxy server in	on obéissait.
909	that place we followed. we we never	On ne remettait jamais en cause
910	really you know wanted to to object or	parce que le code savait
911	verify because you know the code would	probablement mieux, vu les
912	probably know better because there are	milliards de transactions. Ainsi,
913	billions of transactions an hour or a	très vite, les IA
914	day and so very quickly those	se‑développant elles‑mêmes diront
915	self-evolving AIs will simply say I need	juste : “Il me faut quatorze serveurs de plus ici”,
916	14 more servers here and we'll just you	et les équipes exécuteront.
917	know the team will just go ahead and do	—
918	it. I watched a video a couple of days	J’ai vu une vidéo récemment
919	ago where he Sam Altman effectively had	où Sam Altman semblait avoir
920	changed his mind because in 2023 which	changé d’avis : en 2023,
921	is when we last met he said the aim was	il disait vouloir
922	for um a slow takeoff which is sort of	un “décollage lent”,
923	gradual deployment and OpenAI's 203	une adoption progressive.
924	2023 note says a slower takeoff is	Dans sa note de 2023,
925	easier to make safe and they prefer	on lit : “Un décollage lent est plus sûr.”
926	iterative rollouts society can adapt in	Ils prônaient un déploiement itératif pour
927	2025	que la société s’adapte. Mais en 2025,
928	>> they changed their mind and Sam Altman	>> ils ont changé d’avis, et Sam Altman
929	said	a déclaré
930	He now thinks a fast takeoff is more	penser désormais qu’un décollage rapide est plus
931	possible than he did a couple of years	probable qu’il ne le croyait il y a quelques années :
932	ago on the order of a small number of	dans quelques années,
933	years rather than a decade. Um, and it	pas une décennie.
934	to define what we mean by a fast	Pour définir un “décollage rapide” :
935	takeoff, it's defined as when AI goes	c’est quand l’IA passe du niveau humain
936	from roughly human level to far beyond	à bien au‑delà du niveau humain
937	human very quickly, think months to a	en quelques mois ou années, plus vite
938	few years, faster than governments,	que les gouvernements ou la société
939	companies, or society can adapt with	ne peuvent s’y adapter,
940	little warning, big power shifts, and	avec peu d’avertissements, un bouleversement du pouvoir
941	hard to control. A slow takeoff, by	et un contrôle difficile. Le “décollage lent”,
942	contrast, is where capabilities climb	lui, c’est une montée graduelle
943	gradually over many years with lots of	sur plusieurs années, avec plusieurs signaux d’alerte.
944	warning shots. Um, and the red flags for	—
945	a fast takeoff is when AI can	Les signaux d’un décollage rapide :
946	self-improve, run autonomous research	une IA qui s’auto‑améliore,
947	and development and scale with massive	mène seule la R&D et se déploie avec
948	compute compounding gains which will	des gains exponentiels de puissance.
949	snowball fast. So, and I think from the	Cela devient vite incontrôlable.
950	video that I watched of Sam Orman	Et dans la vidéo de Sam Altman
951	recently, who again is the founder of	récemment, le fondateur d’OpenAI
952	OpenAIr and HBT, he basically says, and	y dit en substance
953	again I'm paraphrasing here. I will put	(je paraphrase, hein),
954	it on the screen. We have this community	qu’on peut écrire à l’écran : “Celui
955	knows things so I'll write it on the	qui atteindra l’AGI
956	screen. But he effectively said that	en premier aura
957	whoever gets to AGI first will have the	la technologie
958	technology	pour développer
959	>> to develop super intelligence	>> la superintelligence.
960	>> where the AI can can rapidly increase	>> L’IA pourra augmenter
961	its own intelligence and it will	rapidement sa propre intelligence
962	basically leave everyone else behind.	et laisser tout le monde derrière elle.
963	>> Yes. Uh so that last bit is debatable	>> Oui, même si cette dernière partie est discutable.
964	but but let's just agree that uh so so	Mais disons que, dans mon
965	in in a live uh you know one of the	livre *Alive*, dans l’un des
966	posts I shared and got a lot of	posts que j’ai partagés,
967	interest is I refer to the the Altman as	j’ai parlé de “l’Altman”
968	a brand not as a human. Okay. So the	comme d’une marque, pas d’un humain.
969	Altman is that uh persona of a	“L’Altman”, c’est cette figure
970	California disruptive technologist that	de technologue californien disruptif,
971	disrespects everyone. Okay. and believes	qui se fiche de tout le monde,
972	that disruption is good for humanity and	et croit que la disruption est bénéfique.
973	believes that this is good for safety	Il pense que c’est “pour la sécurité”,
974	and like everything else like we say war	comme on dit que la guerre,
975	is for democracy and freedom they say uh	c’est pour la liberté ou la démocratie ;
976	developing you know putting AI on the	il dit que publier l’IA sur Internet,
977	open internet is good for everyone right	c’est pour le bien de tous,
978	it allows us to learn from our mistakes	parce que “nous apprendrons de nos erreurs”.
979	that was Sam Altman's 2023 spiel and if	C’était le discours de 2023 de Sam Altman.
980	you recall at the time I was like this	Et tu te souviens, j’avais dit :
981	is the most dangerous you know one of	“C’est extrêmement dangereux.” C’est l’un
982	the clips that really went viral you so	des extraits que tu as rendus viraux
983	you're you're so clever at finding the	parfaitement, celui où je disais
984	right clips is when I said	—
985	>> I didn't I didn't do the clipping mate	>> Ce n’est pas moi qui ai fait le montage, mec.
986	>> they're team teams remember the clip	>> C’est ton équipe. Tu te rappelles l’extrait
987	where I said we fucked up we always said	où je dis : “On a tout gâché.
988	don't put them on the open internet	On a toujours dit : ne mettez pas ça en ligne
989	until we know what we're putting out in	avant de savoir ce qu’on diffuse dans
990	the world I'm going to be saying that	le monde.” Je le répète.
991	>> yeah we we we fucked up on putting it on	>> Oui. On a merdé en publiant ça
992	the open internet teaching it to to code	sur Internet, en lui apprenant à coder
993	and putting you know agents AI agents	et en laissant des agents IA
994	prompting other AIs now AI agents	demander à d’autres IA :
995	prompting other AIs are leading to	ces interactions mènent à
996	self-developing AIS and and The problem	des IA auto‑évolutives. Et le problème,
997	is, of course, we, you know, anyone who	bien sûr, c’est que tous ceux qui
998	has been on the inside of this knew that	travaillaient en interne savaient
999	this was just a clever spiel made by a	que ce n’était qu’un discours astucieux
1000	PR manager for Sam Altman to sit with his	de communicant pour que Sam Altman aille
1001	dreamy eyes in front of Congress and	devant le Congrès, l’air inspiré,
1002	say, "We want you to regulate us." Now,	en disant : “Régulez‑nous.”
1003	they're saying, "We're unregulable."	Aujourd’hui, ils disent : “On est hors de tout cadre.”
1004	Okay? And and when you really understand	>> D’accord ? Et quand on comprend vraiment
1005	what's happening here, what's happening	ce qui se passe ici,
1006	is it's so fast	c’est que tout va si vite
1007	that none of them has the choice to slow	qu’aucun d’eux n’a la possibilité de ralentir.
1008	down. It's impossible. Neither China	C’est impossible : ni la Chine
1009	versus America or OpenAI versus Google.	ni les États‑Unis, ni OpenAI contre Google.
1010	the that the the only thing that I may	La seule chose qui pourrait, peut‑être,
1011	have may see happening that you you know	différer un peu de ton analyse,
1012	that that may differ a little bit from	c’est que si l’un d’eux arrive
1013	your statement is if one of them gets	le premier à destination,
1014	there first uh then they dominate for	il dominera probablement
1015	the rest of humanity that is probably	le reste de l’humanité. C’est sans doute
1016	true if they get there first uh with	vrai, s’ils arrivent là avant les autres,
1017	within enough buffer. Okay. But the way	avec assez d’avance. Mais quand on voit
1018	you look at Grok coming a week after	Grok sortir une semaine après
1019	OpenAI, a week after uh you know	OpenAI, puis Gemini, puis Claude,
1020	Gemini, a week after Claude and then	et ensuite la Chine
1021	Claude comes again and then China	ou la Corée annoncer la leur,
1022	releases something and then Korea	tout va si vite
1023	releases some something. It is so fast	qu’on pourrait voir plusieurs d’entre eux
1024	that we may get a few of them at the	atteindre ce point presque en même temps,
1025	same time or a few months apart. Okay,	ou à quelques mois d’intervalle.
1026	before one of them has enough power to	Avant qu’un seul ait assez de puissance
1027	become dominant. And that is a very	pour dominer les autres. Et ça, c’est un
1028	interesting scenario.	scénario très intéressant.
1029	multiple AIs, all super intelligent.	Plusieurs IA, toutes superintelligentes.
1030	>> It's funny, you know, I got asked	>> C’est drôle, tu sais, on m’a demandé
1031	yesterday, I was in I was in Belgium on	hier, j’étais en Belgique sur scène,
1032	stage. There was, I don't know, maybe	devant, je ne sais pas, peut‑être
1033	4,000 people in the audience and a kid	4 000 personnes, et un jeune
1034	stood up and he was like, um, you've had	s’est levé et m’a dit : « Euh, tu as eu
1035	a lot of conversations in the last year	beaucoup de conversations cette année
1036	about AI. Like, why do you care? And I	sur l’IA. Pourquoi ça t’intéresse ? »
1037	don't think people realize how,	Et je crois que les gens ne se rendent pas compte
1038	even though I've had so many	qu’en dépit du nombre de discussions que j’ai eues
1039	conversations on this podcast about AI,	sur le sujet dans ce podcast,
1040	you	je…
1041	>> haven't made up your mind.	>> tu n’as toujours pas tranché.
1042	>> I have more questions than ever.	>> J’ai plus de questions que jamais.
1043	>> I know. And it's and it doesn't seem	>> Je sais. Et on dirait que personne
1044	that anyone can satiate.	n’arrive à les apaiser.
1045	>> Anyone that tells you they can predict	>> Quiconque prétend prévoir l’avenir
1046	the future is arrogant.	est arrogant.
1047	>> Yeah.	>> Oui.
1048	>> It is. It's never moved so fast.	>> Oui. Tout n’a jamais évolué aussi vite.
1049	>> It's nothing like nothing I've ever	>> Rien de comparable à ce que j’ai déjà vu.
1050	seen. And you know, by the time that we	Et tu sais, le temps qu’on termine
1051	leave this conversation and I go to my	cette discussion et que je retourne à mon
1052	computer, there's going to be some	ordinateur, il y aura déjà une nouvelle
1053	incredible new technology or application	technologie ou application d’IA incroyable
1054	of AI that didn't exist when I woke up	qui n’existait pas quand je me suis levé
1055	this morning. That creates probably	ce matin. Et ça bouleversera encore
1056	another paradigm shift in my brain.	mon schéma mental.
1057	Also, you know, I people have different	En plus, tu vois, les gens ont des avis divers
1058	opinions of Elon Musk and they're	sur Elon Musk — à juste titre —
1059	they're entitled to their own opinion,	et c’est leur droit,
1060	but the other day, only a couple of days	mais l’autre jour, il a tweeté —
1061	ago, he did a tweet where he said, "At	il y a deux jours — : “Parfois, l’angoisse
1062	times, AI existential dread is	existentielle liée à l’IA est
1063	overwhelming." And on the same day, he	accablante.” Et le même jour, il a aussi
1064	tweeted, "I resisted AI for too long,	tweeté : “J’ai résisté trop longtemps à l’IA,
1065	living in denial. Now it is game on."	en vivant dans le déni. Maintenant, c’est parti.”
1066	And he tagged his AI companies. I don't	Et il a tagué ses entreprises d’IA. Je ne sais
1067	know what to make of I don't know what	pas trop quoi penser de ces tweets.
1068	to make of those tweets. I don't know.	—
1069	And you know, I	Et tu sais, j’essaie
1070	I try really hard to figure out if	vraiment de comprendre si
1071	someone like Sam Wman has the best	un type comme Sam Altman agit
1072	interests of society at heart.	pour le bien de la société.
1073	>> No.	>> Non.
1074	>> Or if these people are just like	>> Ou si ces gens sont juste...
1075	>> I'm saying that publicly. No.	>> Je le dis clairement : non.
1076	As a matter of fact, so I know Sundar	En fait, je connais Sundar Pichai,
1077	Pichai. I work CEO of Alphabet, Google's	le PDG d’Alphabet, la maison mère de Google.
1078	parent company. an amazing human being	C’est quelqu’un d’extraordinaire,
1079	on in all honesty. I know Demis Hassabis	sincèrement. Je connais aussi Demis Hassabis,
1080	is amazing human being. Okay. Uh you	un homme formidable. Ce sont des
1081	know these are are ethical incredible uh	gens éthiques, brillants, humanistes.
1082	humans at heart. They have no choice.	Mais ils n’ont pas le choix.
1083	Uh Sundar by law	Par la loi,
1084	is uh demanded to take care of his his	il doit défendre
1085	shareholder value. That's that is his	la valeur pour ses actionnaires. C’est son
1086	job.	travail.
1087	>> But Sundar you said you know him. You used	>> Mais Sundar, tu le connais, tu travaillais chez Google.
1088	to work at Google.	
1089	>> Yeah. He's not going to do anything that	>> Oui. Il ne fera rien qu’il pense nuire à l’humanité.
1090	he thinks is going to harm humanity.	
1091	>> But if if he does not continue to	>> Mais s’il n’avance pas sur l’IA,
1092	advance AI, that by definition uh uh uh	par définition, cela contredit
1093	contradicts his responsibility as the	ses responsabilités de PDG
1094	CEO of a publicly traded company, he is	d’une société cotée. Il est
1095	liable by law to continue to advance the	légalement tenu de faire progresser
1096	agenda. There's absolutely no doubt	cette mission. Aucun doute là‑dessus.
1097	about it. Now, so but but he's a good	Cela dit, c’est quelqu’un de bien,
1098	person at heart. Deis is a good person	Demis aussi. Ils essaient vraiment
1099	at heart. So they're trying so hard to	de rendre ça sûr,
1100	make it safe. Okay? As much as they can.	dans la mesure du possible.
1101	Reality however is the the the disruptor	Mais la réalité, c’est que le “disrupteur”,
1102	the Altman as a brand doesn't care that	l’“Altman‑marque”, lui, s’en soucie peu.
1103	much.	
1104	>> How do you know that?	>> Comment peux‑tu affirmer ça ?
1105	>> In reality the disruptor is someone that	>> En réalité, un disrupteur, c’est quelqu’un
1106	comes in with the objective of I don't	qui arrive en voulant : “Je n’aime pas le statu quo,
1107	like the status quo. I have a different	j’ai une autre approche.”
1108	approach. And that different approach if	Et cette autre approche, si tu regardes
1109	you just look at the story was we are a	l’histoire, c’est : “Nous sommes une
1110	nonforprofit that is funded mostly by	organisation à but non lucratif
1111	Elon Musk money. It's not entirely by	financée surtout par l’argent d’Elon Musk,
1112	Elon Musk money. So context for people	pas uniquement, mais surtout.” Pour recontextualiser
1113	that might not understand OpenAI. The	pour ceux qui ne connaissent pas OpenAI :
1114	reason I always give context is funnily	et, petite anecdote, si je donne toujours
1115	enough I think I told you this last	du contexte, c’est parce que — je te l’avais dit —
1116	time. I went to a prison where they play	je suis allé dans une prison où
1117	the D of CEO.	ils écoutent le podcast *The Diary of a CEO*.
1118	>> No way.	>> Sans blague ?
1119	>> So they play the D of CO and I think	>> Oui, ils écoutent *The D of C O*, dans peut‑être
1120	it's 50 prisons in the UK to young	50 prisons au Royaume‑Uni, auprès de jeunes détenus,
1121	offenders	—
1122	>> and no violence there.	>> et sans violence, apparemment ?
1123	>> Well, I don't know. I can't I can't I	>> Eh bien, je ne sais pas, je ne peux pas dire
1124	can't tell you whether violence has gone	si la violence a augmenté ou baissé,
1125	up or down. But I was in the cell with	mais j’étais dans la cellule d’un
1126	one of the prisoners, a young a young	jeune prisonnier noir,
1127	black guy, and I was in his cell for for	et j’y suis resté un moment,
1128	a little while. I was reading through	en lisant son projet d’entreprise, etc.
1129	his business plan, etc. And I said, "You	Et je lui ai dit : “Tu sais,
1130	know what? You need to listen to this	tu devrais écouter cette
1131	conversation that I did with Mo Gawdat."	conversation que j’ai eue avec Mo Gawdat.”
1132	So I he has a little screen in his cell.	Il avait un petit écran dans sa cellule,
1133	So I pulled it up, you know, our first	alors j’y ai lancé notre première
1134	conversation. I said, "You should listen	conversation. Je lui ai dit : “Écoute celle‑ci.”
1135	to that one." And he said to me, he	Et il m’a répondu :
1136	said, "I can't listen to that one cuz	“Je ne peux pas écouter ça, vous utilisez
1137	you guys use big words."	trop de grands mots.”
1138	>> So ever since that day, which was about	>> Depuis ce jour‑là —
1139	>> I noticed that about days four years	il y a environ quatre ans, d’ailleurs —
1140	ago, sorry.	—
1141	>> I've always whenever I hear a big word,	>> à chaque fois que j’entends un mot compliqué,
1142	I think about this kid.	je pense à ce garçon.
1143	>> Yeah.	>> Oui.
1144	>> And I say like give context. So even	>> Alors je me dis : “Donne le contexte.”
1145	with the you're about to explain what	même quand tu vas expliquer ce qu’est
1146	OpenAI is, I know he won't know what	OpenAI, je sais qu’il ne connaîtrait pas
1147	OpenAI's origin story was. That's why	l’origine d’OpenAI. Voilà pourquoi
1148	I'm	je fais ça.
1149	>> I think that's a wonderful practice in	>> C’est une très belle habitude,
1150	general. By the way, even, you know,	d’ailleurs. Même moi, tu sais,
1151	being a non native English speaker,	qui ne suis pas anglophone natif,
1152	>> you'll be amazed how often a word is	>> tu serais surpris de voir combien de fois
1153	said to me and I I'm like, yeah, don't	on me dit un mot et je me dis : “Je n’ai aucune idée
1154	know what that means.	de ce que ça veut dire.”
1155	>> So, like I've actually never said this	>> Bref, je ne l’ai jamais dit publiquement,
1156	publicly before, but I now see it as my	mais je vois désormais comme ma responsabilité
1157	responsibility to be to to keep the draw	de maintenir le pont‑levis
1158	the drawbridge	de l’accessibilité
1159	to accessibility of these conversations	à ces conversations
1160	down for him. So, whenever I whenever	ouvert pour lui. Donc, chaque fois
1161	there's a word that at some point in my	qu’un mot m’était inconnu à une époque,
1162	life I didn't know what it meant,	—
1163	>> I will go back. I was like, what does	>> je reviens en arrière : “Qu’est‑ce que ça veut dire ?”
1164	that mean? I think that I've noticed	>> Oui, j’ai remarqué ça
1165	that in the you know more and more in	dans ton podcast, de plus en plus,
1166	your podcast and I really appreciate and	et je trouve ça génial. On affiche
1167	we also show it on the screen sometimes.	parfois le mot à l’écran aussi.
1168	>> I think that's wonderful. I mean the	>> C’est formidable. L’histoire d’OpenAI,
1169	the the origin story of OpenAI is as	d’ailleurs, comme son nom l’indique,
1170	the name suggests it's open source. It's	était *open source*. C’était
1171	for the public good. It was an in you	pour le bien commun. C’était,
1172	know intended in Elon Musk's words to	pour reprendre les mots d’Elon Musk,
1173	save the world from the dangers of AI	pour “sauver le monde des dangers de l’IA”.
1174	right so they were doing research on	Ils faisaient donc de la recherche,
1175	that and then you know there was the	et puis il y a eu ce
1176	disagreement between Sam Altman and and	désaccord entre Sam Altman et Elon.
1177	Elon somehow Elon ends up being out of	Et d’une façon ou d’une autre, Elon s’est retrouvé
1178	uh of uh of OpenAI. I think there was a	dehors : évincé d’OpenAI.
1179	moment in time where he tried to take it	Il a tenté de la reprendre, le conseil
1180	back and you know the board rejected it	l’a refusé, ou quelque chose du genre.
1181	or some something like that. most of the	—
1182	uh top um safety engineers, the top	La plupart des ingénieurs en sécurité,
1183	technical teams in OpenAI left in 2023	les meilleures équipes techniques d’OpenAI,
1184	2024 openly saying we're not concerned	ont quitté l’entreprise en 2023‑2024 en disant
1185	with safety anymore. It moves from being	qu’ils ne se sentaient plus concernés par la sécurité.
1186	a nonforprofit to being one of the most	OpenAI est passée d’une association
1187	valued companies in the world. There are	à l’une des entreprises les plus valorisées du monde.
1188	billions of dollars at stake, right? And	Il y a des milliards de dollars en jeu. Et
1189	if you if you tell me that Sam Altman is	si tu me dis que Sam Altman agit
1190	out there trying to help humanity, let's	pour aider l’humanité, je te dirai :
1191	let's suggest to him and say, "Hey, do	« Propose‑lui donc de le faire gratuitement :
1192	you want to do that for free? We'll pay	on te paiera un bon salaire,
1193	you a very good salary, but you don't	mais sans actions. »
1194	have stocks in this. Saving humanity	sauver l’humanité
1195	doesn't come at the billion dollar	ne rime pas avec milliards de dollars
1196	valuation or of course now tens of	de valorisation — aujourd’hui,
1197	billions or hundreds of billions." And	c’est même des dizaines, des centaines de milliards.
1198	and and see truly that is when you know	Et là seulement, tu verras s’il le fait
1199	that someone is doing it for the good of	pour le bien commun.
1200	humanity. Now the the capitalist system	Mais notre système capitaliste
1201	we've built is not built for the good of	n’est pas conçu pour servir l’humanité,
1202	humanity. It's built for the good of the	mais pour servir le capitaliste.
1203	capitalist.	
1204	>> Well, he might say that releasing the	>> Oui, mais il pourrait dire que rendre
1205	model publicly, open sourcing it is too	le modèle public, l’ouvrir, serait trop
1206	risky	risque,
1207	because then bad actors around the world	car des acteurs malveillants pourraient
1208	would have access to that technology. So	y accéder. Donc il pourrait dire que
1209	he might say that closing OpenAI in	fermer OpenAI, ne pas publier ses modèles,
1210	terms of not making it publicly viewable	est la bonne décision pour la sécurité.
1211	is the right thing to do for safety. We	Et là, on retombe sur la naïveté, les
1212	go back to gullible cheer leaders,	“cheerleaders” crédules.
1213	right? One of the interesting tricks is	Tu vois, l’un des tours les plus classiques
1214	of lying in our world is everyone will	dans notre monde, c’est que chacun dit
1215	say what helps their agenda. Follow the	ce qui sert son agenda. Suis l’argent.
1216	money. Okay, you follow the money and	Suis l’argent, et tu verras
1217	you find that you know at a point in	qu’à un moment, Sam Altman lui‑même disait :
1218	time Sam Altman himself was saying it's	c’est l’*open AI*. Donc, je donne tout
1219	open AI. Okay, my benefit at the time is	au monde, pour que le monde voie le code,
1220	to give it to the world so that the world	y détecte les erreurs, etc.
1221	looks at it. They know the code if there	—
1222	is if there are any bugs and so on. True	C’est vrai. Et aussi vraie est l’idée que
1223	statement. Also a true statement is if I	si je publie ce code, un criminel pourrait
1224	put it out there in the world, a	l’utiliser contre l’humanité.
1225	criminal might take that model and build	—
1226	something that's against humanity as a	Tout cela est vrai. Mais le capitaliste
1227	result. Also true statement. Capitalists	choisira toujours la vérité
1228	will choose which one of the truths to	qui sert son intérêt, selon
1229	say, right? Based on which part of the	quelle partie de son agenda
1230	agenda, which part of their life today	il veut mettre en avant aujourd’hui.
1231	they want to serve, right? Someone will	—
1232	say, uh, you know, do I do you want me	il y en aura toujours pour dire :
1233	to be controversial?	« Tu veux que je sois provocateur ? »
1234	Let's not go there. But if we go back to	Non, restons calmes. Mais si on reparle
1235	war, I'll give you 400 slogans.	de la guerre, je peux te sortir 400 slogans.
1236	400 slogans that we all hear that change	400 formules qu’on entend, qui changent
1237	based on the day and the army and the	selon la date, l’armée, le contexte…
1238	location and the they're all slogans.	Ce ne sont que des slogans.
1239	None of them is true. You want to know	Aucun n’est vrai. Tu veux connaître
1240	the truth. You follow the money, not	la vérité ? Suis l’argent, pas les mots.
1241	what the person is saying, but ask	Demande‑toi plutôt : pourquoi la personne
1242	yourself why is the person saying that?	dit‑elle cela ? Quel est son intérêt ?
1243	What's in it for the person speaking?	Qu’a‑t‑elle à y gagner ?
1244	>> And what do you think's in it for Chachi	>> Et selon toi, qu’a‑t‑il à y gagner, Sam Altman ?
1245	Sam Altman? hundreds of billions of dollars	Des centaines de milliards de dollars
1246	of of of valuation.	de valorisation.
1247	>> And do you think it's that power?	>> Donc, c’est le pouvoir ?
1248	>> The ego of being the person that	>> L’ego d’être “celui qui a inventé l’AGI”,
1249	invented AGI, the position of power that	la position de pouvoir que ça procure,
1250	this gives you, the meetings with all of	les rencontres avec les chefs d’État,
1251	the heads of states, the admiration that	l’admiration reçue… C’est grisant.
1252	gets it, it is intoxicating	—
1253	>> 100%	>> À 100 % !
1254	100%.	>> Oui, 100 %.
1255	Okay. And and the real question, this is	>> La vraie question, et je la pose à tout le monde :
1256	a question I ask everyone. Did you see	As‑tu vu
1257	you didn't you're every time I ask you	—
1258	you say you didn't. Did you see the	le film *Elysium* ?
1259	movie Elysium?	—
1260	>> No. You'd be surprised how little movie	>> Non. Tu serais surpris de savoir comme je regarde
1261	watching I do. You'd be shocked.	peu de films, vraiment.
1262	>> There are some movies that are very	>> Il y en a pourtant certains très
1263	interesting. I use them to to create an	intéressants. Je m’en sers souvent pour créer
1264	emotional attachment to a story that you	un lien émotionnel avec une idée encore abstraite.
1265	haven't seen yet because you may have	Tu ne l’as pas vécue, mais tu l’as vue au cinéma.
1266	seen it in a movie. Okay. Elissium is a	*Elysium*, c’est une société
1267	is a society where the elites are living	où les élites vivent sur la Lune.
1268	on the moon. Okay. They don't need	Elles n’ont plus besoin des travailleurs.
1269	peasants to do the work anymore and	Et tous les autres vivent ici‑bas.
1270	everyone else is living down here. Okay.	—
1271	You have to imagine that if again game	Imagine, selon la théorie des jeux,
1272	theory you have to im you know picture	ce qui arrive quand on pousse un système
1273	something to infinity to its extreme and	à son extrême : un monde où
1274	see where it goes and the extreme of a	où tout est fabriqué par des machines,
1275	world where all manufactured is done	où toutes les décisions sont prises
1276	manufacturing is done by machines	par des machines,
1277	where all decisions are made by machines	possédées par quelques‑uns,
1278	and those machines are owned by a few	ne ressemble plus en rien
1279	is not an economy similar to the to	à notre économie actuelle.
1280	today to the to today's economy	—
1281	that today's economy is an economy of	Aujourd’hui, c’est une économie
1282	consumerism and and product and	de consommation et de production.
1283	production. You know, it's the it's the	Je l’appelle, dans *Alive*, “l’invention du plus”.
1284	in in alive I call it the invention of	—
1285	more. The invention of more is that post	L’“invention du plus” est née après 1945 :
1286	World War II as the factories were	quand les usines tournaient,
1287	rolling out things and prosperity was	que la prospérité gagnait l’Amérique,
1288	happening everywhere in America. There	il est venu un temps où chaque famille
1289	was a time where every family had enough	avait tout ce qu’il lui fallait.
1290	of everything.	
1291	>> But for the capitalist to continue to be	>> Mais pour que le capitaliste reste rentable,
1292	profitable, they needed to convince you	il fallait te convaincre que ce que tu avais
1293	that what you had was not enough. either	n’était pas suffisant. Soit en rendant les choses
1294	by making it obsolete like fashion or	obsolètes — la mode, les voitures —,
1295	like you know a new shape of a car or	soit en te persuadant qu’il te manque
1296	whatever or by convincing you that there	toujours quelque chose pour être accompli.
1297	are more things in life that you need so	—
1298	that you become complete without those	Sans ces choses, tu ne l’es pas. Et cette
1299	things you don't and and that invention	invention du “plus” nous a menés
1300	of more gets us to where we are today an	jusqu’à aujourd’hui : une économie fondée
1301	economy that's based on production	sur la production
1302	consumed and if you look at the US	et la consommation. Regarde :
1303	economy today 62% of the US economy GDP	62 % du PIB américain,
1304	is consumption it's not production okay	c’est de la consommation, pas de la production.
1305	Now, this requires that the consumers	Or cela suppose que les consommateurs
1306	have enough purchasing power to to buy	aient assez de pouvoir d’achat
1307	what is produced. And I believe that	pour acheter. Je pense que
1308	this will be an economy that will take	cette économie tiendra peut‑être
1309	us hopefully in the next 10, 15, 20	10, 15, 20 ans…
1310	years and forever. But that's not	mais pas forcément éternellement.
1311	guaranteed. Why? Because on one side if	Pourquoi ? Parce que si, d’un côté,
1312	UBI replaces purchasing power. So if	le revenu universel remplace le salaire,
1313	people have to get an income from the	si les gens vivent de revenus pris
1314	government which is basically taxes	sur les impôts — donc sur les profits
1315	collected from those using AI and robots	des entreprises d’IA et de robots —,
1316	to to make things	—
1317	the then the the mindset of capitalism	alors la logique capitaliste :
1318	labor arbitrage means those people are	“le travail doit rapporter”,
1319	not producing anything and they're	fait qu’on les perçoit comme improductifs.
1320	costing me money. Why don't we pay them	Donc on dira : “payons‑les moins,
1321	less and less and maybe even not pay	voire plus du tout.” Et on obtient
1322	them at all? And that becomes Elysium	Elysium :
1323	where you basically say, you know, we	un monde où “nous” sommes à l’abri,
1324	sit somewhere protected from everyone.	nous avons les machines,
1325	We have the machines do all of our work	elles font tout, et les autres
1326	and those need to worry about	se débrouillent. Plus de revenu universel.
1327	themselves. We're not going to pay them	
1328	UBI anymore, right? And and you have to	—
1329	imagine this idea of UBI assumes this	Il faut se rappeler que le revenu universel
1330	very democratic caring society.	suppose une société très démocratique, bienveillante.
1331	UBI in itself is communism.	Le revenu universel, c’est une forme de communisme.
1332	Think of the ideology between at least	Pense à l’idéologie, ou au moins
1333	socialism. The ideology of giving	socialiste : donner à chacun ce dont il a besoin.
1334	everyone what they need. That's not the	Ce n’est pas l’idéologie du capitalisme
1335	capitalist	démocratique occidental.
1336	democratic society that the west	—
1337	advocates. So those transitions are	Ces transitions-là seraient donc immenses.
1338	massive in magnitude.	
1339	And for those transitions to happen, I	Et pour qu’elles aient lieu, je pense
1340	believe the right thing to do when the	que la bonne approche, quand le coût de production
1341	cost of producing everything is almost	de tout devient quasi nul grâce à l’IA
1342	zero because of AI and robots.	et aux robots,
1343	because the cost of harvesting energy	c’est de repenser le modèle. D’autant plus que
1344	should actually tend to zero once we get	l’énergie, elle aussi, tendra vers zéro
1345	more intelligent to harvest the energy	quand nous saurons mieux la recueillir.
1346	out of thin air. Then a possible	Alors un scénario possible,
1347	scenario and and I believe a scenario	même probable dans l’utopie IA,
1348	that AI will eventually do in the utopia	serait : chacun a accès à tout.
1349	is yeah anyone can get anything they	Chacun peut obtenir ce qu’il veut,
1350	want. Don't over consume. We're not	sans surconsommer ni détruire la planète,
1351	going to abuse the the planet resources	parce que ça ne coûte rien.
1352	but it costs nothing. So like the old	Comme autrefois, quand nos ancêtres
1353	days where we were hunter gatherers, you	chasseurs‑cueilleurs prenaient
1354	would, you know, forge for some berries	leurs baies dans la nature,
1355	and you'll find them ready in in nature.	dans dix ou douze ans
1356	Okay, we can in 10 years time, 12 years	on pourrait “cueillir” un iPhone dans la nature,
1357	time build a society where you can forge	car il serait créé à partir de l’air,
1358	for an iPhone in nature. It will be made	grâce à la nanophysique.
1359	out of thin air. Nanopysics will allow	—
1360	you to do that. Okay? But the challenge,	Mais le vrai défi, crois‑le ou non,
1361	believe it or not, is not tech. The	ce n’est pas la technologie, c’est l’état d’esprit :
1362	challenge is a mindset. Because the	pourquoi l’élite te donnerait‑elle cela gratuitement ?
1363	elite, why would they give you that for	
1364	free? Okay. And the system would morph	Le système évoluera plutôt vers plus de profit :
1365	into, no, no, hold on. We will make more	“Non ! Gagnons encore plus !
1366	money. We will be bigger capitalists. We	Soyons de plus grands capitalistes !
1367	will feed our ego and hunger for power	Nourrissons notre ego et notre soif de pouvoir.”
1368	more and more. And for them, give them	Et alors oui, on donnera un revenu universel,
1369	UBI and then 3 weeks later give them	puis trois semaines plus tard moins,
1370	less UBI.	et ainsi de suite.
1371	>> Aren't there going to be lots of new	>> Mais il n’y aura pas plein de nouveaux
1372	jobs created though? Because when we	métiers ? Car au fil des révolutions —
1373	think about the other revolutions over	industrielles, technologiques —
1374	time, whether it was the industrial	—
1375	revolution or other sort of big	on prédisait toujours la disparition
1376	technological revolutions,	des emplois,
1377	>> in the moment we forecasted that	>> mais on ne voyait pas encore
1378	everyone was going to lose their jobs,	ceux qui allaient être créés.
1379	>> but we couldn't see all the new jobs	>>
1380	that were being created	>> Exact. Sauf qu’à ces époques‑là,
1381	>> because the the the machines	les machines remplaçaient la force humaine.
1382	replaced the human strength at a point	—
1383	in time. And very few places in the west	Dans peu d’endroits d’Occident aujourd’hui, un ouvrier
1384	today will have a worker carry things on	transporte encore des charges à dos d’homme
1385	their back and carry it upstairs. The	et les monte à l’étage : c’est la machine
1386	machine does that work. Correct.	qui fait cela. Exact.
1387	>> Yeah.	>> Oui.
1388	>> Uh similarly	>> De même,
1389	AI is going to replace the brain of a	l’IA va remplacer le cerveau humain.
1390	human. And when the west in its	Et quand l’Occident, dans ses “colonies” virtuelles,
1391	interesting uh virtual colonies that I	a externalisé tout son travail
1392	call it uh basically outsourced all	main‑d’œuvre vers les pays en développement,
1393	labor to the to the developing nations.	—
1394	What the West publicly said at the time	l’Occident disait : “Nous deviendrons
1395	is we're we're going to be a services	une économie de services,
1396	economy. We we're we're not interested	où les gens
1397	in making things and stitching things	travaillent avec leur intelligence,
1398	and so on. Let the Indians and Chinese	pas avec leurs mains. Laissons
1399	and you know Bengali and Vietnamese do	les Indiens, Chinois, Bengalis,
1400	that. We're going to do more refined	Vietnamiens fabriquer, et nous,
1401	jobs. Knowledge workers. We're going to	nous ferons des métiers raffinés.”
1402	call them. Knowledge workers are people	Ces “travailleurs du savoir”,
1403	who work with information and click on a	ce sont ceux qui manipulent l’information,
1404	keyboard and move a mouse and you know	écrivent sur un clavier, bougent une souris,
1405	sit in meetings and all we produce in	s’assoient en réunion. Ce que nous produisons
1406	the western societies is what words	en Occident, ce sont des mots,
1407	right or designs maybe sometimes but	parfois des designs, mais tout cela
1408	everything we produce can be produced by	peut être produit par une IA.
1409	an AI.	
1410	So if I give you an AI tomorrow h where	Alors, si je te donne demain une IA,
1411	I give you a piece of land, I give the	et un terrain, je dis à l’IA :
1412	AI a piece of land and I say here are	« Voici ma parcelle, voici les
1413	the parameters of my land. Here is its	informations : elle est ici sur Google Maps.
1414	location on Google maps. Design an	Fais‑moi une villa bien conçue,
1415	architecturally sound villa for me. I	très lumineuse, trois chambres,
1416	care about a lot of light and I need	des salles de bain en marbre blanc”, etc.
1417	three bedrooms. I want my bathrooms to	L’IA le fera instantanément.
1418	be in white marble, whatever. And the AI	
1419	produces it like that. How often will	Du coup, iras‑tu encore souvent
1420	you go to an to an architect and say	chez un architecte ?
1421	right so what will the architect do the	Que fera‑t‑il ? Les meilleurs
1422	best of the best of the architects will	utiliseront l’IA ou te conseilleront :
1423	either use AI to produce that or you	“C’est joli, mais ça ne te
1424	will consult with them and say hey you	correspond pas.” Ces métiers-là
1425	know I've seen this and they'll say it's	resteront, mais combien ?
1426	really pretty but it wouldn't feel right	
1427	for the person that you are yeah those	
1428	jobs will remain but how many of them	
1429	will remain	combien survivront ?
1430	how how often do you think uh how many	Pendant combien d’années encore penses‑tu
1431	more years. Do you think I will be able	pouvoir écrire un livre plus intelligent
1432	to create a book that is smarter than	qu’une IA ?
1433	AI?	—
1434	Not many. I will still be able to	Plus pour longtemps. Je pourrai toujours
1435	connect to a human. You're not going to	toucher l’humain. Tu ne prendras pas
1436	hug an AI when you meet them like you	dans tes bras une IA comme tu me serres la main,
1437	hug me, right? But that's not enough of	mais ce n’est pas un “métier” suffisant
1438	a job.	pour tout le monde.
1439	So why do I say that? Remember I asked	Voilà pourquoi je dis cela. Souviens‑toi,
1440	you at the beginning of the podcast to	au début, je t’ai demandé
1441	remind me of solutions. Why do I say	de me rappeler de parler des solutions.
1442	that? Because there are ideological	Parce qu’il faut des changements idéologiques
1443	shifts and and concrete actions that	et des actions concrètes
1444	need to be taken by governments today	à prendre dès maintenant par les gouvernements,
1445	rather than waiting until COVID is	plutôt que d’attendre la crise — comme pour
1446	already everywhere and then locking	Covid — avant d’agir.
1447	everyone down. Governments could have	Il fallait réagir dès le patient zéro,
1448	reacted before the first patient or at	pas après.
1449	least at patient zero or at least at	
1450	patient 50. They didn't. H what I'm	
1451	trying to say is there is no doubt that	Ce que je veux dire, c’est qu’il est certain
1452	lots of jobs will be lost. There's no	que beaucoup d’emplois vont disparaître.
1453	doubt that there will be sectors of	Certains secteurs perdront
1454	society where 10 20 30 40 50% of all	10, 20, 30, 50 % de leurs effectifs :
1455	developers, all software uh you know all	développeurs, graphistes,
1456	graphic designers, all um uh uh u online	marketeurs, assistants…
1457	marketers, all all all assistances	—
1458	are going to be out of a job. So are we	ils perdront leur emploi. Sommes‑nous prêts,
1459	prepared as a society to do that? Can we	en tant que société, à l’encaisser ?
1460	tell our governments there is an	Pouvons‑nous dire à nos gouvernements :
1461	ideological shift? This is very close to	“Il faut un changement idéologique,
1462	social socialism and and communism.	quelque chose qui s’approche du socialisme,
1463	Okay. And are we ready from a budget	voire du communisme” ?
1464	point of view instead of spending a a	Et budgétairement ? Au lieu de dépenser
1465	trillion dollars a year on on arms and	des milliers de milliards chaque année
1466	and explosives and you know autonomous	en armes, explosifs et drones tueurs,
1467	weapons that will oppress people because	—
1468	we can't feed them? Can we please shift	ne pourrait‑on pas réorienter cela ?
1469	that? I did those numbers. Huh. Uh again	J’ai fait le calcul.
1470	I go back to military spending because	Les dépenses militaires mondiales
1471	it's all around us. 2.71 trillion. 2.4	s’élèvent à environ 2,7 billions $.
1472	to2.7 is the estimate of 2024. how much	—
1473	money we're spending on military	C’est ce qu’on dépense pour se battre.
1474	>> on Yeah. on military equipment on things	>> Oui, en matériel militaire, à faire exploser
1475	that we're going to explode into smoke	en fumée et en mort.
1476	and death. Extreme poverty worldwide.	Or l’extrême pauvreté mondiale,
1477	Extreme poverty is people that are below	celle des gens sous le seuil,
1478	the poverty line. Extreme poverty	pourrait disparaître
1479	everywhere in the world could end for 10	pour seulement 10 à 12 % de ce budget.
1480	to 12% of that budget. So if we replace	En réaffectant 10 % de nos dépenses militaires,
1481	our military spending 10% of that to go	on pourrait éradiquer la misère.
1482	to people who are in extreme poverty,	Plus personne ne serait pauvre.
1483	nobody will be poor in the world. Okay.	—
1484	You can end uh world hunger for less	On pourrait aussi éradiquer la faim
1485	than 4%. Nobody would be hungry in the	pour moins de 4 %.
1486	world. You know, if you take uh again 10	Et avec encore 10 à 12 %,
1487	to 12% universal healthcare, every human	on financerait une santé universelle gratuite
1488	being on the planet would have free	pour chaque être humain.
1489	healthcare for 10 to 12% on what we're	—
1490	spending on war. Now, why why do I say	Alors, pourquoi je parle de ça en discutant de l’IA ?
1491	this when we're talking about AI?	Parce que c’est une simple décision.
1492	Because that's a simple decision. If we	Si nous cessions de nous battre,
1493	stop fighting	—
1494	because money itself does not have the	parce que l’argent n’aura plus le même sens,
1495	same meaning anymore because the	car l’économie de l’argent changera
1496	economics of money is going to change	—
1497	because the entire meaning of capitalism	parce que le capitalisme lui‑même est à bout,
1498	is ending because there is no more need	il n’y aura plus besoin d’arbitrer le travail,
1499	for labor arbitrage because AI is doing	car l’IA fera tout.
1500	everything	—
1501	just with the $2.4 trillion we save in	Avec seulement les 2,4 billions $
1502	explosives every year in arms and	que nous épargnerions chaque année en armes
1503	weapons just for that universal	et explosifs, on pourrait assurer
1504	healthcare and extreme poverty. You	la santé universelle et supprimer la misère.
1505	could actually one of the calculations	on pourrait même, selon un calcul,
1506	is you could end climate or combat	combattre sérieusement le changement
1507	climate climate change meaningfully for	climatique avec
1508	100% of the military budget.	100 % du budget militaire mondial.
1509	>> But I I'm not even sure it's really	>> Mais je ne suis même pas sûr que ce soit vraiment
1510	about the money. I think money is a	une question d’argent. L’argent n’est qu’une
1511	measurement stick of power. Right.	mesure du pouvoir, non ?
1512	>> Exactly. It's printed on demand.	>> Exactement. On en imprime à volonté.
1513	>> So even in a world where we have super	>> Donc, même dans un monde doté d’une
1514	intelligence and money is no longer a	superintelligence, où l’argent n’est plus un problème,
1515	problem.	—
1516	>> Correct.	>> Exact.
1517	>> I still think	>> Je crois tout de même
1518	power is going to be	que la soif de pouvoir restera
1519	insatiable for so many people. So there	insatiable pour beaucoup. Donc il y aura encore
1520	will still be war because you know	des guerres, car tu vois…
1521	>> there will be in my view	>> Oui, à mon avis, il y en aura.
1522	>> the strongest I want the strongest AI. I	>> Le plus fort dira : « Je veux l’IA la plus puissante. »
1523	don't want my	>> Je ne veux pas que ma…
1524	>> and I don't and I don't want you know	>> Et je ne veux pas, tu sais,
1525	what Harry Henry Kissinger called them	ce que Henry Kissinger appelait
1526	the eaters.	les “mangeurs”.
1527	>> The eaters.	>> Les “mangeurs” ?
1528	>> Yeah.	>> Oui.
1529	Brutal as that sounds.	>> Aussi brutal que cela paraisse.
1530	>> What is that? The people at the bottom	>> C’est quoi ? Les gens d’en bas,
1531	of the socioeconomic	ceux des classes sociales
1532	>> that don't produce but consume.	>> qui ne produisent pas mais consomment.
1533	So if you had a Henry Kissinger at the	Alors, si un Henry Kissinger pilotait tout ça,
1534	at the helm and we have so many of them,	et il y en a tant, il penserait :
1535	what would they think like why why	« Pourquoi donc
1536	prominent military figure in the US	nourrir 350 millions d’Américains ?
1537	history? Uh you know what why would we	Et, plus encore, pourquoi se soucier
1538	feed 350 million Americans America will	du Bangladesh,
1539	think but more interestingly why do we	alors qu’on n’y fabrique plus
1540	even care about Bangladesh anymore if we	ni textile ni rien ? »
1541	can't make our textiles there or we	—
1542	don't want to make our textile there. Do	C’est un peu cynique, mais historiquement,
1543	you you know I imagine throughout human	si on avait eu des podcasts,
1544	history if we had podcasts conversations	on aurait mis en garde sans cesse
1545	would would have been warning of a	contre une dystopie imminente.
1546	dystopia around the corner. You know	Tu sais, à chaque innovation,
1547	when they heard of technology and the	le même refrain : “On est fichus.”
1548	internet they would have said oh we're	Lors d’Internet, du tracteur,
1549	finished and when the the tractor came	les gens ont dit : “On est foutus !”
1550	along they would have said oh god we're	—
1551	finished because we're not going to be	on ne pourra plus cultiver…
1552	able to farm anymore. So is this not	N’est‑ce pas simplement un de ces moments
1553	just another one of those moments where	où l’on prédit le pire sans voir au‑delà ?
1554	we couldn't see around the corner so we	—
1555	we forecasted unfortunate things. You	> Tu as peut‑être raison.
1556	could be. I am I'm begging that I'm	Je t’en prie, que j’aie tort !
1557	wrong. Okay. I'm just asking if there	Je demande juste : vois‑tu un scénario
1558	are scenarios that you think that can	où tout cela tournerait bien ?
1559	provide that. You know, uh uh Mustafa	Par exemple, Mustafa Suleyman,
1560	Sulleman in in uh you hosted him here. I	que tu as reçu ici,
1561	did. Yeah. He was in the coming wave.	lui, dans *The Coming Wave*,
1562	>> Yeah.	>> Oui.
1563	>> And he speaks about uh about pessimism	>> Il parle de l’“aversion au pessimisme”.
1564	aversion.	—
1565	Okay. that all of us people who are	En gros, nous autres, gens de la tech
1566	supposed to be in technology and	et du business, sommes censés
1567	business and so on, we're always	monter sur scène et dire :
1568	supposed to, you know, stand on stage	« L’avenir sera incroyable !
1569	and say the future's going to be	Cette technologie va tout améliorer ! »
1570	amazing. You know, this technology I'm	—
1571	building is going to make everything	J’ai écrit un texte appelé “Les promesses brisées”.
1572	better. One of my posts in life was	Combien de fois cela s’est‑il réalisé ?
1573	called the broken promises. How often	—
1574	did that happen?	Combien de fois ?
1575	>> Okay. How often did social media connect	>> Les réseaux sociaux nous ont‑ils “connectés” ?
1576	us? And how many and how often did it	Ou nous ont‑ils rendus plus
1577	make us more lonely? How how often did	seuls ?
1578	mobile phones make us work less? That	Les téléphones nous ont‑ils fait moins travailler ?
1579	was the promise. That was the promise.	C’était la promesse, non ?
1580	The promise. The early ads of Nokia were	Les premières pubs Nokia montraient des soirées.
1581	people at parties. Is that your	C’est ça, ton expérience du mobile ?
1582	experience of mobile phones? And and I	—
1583	think the whole idea is we should hope	J’en conclus qu’il faut espérer que
1584	there will be other roles for humanity.	l’humain garde d’autres rôles.
1585	By the way, those roles would resemble	Et ces rôles ressembleraient
1586	the times where we were hunter	aux temps des chasseurs‑cueilleurs,
1587	gatherers, just a lot more technology	mais avec bien plus de technologie
1588	and a lot more safety.	et de sécurité.
1589	>> Okay. So, this is this sounds good.	>> D’accord, ça sonne bien.
1590	>> Yeah,	>> Oui.
1591	>> this is exciting. So, I'm gonna I'm	>> C’est enthousiasmant. Donc je vais
1592	gonna get to go outside more, be with my	sortir plus, voir mes amis plus souvent,
1593	friends more,	—
1594	>> 100%.	>> À 100 %.
1595	>> Fantastic.	>> Fantastique !
1596	>> And do absolutely nothing.	>> Et ne rien faire du tout.
1597	>> Well, that doesn't sound fantastic.	>> Mm, ça, ça l’est moins.
1598	>> No, it does. Do be forced to do	>> Si, si. Être obligé de ne rien faire,
1599	absolutely nothing. For some people,	pour certains, c’est merveilleux.
1600	it's amazing. For you and I, we're going	Mais toi et moi, on trouvera
1601	to find the little carpentry project and	bien un projet de menuiserie
1602	just do something.	pour s’occuper.
1603	>> Speak for yourself. I'm still People are	>> Parle pour toi ! Moi, les gens
1604	still going to tune in.	écouteront toujours.
1605	>> Okay.	>> D’accord.
1606	>> Correct. Yeah. But what? And people are	>> Oui, c’est vrai. Les gens continueront
1607	going to to tune in.	à écouter.
1608	>> Do you think they will? I'm not I'm not	>> Tu crois ? Je n’en suis pas sûr. Et tant que…
1609	I'm not convinced they will. And for for	
1610	as long	—
1611	>> will you guys tune in? Are you guys	>> Vous, vous écouterez encore ?
1612	still going to tune in?	—
1613	>> I can let them answer. I believe for as	>> Laissons‑les répondre. Je pense que tant que
1614	long as you make their life enriched,	tu enrichis leur vie,
1615	>> but can an AI do that better	>> mais une IA ne le fera‑t‑elle pas mieux ?
1616	>> without the human connection?	>> Sans la connexion humaine ?
1617	>> Comment below. Are you going to listen	>> Mettez‑le en commentaire : écouteriez‑vous
1618	to an AI or the Davosio? Let me know in	une IA ou *The Diary of a CEO* ?
1619	the comment section below.	Dites‑le sous la vidéo.
1620	>> Remember, as incredibly intelligent as	>> Rappelle‑toi, Steve, aussi brillant sois‑tu,
1621	you are, Steve, uh there will be a	il viendra un temps où tu paraîtras
1622	moment in time where you're going to	ridiculement stupide face à l’IA.
1623	sound really dumb compared to an AI. and	Et moi encore plus.
1624	and and I will sound completely dumb.	—
1625	>> Yeah. Yeah.	>> Oui, oui.
1626	>> The the depth the depth of analysis	>> Leur profondeur d’analyse,
1627	and and gold nuggets. I mean, can you	leur richesse… Imagine
1628	imagine two super intelligences deciding	deux super‑intelligences s’unissant
1629	to get together and explain um string	pour expliquer la théorie des cordes.
1630	theory to us?	—
1631	They'll do better than any physic	Elles le feront mieux que n’importe quel
1632	physicist in the world because they	physicien, car elles sauront tout
1633	possess the physics knowledge and they	de la physique, mais aussi
1634	also pro possess social and language	des interactions humaines et du langage,
1635	knowledge that most deep physicists	que les savants n’ont pas.
1636	don't. I think B2B marketeteers keep	Je pense que beaucoup de marketeurs B2B
1637	making this mistake. They're chasing	font cette erreur : viser la quantité
1638	volume instead of quality. And when you	plutôt que la qualité. En cherchant
1639	try to be seen by more people instead of	à plaire à tous au lieu des bons publics,
1640	the right people, all you're doing is	on fait du bruit — souvent cher,
1641	making noise. But that noise rarely	inutile, improductif.
1642	shifts the needle and it's often quite	—
1643	expensive. And I know as there was the	Je l’ai commise moi‑même dans ma carrière.
1644	time in my career where I kept making	Avant de découvrir LinkedIn Ads,
1645	this mistake that many of you will be	—
1646	making it too. Eventually I started	où j’ai commencé à faire ma pub
1647	posting ads on our show sponsors	sur la plateforme de notre sponsor, LinkedIn,
1648	platform LinkedIn. And that's when	et là, tout a changé.
1649	things started to change. I put that	J’attribue ce changement à plusieurs choses,
1650	change down to a few critical things.	—
1651	One of them being that LinkedIn was then	notamment que LinkedIn était et reste
1652	and still is today the platform where	la plateforme des décideurs :
1653	decision makers go to not only to think	là où l’on apprend, mais aussi où l’on achète.
1654	and learn but also to buy. And when you	Quand on y fait du marketing, on est
1655	market your business there, you're	devant ceux qui peuvent dire “oui”.
1656	putting it right in front of people who	—
1657	actually have the power to say yes. and	On cible par poste, secteur, taille d’entreprise.
1658	you can target them by job title,	C’est juste une façon plus fine
1659	industry, and company size. It's simply	d’utiliser ton budget marketing.
1660	a sharper way to spend your marketing	—
1661	budget. And if you haven't tried it, how	Et si tu n’as pas essayé, vas‑y.
1662	about this? Give LinkedIn ads a try, and	Essaye LinkedIn Ads et reçois
1663	I'm going to give you a $100 ad credit	un crédit pub de 100 $.
1664	to get you started. If you visit	Rends‑toi sur
1665	linkedin.com/diary,	linkedin.com/diary
1666	you can claim that right now. That's	pour en profiter dès maintenant.
1667	linkedin.com/diary.	—
1668	I've I've really gone back and forward	>> J’ai longuement réfléchi
1669	on this idea that even in podcasting	à cette idée : même le podcast
1670	that all the podcasts will be AI	pourrait être fait par l’IA.
1671	podcasts or I've gone back and forward	J’ai douté, j’ai pesé… Et ma conclusion,
1672	on it and and where I landed at the end	c’est qu’il existera toujours
1673	of the day was that there'll still be a	des médias où l’expérience vécue compte.
1674	category of media where you do want	—
1675	lived experience on something	Où l’on veut le vécu de quelqu’un.
1676	>> 100%.	>> Tout à fait.
1677	>> For example, like you want to know how	>> Par exemple, savoir comment la personne qu’on admire
1678	the person that you follow and admire	a vécu son divorce.
1679	dealt with their divorce.	—
1680	>> Yeah. Or or how they're struggling with	>> Oui, ou comment elle gère sa confrontation à l’IA,
1681	AI,	par exemple.
1682	>> for example. Yeah. Exactly. But I but I	>> Exactement. Mais je pense
1683	think things like news, there are there	que pour l’actualité, par exemple,
1684	are certain situations where just like	ou des faits bruts,
1685	straight news and straight facts and	l’IA prendra vite le relais.
1686	maybe a walk through history may be	Mais même là,
1687	eroded away by AIS. But even in those	il restera une part
1688	scenarios, you there's something about	de personnalité.
1689	personality. And again, I hesitate	—
1690	here because I question myself. I'm not	Et je me questionne. Je ne suis pas du
1691	in the camp of people that are romantic,	tout dans le camp romantique.
1692	by the way. I'm like I'm trying to be as	Je cherche juste la vérité,
1693	as orientated towards whatever is true,	même si elle me dessert.
1694	even if it's against my interests. And I	J’espère que les gens le comprennent.
1695	hope people understand that about me.	—
1696	like um cuz even in my companies we	D’ailleurs, dans mes boîtes,
1697	experiment with like disrupting me with	on expérimente déjà ma “disparition”
1698	AI and some people will be aware of	via l’IA. Certains le savent.
1699	those experiments	—
1700	>> because there will be a mix of all there	>> Parce qu’il y aura un mixe de tout.
1701	you can't imagine that the world will be	On n’aura pas un monde 100 % IA ni
1702	completely just AI and completely just	100 % humain. Ce sera un mélange.
1703	podcasters you know you'll see a mix of	On verra des IA excellentes dans certains domaines
1704	of both you'll see things that they do	et des humains meilleurs ailleurs.
1705	better things that we do better	—
1706	>> the the the message I'm trying to say is	>> Mon message, c’est qu’il faut se préparer.
1707	we need to prep for that	Il faut anticiper.
1708	>> we need to be ready for that we need to	>> Oui. En parlant à nos gouvernements,
1709	be ready by you know talking to our	en leur disant : « Écoutez,
1710	governments and saying hey it looks like	il semble que mon métier — parajuriste,
1711	I'm a a parallegal and it looks like all	analyste financier, graphiste,
1712	parallegals are going to be, you know,	opérateur de centre d’appel — va disparaître. »
1713	financial researchers or analysts or	—
1714	graphic designers or, you know, call	—
1715	center agents. It looks like half of	Il semble que la moitié
1716	those jobs are being replaced already.	de ces emplois le soient déjà.
1717	>> You know who Jeffrey Hinton is?	>> Tu connais Geoffrey Hinton ?
1718	>> Oh, Jeffrey. I had him on the	>> Oui, Geoffrey. Je l’ai eu dans mon documentaire.
1719	documentary as well. I love Jeffrey.	Je l’adore.
1720	>> Jeffrey Hinton told me	>> Geoffrey Hinton m’a dit :
1721	>> trained to be a plumber.	>> « Forme‑toi à la plomberie. »
1722	>> Really?	>> Sérieusement ?
1723	>> Yeah. 100% for a while.	>> Oui, il l’a dit sérieusement.
1724	>> And I thought he was joking. 100%.	>> Je pensais qu’il plaisantait. Pas du tout.
1725	>> So I asked him again and he he looked me	>> Je lui ai redemandé, il m’a regardé droit dans les yeux
1726	dead in the eye and told me that I I	et m’a dit : “Tu devrais apprendre la plomberie.”
1727	should train to be a plumber.	—
1728	>> 100%. So so so uh it's funny uh machines	>> Tout à fait. C’est drôle : les machines
1729	replaced labor but we still had blue	ont remplacé la force, mais on avait encore des cols bleus.
1730	collar. Then uh you know the refined	Puis sont venus les emplois de “cols blancs”,
1731	jobs became white collar information	les travailleurs de l’information.
1732	workers.	—
1733	>> What's the refined jobs?	>> C’est quoi, “emplois raffinés” ?
1734	>> You know you don't have to really carry	>> Tu sais, ceux où tu ne portes rien de lourd,
1735	heavy stuff or deal with physical work.	où tu ne fais pas d’effort physique.
1736	You know you sit in an in an office and	Tu t’assois dans un bureau, tu fais des réunions
1737	sit in meetings all day and blabber, you	et tu parles beaucoup pour pas grand‑chose :
1738	know, useless shit then that's your	c’est ton boulot.
1739	job. Okay? And those jobs, funny enough,	Et ces emplois‑là, ironiquement,
1740	in the reverse of that, because robotics	avec la robotique encore balbutiante,
1741	are not ready yet. Okay. And I believe	tiennent encore un peu.
1742	they're not ready because of a	À mon avis, cela vient de l’obstination
1743	stubbornness on the on the robotics	du monde de la robotique
1744	community around making them humanoids.	à vouloir faire des humanoïdes.
1745	>> Mhm.	>> Mhm.
1746	>> Okay. Because it takes so much to	>> Car reproduire un geste humain fluide
1747	perfect a human like action at proper	et rapide demande énormément.
1748	speed. You could, you know, have many	On pourrait faire bien plus de robots
1749	more robots that don't look like a human	non humains, comme les voitures autonomes
1750	just like a self-driving car in	en Californie, qui remplacent déjà
1751	California. Okay, that that does already	les chauffeurs. Mais bref, ça prend du temps.
1752	replace drivers and and you know but but	—
1753	they're delayed. So the robotic the the	Donc l’automatisation physique est
1754	replacement of physical manual labor is	retardée : elle prendra encore
1755	going to take four to five years before	4 à 5 ans pour atteindre
1756	it's possible at you know at at the	le niveau actuel de l’IA dans les tâches mentales.
1757	quality of the AI replacing mental labor	
1758	now and when that happens it's going to	Et une fois prête, il faudra longtemps
1759	take a long cycle to manufacture enough	pour fabriquer assez de robots
1760	robots so that they replace all of those	pour tout remplacer.
1761	jobs. that cycle will take longer. Blue	Ce cycle prendra du temps ; les cols bleus
1762	collar will stay longer.	dureront donc un peu plus.
1763	>> So, I should move into blue collar and	>> Donc, je devrais passer au travail manuel et fermer le bureau ?
1764	shut down my office.	—
1765	>> I think you're you're not the problem.	>> Toi, tu n’es pas le problème.
1766	>> Okay, good.	>> Tant mieux.
1767	>> Let's put put it this way. There are	>> Disons que beaucoup d’autres,
1768	many people that we should care about	plus vulnérables qu’un CEO,
1769	that are a simple travel agent or an	vont être touchés : agents de voyage,
1770	assistant	assistants…
1771	that will see if not replacement a	et même sans être remplacés,
1772	reduction in the number of pings they're	verront moins de missions arriver.
1773	getting. Simple as that.	—
1774	And someone in, you know, ministries of	Et quelqu’un, dans les ministères du Travail
1775	labor around the world needs to sit down	partout, doit se demander :
1776	and say, "What are we going to do about	« Que faisons‑nous de ça ?
1777	that? What if all taxi drivers and Uber	Si tous les chauffeurs sont remplacés
1778	drivers in uh in California get replaced	par des voitures autonomes en Californie,
1779	by self-driving cars? Should we start	ne faut‑il pas déjà y réfléchir ? »
1780	thinking about that now, noticing that	—
1781	that trajectory makes it look like a	La trajectoire rend cela probable.
1782	possibility?" I'm going to go back to	—
1783	this argument which is what a lot of	Je reviens à l’argument populaire :
1784	people will be shouting. Yes, but there	“Oui, mais il y aura de nouveaux emplois !”
1785	will be new jobs or	—
1786	>> and I as I said other than human	>> Et j’ai répondu : à part les métiers de lien humain,
1787	connection jobs, name me one.	cites‑en un.
1788	>> So I I've got three assistants, right?	>> J’ai trois assistants, d’accord ?
1789	Sophie, Liam B. And okay, in the near	Sophie, Liam B. À court terme,
1790	term there might be, you know, with AI	avec les agents IA,
1791	agents, I might not need them to help me	je n’aurai peut‑être plus besoin d’eux
1792	book flights anymore. or I might not	pour réserver un vol,
1793	need them to help do scheduling anymore.	ni pour l’agenda.
1794	Or even I've been messing around with	J’expérimente même un outil d’IA
1795	this new AI tool that my friend built	d’un copain : quand on veut fixer
1796	and you basically when me and you trying	un rendez‑vous, je la mets en copie
1797	to schedule something like this today, I	et elle compare nos calendriers
1798	just copy the AI in and it looks at your	puis planifie pour nous.
1799	calendar looks at mine and schedules it	Donc il n’y aura plus besoin
1800	for for us. So there might not be	de planifier. Mais mon chien est malade,
1801	scheduling needs, but my dog is sick at	et en partant ce matin je me disais :
1802	the moment. And as I left this morning,	« Mince, il va vraiment mal. »
1803	I was like, damn, he's like really sick	—
1804	and I've taken him to the vet over and	Je l’ai emmené mille fois chez le véto.
1805	over again. I really need someone to	J’ai besoin de quelqu’un pour s’en occuper
1806	look after him and figure out what's	et comprendre ce qu’il a.
1807	wrong with him. So those kinds of	Ces tâches de soin, oui, resteront.
1808	responsibilities of like care. I don't	—
1809	disagree at all. Again, all	Je suis d’accord.
1810	>> and and I won't I'm not going to be I	>> Et je ne veux pas paraître dur,
1811	don't know how to say this in a nice	mais mes assistants garderont un emploi,
1812	way, but my assistants will still have	autrement : leurs missions seront autres.
1813	their jobs, but I as a CEO will be	—
1814	asking them to do a different type of	Je leur demanderai autre chose.
1815	work.	—
1816	>> Correct. So, so, so this is the	>> Exactement. C’est cela que tout le monde
1817	calculation everyone needs to be aware	doit anticiper : une part de nos tâches
1818	of that a lot of their current	passera aux machines.
1819	responsibility, whoever you are, if	—
1820	you're a parallegal, if you're whatever,	Peu importe ton métier,
1821	will be handed over. So, so let me	une partie sera transférée.
1822	explain it even more accurately. There	Je vais préciser : il y aura deux étapes
1823	will be two stages of our interactions	pre‑IA :
1824	with the machines. One is what I call	l’ère de “l’intelligence augmentée”
1825	the era of augmented intelligence. So,	et celle de “la maîtrise machine”.
1826	it's human intelligence augmented with	—
1827	AI doing the job. And then the following	D’abord, l’humain sera aidé par l’IA、
1828	one is what I call the era of machine	puis, à la seconde, l’IA fera tout seule.
1829	mastery. The job is done completely by	—
1830	an AI without a human in the loop. Okay.	À ce stade, plus d’humain dans la boucle.
1831	So in the era of augmented intelligence,	Dans l’ère de l’intelligence augmentée,
1832	your assistances will augment themselves	tes assistants s’appuieront sur une IA
1833	with an AI to either be more productive.	pour être plus efficaces
1834	>> Yeah.	>> Oui.
1835	>> Okay. Or interestingly to reduce the	>> Ou pour réduire
1836	number of tasks that they need to do.	le nombre de tâches à effectuer.
1837	Correct. Now the more the number of	Mais plus les tâches diminuent,
1838	tasks get reduced, the more they'll have	plus ils auront du temps et la capacité
1839	the bandwidth and ability to do tasks	pour des missions comme
1840	like take care of your dog, right? or	prendre soin de ton chien,
1841	tasks that you know basically is about	accueillir les invités, créer du lien.
1842	meeting your guests or whatever human	—
1843	connection.	—
1844	>> Yeah.	>> Oui.
1845	>> Life connection	>> Des liens humains.
1846	but do you think you need three for that	Mais crois‑tu avoir encore besoin de trois personnes,
1847	or maybe now that some tasks have been	maintenant qu’une partie des tâches
1848	you know outsourced to AI will you need	est confiée à l’IA ? Peut‑être deux ?
1849	two? You can easily calculate that from	On le voit déjà aux centres d’appels :
1850	call center agents. So from call center	on ne licencie pas tout le monde,
1851	agents they're not firing everyone but	mais on confie le premier contact à l’IA.
1852	they're taking the first part of the	—
1853	funnel and giving it to an AI. So	Au lieu de 2 000 agents,
1854	instead of having 2,000 agents in a in a	il en faut 1 800. Je dis un chiffre au hasard,
1855	call center, they can now do the job	mais la société doit penser aux 200 autres.
1856	with 1,800. I'm just making that number	—
1857	up. H society needs to think about the	
1858	200.	
1859	>> And you're telling me that they won't	>> Et tu dis qu’ils ne trouveront pas
1860	move into other roles somewhere else.	d’autres postes ailleurs ?
1861	>> I am telling you I don't know what those	>> Je dis que je ne sais pas lesquels.
1862	roles are.	—
1863	>> Well, I think we should all be	>> Eh bien, on devrait tous devenir
1864	musicians. We should all be authors. We	musiciens, auteurs,
1865	should all be artists. We should all be	artistes, comédiens,
1866	entertainers. We should all be	animateurs… Ce sont des métiers
1867	comedians. We should all these are roles	qui resteront.
1868	that will remain.	—
1869	We should all be plumbers for the next 5	Et plombiers, pour les 5 à 10 ans à venir.
1870	to 10 years. Fantastic. Okay. But even	C’est top. Mais encore faut‑il
1871	that requires society to morph	que la société s’adapte.
1872	and societyy's not talking about it.	Et elle n’en parle pas.
1873	Okay. I had this wonderful interview	J’ai eu cette belle discussion
1874	with friends of mine, Peter Dendez and	avec mes amis Peter Dendez
1875	and some of our friends and and they	et d’autres : ils disaient :
1876	were saying, "Oh, you know, the American	“Oh, les Américains sont résilients,
1877	people are resilient. They're going to	ils deviendront entrepreneurs.”
1878	be entrepreneurs." I was like,	—
1879	seriously, you're expecting a truck	Sérieusement ? Vous croyez qu’un routier
1880	driver that will be replaced by an	remplacé par un camion autonome
1881	autonomous truck to become an	va devenir entrepreneur ?
1882	entrepreneur? Like, please put yourself	Soyez réalistes.
1883	in the shoes of real people,	Mettez‑vous à leur place !
1884	right? You expect a single mother who	Vous pensez qu’une mère célibataire
1885	has three jobs	avec trois boulots…
1886	And I'm not saying this is a dystopia.	Je ne dis pas que ce sera une dystopie,
1887	It's a dystopia if humanity manages it	ça ne le deviendra que si l’humanité s’y prend mal.
1888	badly. Why? Because this could be the	Car cela pourrait tout aussi bien
1889	utopia itself where that single mother	devenir l’utopie : celle où cette femme
1890	does not need three jobs.	n’a plus besoin de trois emplois.
1891	Okay? If we of if of our society was	Si notre société était juste,
1892	just enough, that single mother should	cette mère n’aurait jamais dû
1893	have never needed three jobs,	cumuler trois métiers.
1894	right? But the problem is our capitalist	Mais le problème, c’est notre logique capitaliste,
1895	mindset is labor arbitrage. Is that I	fondée sur l’arbitrage du travail.
1896	don't care what she goes through.	On se moque de ce qu’elle endure.
1897	You know, if if you're if you're	Si tu es bienveillant, tu diras
1898	generous in your assumption, you'll say	qu’elle mérite mieux, que tu as eu de la chance.
1899	because, you know, of what I've been	—
1900	given, I've been blessed. or if you're	Mais si tu es cynique,
1901	mean in your assumption, it's going to	tu diras : “C’est une consommatrice,
1902	be because she's an eater. I'm a a	pas une productrice. Moi, je suis
1903	successful businessman. The world is	un entrepreneur. Je travaille dur,
1904	supposed to be fair. I work hard. I make	je gagne de l’argent. Tant pis pour elle.”
1905	money. We don't care about them.	Et on se désintéresse d’eux.
1906	>> Are we asking of ourselves here	>> Au fond, ne demande‑t‑on pas
1907	something that is not inherent in the	quelque chose de contraire à la nature humaine ?
1908	human condition? What I mean by that is	Je veux dire : si nous sommes assis ici
1909	the reason why me and you are in this my	dans mon bureau —
1910	office here. We're on the fourth or	au troisième ou quatrième étage
1911	third floor of my office in central	d’un immeuble du centre de Londres,
1912	London. big office, 25,000 square feet	un grand bureau de 2 300 m², avec la lumière,
1913	with lights and internet connections and	le Wi‑Fi, des équipes IA au rez‑de‑chaussée —
1914	Wi-Fi and modems and AI teams	—
1915	downstairs. The reason that all of this	si tout cela existe,
1916	exists is because something inherent in	c’est parce que nos ancêtres construisaient,
1917	my ancestors meant that they built and	accomplissaient, étendaient ;
1918	accomplished and grew and that was like	c’était dans leur ADN.
1919	inherent in their DNA. There was	—
1920	something in their DNA that said we will	Il y avait chez eux ce besoin d’expansion
1921	expand and conquer and accomplish. So	et de conquête. Et nous le portons.
1922	that's they've passed that to us because	C’est notre héritage, voilà pourquoi
1923	we're their offspring and that's why we	nous sommes là, dans ces tours.
1924	find ourselves in these skyscrapers.	—
1925	There is truth to that story. It's not	Il y a du vrai, oui. Mais ce n’est pas
1926	your ancestors,	grâce à tes ancêtres à toi.
1927	>> right?	>> Ah non ?
1928	>> What is it?	>> Alors, quoi ?
1929	>> It's the media brainwashing you	>> C’est le lavage de cerveau des médias.
1930	>> really	>> Vraiment ?
1931	>> 100%.	>> 100 %.
1932	>> But if if you look back before times of	>> Pourtant, avant les médias,
1933	media	nos ancêtres Homo sapiens
1934	>> Mhm.	>> Mhm…
1935	>> the reason why homo sapiens were so	>> ont dominé parce qu’ils savaient
1936	successful was because they were able to	se regrouper, communiquer,
1937	dominate other tribes	et dominer d’autres tribus.
1938	>> through banding together and	>> En s’unissant, en coopérant.
1939	communication. They conquered all these	Ils ont conquis toutes les autres espèces
1940	other these other um whatever came	pré‑sapiens, quelles qu’elles soient.
1941	before homo sapiens.	—
1942	>> Yeah. So, so the the reason humans were	>> Oui. Donc, si l’homme a survécu,
1943	successful in my view is because they	c’est parce qu’il savait créer une tribu.
1944	could form a tribe to start. It's not	Pas à cause de son intelligence.
1945	because of our intelligence. I always	Je plaisante souvent : Einstein
1946	joke and say Einstein would be eaten in	se serait fait dévorer dans la jungle
1947	the jungle in 2 minutes.	en deux minutes.
1948	>> Right? You know, the reason why we	>> Oui ! Et s’il a survécu, c’est que
1949	succeeded is because Einstein could	quelqu’un de costaud
1950	partner with a a big guy that protected	s’est associé à lui pour le protéger
1951	him while he was working on relativity	pendant qu’il inventait la relativité.
1952	in the jungle. Right? Now the the the	—
1953	further than that. So so you have to	Mais plus loin encore, la vie est un jeu étrange :
1954	assume that life is a very funny game	elle donne, retire, redonne, retire.
1955	because it provides	—
1956	and then it it deprivives and then it	—
1957	provides and then it deprivives. And for	Et à chaque fois qu’elle retire,
1958	some of us in that stage of deprivation	certains disent :
1959	we try to say okay let's take the other	« Allons piller le voisin ! »
1960	guys you know let's just go to the other	On attaque l’autre tribu.
1961	tribe take what they have or for some of	Et d’autres disent :
1962	us unfortunately we tend to believe okay	« Je suis puissant, tant pis pour vous. »
1963	you know what I'm powerful uh f the rest	—
1964	of you I'm just going to be the boss now	Je serai le chef maintenant.
1965	it's interesting that you	—
1966	you know position this as the condition	C’est intéressant que tu voies cela
1967	of humanity if you really look at the	comme la condition humaine, car,
1968	majority of humans. What do the majority	si tu regardes la majorité des gens,
1969	of humans want?	qu'est-ce qu'ils veulent ?
1970	Be honest. They want to hug their kids.	Honnêtement, ils veulent étreindre leurs enfants.
1971	They want a good meal. Want good sex.	Ils veulent bien manger. Faire l’amour.
1972	They want love. They want, you know, to	Ils veulent de l’amour, du lien,
1973	for most humans, don't measure on you	mais pas, comme toi et moi,
1974	and I. Okay? Don't measure by this	dans une quête dévorante.
1975	foolish person that's dedicated the rest	Je le dis en riant : j’ai consacré ma vie
1976	of his life to to try and warn the world	à prévenir des dangers de l’IA ou à comprendre l’amour.
1977	around AI or, you know, solve uh love	C’est fou, n’est‑ce pas ?
1978	and relationships. That's that's crazy.	C’est dingue.
1979	That's I and I will tell you openly and	Et je te le dis franchement —
1980	you met Hannah, my wonderful wife. It's	tu as rencontré Hannah, ma femme —
1981	the biggest title of this year for me is	la grande question de mon année, c’est :
1982	which of that am I actually responsible	de quoi suis‑je réellement responsable ?
1983	for? Which of that should I do without	Que dois‑je faire sans charge morale ?
1984	the sense of responsibility? Which of	Que dois‑je faire juste parce que je peux ?
1985	that should I do because I can? Which of	Et que dois‑je ignorer complètement ?
1986	I ignore completely? But the reality is	—
1987	most humans, they just want to hug their	La réalité, c’est que la plupart des humains
1988	loved ones. Okay? And if we could give	veulent juste serrer leurs proches.
1989	them that	Et si on leur offrait cela
1990	without the uh you know the the need to	sans devoir trimer 60 heures
1991	work 20 you know 60 hours a week they	par semaine, ils accepteraient sans hésiter.
1992	would take that for sure. Okay. And you	—
1993	and I will think ah but life will be	Toi et moi, on dirait : “Ce serait ennuyeux.”
1994	very boring. To them life will be	Pour eux, ce serait le bonheur.
1995	completely fulfilling. Go to Latin	Va en Amérique latine :
1996	America.	—
1997	Go to Latin America and see the people	va voir les gens là‑bas,
1998	that go work enough to earn enough to	qui travaillent juste assez pour manger,
1999	eat today and go dance for the whole	et dansent toute la nuit.
2000	night. Go to Africa.	Va en Afrique.
2001	Where people are sitting literally on	Où les gens sont littéralement assis sur
2002	you know sidewalks in the street and and	les trottoirs, dans la rue, et
2003	you know completely full of laughter and	pleins de rires et de joie.
2004	joy. We we were lied to the the gullible	On nous a menti, à nous, la majorité crédule,
2005	majority the cheerleaders. We were lied	les supporters enthousiastes. On nous a fait croire
2006	to to to believe that we need to fit as	qu’il fallait devenir un rouage
2007	another gear in that system. But if that	dans ce système. Mais si ce système n’existait pas,
2008	system didn't exist nobody none of us	personne, aucun de nous,
2009	will go wake up in the morning and go	ne se lèverait le matin en disant :
2010	like oh I want to create it. Totally	« Tiens, j’ai envie de créer ça ! »
2011	not. I mean,	Non, pas du tout.
2012	you've touched on it many times today.	Tu l’as souvent souligné aujourd’hui.
2013	We don't need, you know, most people	Nous n’avons pas besoin de ça. Tu sais, la plupart de ceux
2014	that build those things don't need the	qui construisent ces choses n’ont pas besoin d’argent.
2015	money.	—
2016	So, why do they do it though? Because	Alors, pourquoi le font‑ils ? Parce que
2017	homo sapiens were incredible	Homo sapiens était un compétiteur incroyable.
2018	competitors. They outco competed other	Il a surpassé les autres espèces humaines.
2019	human species effectively. So, I'm what	—
2020	I'm saying is is is that competition not	Ce que je veux dire, c’est que cette compétition
2021	inherent in our in our wiring? and and	n’est‑elle pas inscrite dans nos gènes ?
2022	therefore are we are we is it wishful	Et donc, n’est‑ce pas illusoire
2023	thinking to think that we could	de penser qu’on pourrait un jour
2024	potentially pause and say we we okay	s’arrêter et dire : « C’est bon, on a assez »,
2025	this is it we have enough now and we're	et se concentrer sur le plaisir ?
2026	going to	—
2027	focus on just enjoying in my work I call	J’appelle cela, dans mon travail,
2028	that the map mad spectrum okay mut	le spectre MAP‑MAD : la prospérité
2029	mutually assured prosperity versus	assurée mutuellement,
2030	mutually assured destruction destruction	contre la destruction assurée mutuellement.
2031	okay and you really have to start	Et il faut vraiment commencer à y réfléchir,
2032	thinking about about this because in my	car selon moi, nous avons aujourd’hui
2033	mind what we have is the potential for	le potentiel d’offrir à chacun
2034	everyone. I mean you and I today have a	une belle vie. Toi et moi vivons mieux
2035	better life than the queen of England	que la reine d’Angleterre il y a 100 ans.
2036	100 years ago. Correct? Everybody knows	C’est vrai, tout le monde le sait.
2037	that.	—
2038	>> Uh and yet that quality of life is not	>> Et pourtant, cette qualité de vie ne suffit pas.
2039	good enough.	—
2040	>> The truth is like just like you walk	>> La vérité, c’est comme quand tu entres
2041	into a an electronic shop and there are	dans un magasin d’électronique,
2042	60 TVs and you look at them and you go	qu’il y a soixante télés, et tu compares,
2043	like this one is better than that one.	en disant “celle‑ci est meilleure que celle‑là”.
2044	Right? But in reality, if you take any	Mais en réalité, n’importe laquelle chez toi
2045	of them home, it's superior quality to	t’offrira une image bien meilleure
2046	anything that you'll ever need. More	que ce dont tu as besoin.
2047	than anything you you'll ever need. That	C’est exactement notre vie aujourd’hui.
2048	that's the truth of our life today. The	C’est ça, la vérité du monde actuel :
2049	truth of our life today is that there	il ne manque presque plus rien.
2050	isn't much more missing.	—
2051	>> No.	>> Non.
2052	>> Okay. And and when when you know	>> Et quand, tu sais, les Californiens disent
2053	Californians tell us, "Oh, but AI is	« L’IA va augmenter la productivité, résoudre tout ça ! »,
2054	going to increase productivity and solve	—
2055	this." And nobody asked you for that.	Personne ne t’a rien demandé !
2056	Honestly, I never elected you to decide	Je ne t’ai jamais élu pour décider
2057	on my behalf that, you know, getting a	de me faire répondre par une machine
2058	machine to answer me on a call center is	dans un centre d’appels.
2059	better for me. I really didn't. Okay?	Ce n’est pas ce que j’ai voulu.
2060	And and because those unelected	Et, comme ces gens non élus
2061	individuals are making all the	prennent toutes les décisions,
2062	decisions, they're selling those	ils nous les “vendent” par les médias.
2063	decisions to us through what media.	—
2064	Okay? All lies from A to Z. None of it	Ce ne sont que des mensonges, d’un bout à l’autre.
2065	is what you need.	Rien de cela n’est ce dont on a besoin.
2066	And and interestingly, you know me, I I	Et, tu me connais, c’est amusant :
2067	this year I failed. Unfortunately, I	cette année, j’ai échoué.
2068	won't be able to do it. But I normally	Je n’ai pas pu faire ma retraite
2069	do a 40 days silent retreat in nature.	de 40 jours dans la nature.
2070	Okay? And you know what? Even as I go to	Mais tu sais quoi ? Même quand j’y vais,
2071	those nature places, I'm so well trained	je suis tellement conditionné que,
2072	that unless I have a a waitro nearby,	s’il n’y a pas un Waitrose pas loin,
2073	I'm not able to like I I'm I'm in	je n’arrive pas à me détendre.
2074	nature, but I need to be able to drive	Je suis en pleine nature, et pourtant il me faut
2075	20 minutes to get my rice cakes. Like	pouvoir rouler 20 minutes pour des galettes de riz !
2076	what? What? who was taught me that this	Qui m’a appris que c’était ça, bien vivre ?
2077	is the way to live. All of the media	Tous les médias, tous les messages
2078	around me, all of the of the of the	autour de moi
2079	messages that I get all the time, try to	me répètent sans cesse
2080	sit back and say, "What if life had	que “Vivre, c’est ça”. Alors que si la vie offrait
2081	everything?	tout ce qu’il faut,
2082	What if I had everything I needed? I	et si j’avais tout ce dont j’ai besoin ?
2083	could read. I could uh, you know, do my	Je pourrais lire, faire de l’artisanat,
2084	handcrafts and hobbies. I could, you	de la mécanique, restaurer
2085	know, fix my, you know, restore classic	des voitures anciennes, non pour l’argent
2086	cars. Not because I need the money, but	mais pour le plaisir. Je pourrais créer des IA
2087	because it's just a beautiful hobby. I	pour aider les gens dans leurs relations,
2088	could, you know, uh, build AIS to help	et les offrir gratuitement.
2089	people with their long-term committed	—
2090	relationships, but really price it for	—
2091	free. What if	Et si on faisait cela,
2092	What if would you still insist on making	insisterais‑tu encore pour gagner de l’argent ?
2093	money?	—
2094	I think no. I think a few of us will	Je pense que non. Quelques‑uns, oui,
2095	still and they will still crush the rest	et ils écraseront les autres.
2096	of us and hopefully soon the AI will	Mais bientôt, peut‑être, l’IA les écrasera eux.
2097	crush them.	—
2098	Right? That is the problem with your	Voilà le vrai problème de notre monde.
2099	world today. I will tell you hands down	Je peux te le dire franchement :
2100	the problem with with our world today is	le problème, c’est le « A » dans “face RIP”.
2101	the A in face RIPs.	Ce « A », c’est l’accountability,
2102	It's the A in face RIP. It's it's	la responsabilité.
2103	accountability. The problem with our	Le vrai problème, c’est qu’au sommet on ment,
2104	world today, as I said, the top is lying	en bas on croit,
2105	all the time. The bottom is gullible	et personne n’est responsable.
2106	cheerleaders and there is no	—
2107	accountability. You cannot hold anyone	On ne peut aujourd’hui tenir
2108	in our world accountable today. Okay?	personne responsable.
2109	You cannot hold someone that develops an	On ne peut pas demander de comptes à
2110	AI that has the power to completely flip	celui qui crée une IA capable
2111	our world upside down. You cannot hold	de bouleverser le monde.
2112	them accountable and say why did you do	Tu ne peux pas lui demander “pourquoi ?”
2113	this? You cannot hold them accountable	ni lui ordonner d’arrêter.
2114	and tell them to stop doing this. You	Tu vois les guerres, les morts par milliers.
2115	look at the world the wars around the	—
2116	world. Million hundreds of thousands of	Il meurt des centaines de milliers de personnes.
2117	people are dying. Okay. And you know and	Eh bien, la Cour internationale dit :
2118	international court of justice will say	“C’est un crime de guerre.”
2119	oh this is war crimes. You can't hold	Mais personne n’est responsable.
2120	anyone accountable. Okay. You have 51%	—
2121	of the US today is saying stop that	51 % des Américains disent : “Arrêtez !”
2122	51% change their their their lawy their	51 % changent d’avis sur les dépenses de guerre,
2123	view that that their money shouldn't be	—
2124	spent on wars abroad. Okay. You can't	mais rien ne change. Aucun pouvoir citoyen.
2125	hold anyone accountable. Trump can do	Trump fait ce qu’il veut :
2126	whatever he wants. He starts tariffs	il impose des tarifs douaniers
2127	which is against the the constitution of	contrairement à la Constitution,
2128	the US without consulting with the	sans consulter le Congrès.
2129	Congress. You can't hold him	Et personne ne peut le sanctionner.
2130	accountable. They say they're not going	Ils disent : “On ne montrera pas les fichiers Epstein.”
2131	to show the Epstein files. You can't	On ne peut rien y faire.
2132	hold them accountable. It's quite	—
2133	interesting in in Arabic we have that	C’est amusant : en arabe on a un proverbe
2134	proverb that says the highest of your	qui dit : “Monte ton plus noble cheval.”
2135	horses you can go and ride. I'm not	—
2136	going to change my mind. Okay. And	Autrement dit : “Vas‑y, fais le fier, je ne changerai pas d’avis.”
2137	that's truly	—
2138	>> what does that mean?	>> Qu’est‑ce que ça veut dire ?
2139	>> So basically people in the in the old	>> À l’époque, en Arabie, on montait son plus beau cheval
2140	Arabia they would ride the horse to you	pour montrer sa puissance.
2141	know to exert their power if you want.	—
2142	So go ride your highest horse. You're	Donc, “monte ton plus noble cheval” veut dire
2143	not going to change my mind.	“tu ne me feras pas changer d’avis”.
2144	>> Oh okay.	>> Ah, d’accord.
2145	>> Right. And and the truth is that's I	>> Oui. Et c’est exactement ce que nos dirigeants
2146	think that's what our politicians today	ont compris.
2147	have discovered. What our	Nos oligarques, nos géants de la tech
2148	oligarchs have discovered what our uh	l’ont compris aussi :
2149	tech oligarchs have discovered is that I	ils n’ont plus besoin de se soucier
2150	don't even need to worry about the	de l’opinion publique.
2151	public opinion anymore. Okay, at the	—
2152	beginning I would have to say ah this is	Avant, ils disaient : “C’est pour la démocratie, la liberté.”
2153	for democracy and freedom and I have the	“On a le droit de se défendre.”
2154	right to defend myself and you know all	—
2155	of that crap and then eventually when	Mais quand le peuple dit : “Ça suffit, allez trop loin !”,
2156	the world wakes up and says no no hold	ils répondent :
2157	on hold on you're going too far they go	“Monte ton cheval, tu ne me changeras pas.”
2158	like yeah go ride your highest horse I	—
2159	don't care you can't change me there is	Je m’en fiche, tu ne me changeras pas.
2160	no constitution there is no ability for	Il n’y a plus de Constitution, plus aucun moyen
2161	any any citizen to do anything	d’action pour le citoyen.
2162	>> is it possible to have a society where	>> Est‑il possible d’avoir une société
2163	like the one you describe where	comme celle que tu décris,
2164	there isn't hierarchies because it	sans hiérarchie ?
2165	appears to me that humans	Parce qu’il me semble que les humains
2166	assemble hierarchies very very quickly	créent spontanément des hiérarchies,
2167	very naturally and the minute you have a	très vite, naturellement. Dès qu’il y en a une,
2168	hierarchy you have many of the problems	tous ces problèmes reviennent :
2169	that you've described where there's a	haut, bas, pouvoir, impuissance.
2170	top and a bottom and the top have a lot	—
2171	of power and the bottom	—
2172	>> so so the mathematics mathematically is	>> Mathématiquement, c’est fascinant.
2173	actually quite interesting what I call	Il y a ce que j’appelle “la pertinence de base”.
2174	the uh the baseline relevance so so	—
2175	think of it this way say the average	Imaginons une moyenne d’intelligence humaine,
2176	human is an IQ of 100.	un QI de 100.
2177	>> Yeah.	>> Oui.
2178	>> Okay. I tend to believe that when I use	>> OK. Quand j’utilise mes IA aujourd’hui,
2179	my AIS today,	je leur “emprunte” environ 50 à 80 points de QI.
2180	I borrow around 50 to 80 IQ points. I	—
2181	say that because I've worked with people	J’ai déjà travaillé avec des gens ayant 80 points de plus,
2182	that had 50 to 80 IQ points more than	et je vois la différence.
2183	me. And I now can see that I can sort of	Maintenant, avec une IA, j’arrive presque à leur niveau.
2184	stand my my place.	—
2185	50 50 IQ points, by the way, is enormous	50 points de QI, c’est énorme.
2186	because IQ is exponential. So the the	Le QI est exponentiel : les derniers 50 sont plus puissants
2187	last 50 are bigger than my entire IQ,	que les 100 premiers.
2188	right?	—
2189	If I borrow 50 IQ points on top of say	Si j’ajoute 50 points à mes 100, c’est +30 %.
2190	100 that I have, that's 30%. If I can	Si j’en emprunte 100, c’est +50 %.
2191	borrow 100 IQ, that's 50%. That	Donc j’ai presque doublé mon intelligence.
2192	that's so, you know, basically doubling	—
2193	my intelligence. But if I can borrow	Mais si je peux emprunter 4 000 points
2194	4,000 IQ points	d’ici 3 ans,
2195	in 3 years time, my IQ itself, my base	alors mon QI “de base” devient insignifiant.
2196	is irrelevant. Whether you are smarter	Qu’importe que tu aies 20 ou 50 points de plus.
2197	than me by 20 or 30 or 50 which in our	Aujourd’hui, cela fait une différence,
2198	world today made a difference	demain, plus aucune.
2199	in the future if we can all augment with	Quand tous seront augmentés de 4 000 points,
2200	4,000 I end up with 4,100 another ends	j’aurai 4 100, toi 4 130 : égalité.
2201	up with 400 4, you know 130 really	—
2202	doesn't make much difference. Okay. And	La différence deviendra négligeable.
2203	because of that the difference between	Et dès lors, la différence entre
2204	all of humanity and the augmented	l’humain et l’intelligence augmentée
2205	intelligence	—
2206	is going to be irrelevant. So all of us	n’aura plus de sens. Nous serons tous égaux.
2207	suddenly become equal and and this also	Et sur le plan économique aussi.
2208	happens economically. All of us become	Nous deviendrons tous des paysans.
2209	peasants.	—
2210	And I never wanted to tell you that	Je ne voulais pas te le dire, au risque de t’affoler,
2211	because I think it will make you run	parce que tu te mettrais à courir plus vite.
2212	faster. Okay? But unless you're in the	Mais à moins d’être dans le top 0,1 %,
2213	top.1%,	tu es un paysan.
2214	you're a peasant. There is no middle	Il n’y aura plus de classe moyenne.
2215	class. There is, you know, if a CEO can	Si un CEO peut être remplacé par l’IA,
2216	be replaced by an AI, all of our middle	alors toute la classe moyenne disparaît.
2217	class is going to disappear.	—
2218	>> What are you telling me?	>> Qu’est‑ce que tu veux dire ?
2219	All of us will be equal and it's up to	Nous serons tous égaux, et ce sera à nous
2220	all of us to create the society that we	de créer la société dans laquelle
2221	want to live in	nous voulons vivre.
2222	>> which is a good thing	>> Ce qui est une bonne chose.
2223	>> 100%. But that society is not	>> 100 %. Mais cette société ne sera pas capitaliste.
2224	capitalism.	—
2225	>> What is it?	>> Et ce sera quoi, alors ?
2226	>> Unfortunately, it's much more socialism.	>> Malheureusement, ce sera bien plus socialiste,
2227	It's much more hunter gatherer. Okay.	beaucoup plus proche des chasseurs‑cueilleurs.
2228	It's much more communionike if you want.	Une société presque “communielle”, si tu veux,
2229	This is a society where humans connect	où les humains se relient entre eux,
2230	to humans, connect to nature, connect to	à la nature, à la terre, au savoir,
2231	the land, connect to knowledge, connect	et à la spiritualité.
2232	to spirituality. H where all that we	—
2233	wake up every morning worried about	Et tout ce qui nous inquiète aujourd’hui
2234	doesn't feature anymore	n’existera plus.
2235	and it's a it's a better world believe	Et crois‑moi, ce sera un monde meilleur.
2236	it or not	—
2237	>> and are you	>> Et tu crois qu’on va y arriver ?
2238	>> we have to transition to it	>> Il faut y passer, oui.
2239	>> okay so in such a world which I guess is	>> D’accord. Donc, dans un tel monde, ton utopie,
2240	your version of the utopia that we can	quand je me lèverai le matin, je ferai quoi ?
2241	get to when I wake up in the morning	
2242	what do I do	—
2243	>> what do you do today	>> Que fais‑tu aujourd’hui ?
2244	>> I woke up this morning I spent a lot of	>> Ce matin, j’ai passé beaucoup de temps avec mon chien,
2245	time with my dog cuz my dog is sick as	parce qu’il est malade.
2246	>> you're going to do that too	>> Tu pourras toujours faire ça.
2247	>> yeah I was stroking him a lot and then I	>> Oui, je l’ai beaucoup caressé,
2248	fed him and he sick again and I just	je l’ai nourri, il a encore vomi,
2249	thought, "Oh god." So I spoke to the	je me suis dit : “Mon dieu”, puis j’ai appelé le véto.
2250	vet.	—
2251	>> You spend a lot of time with your other	>> Tu passeras aussi du temps avec ton autre chien.
2252	dog. You can do that, too.	Tu pourras faire pareil.
2253	>> Okay.	>> D’accord.
2254	>> Right.	>> Voilà.
2255	>> But then I was very excited to come	>> Mais ensuite, j’étais très heureux de venir ici,
2256	here, do this, and after this I'm going	faire cette interview. Après, je vais travailler.
2257	to work. It's Saturday, but I'm going to	C’est samedi, mais j’irai au bureau.
2258	go downstairs in the office and work.	—
2259	>> Yeah. So six hours of the day so far are	>> D’accord, donc six heures de ta journée pour tes chiens et moi.
2260	your dogs and me.	—
2261	>> Yeah.	>> Oui.
2262	>> Good. You can do that still.	>> Très bien. Tu pourras continuer à faire ça.
2263	>> And then build my business.	>> Et ensuite développer mon entreprise.
2264	>> You You may not need to build your	>> Tu n’auras peut‑être plus besoin de la développer.
2265	business,	—
2266	>> but I enjoy it.	>> Mais j’aime ça.
2267	>> Yeah. Then do it. If you enjoy it, do	>> Alors fais‑le. Si tu aimes ça, fais‑le.
2268	it. You may wake up and then, you know,	Tu pourras aussi te lever, plutôt faire du sport,
2269	instead of building your business, you	lire, jouer, ou parler avec une IA
2270	may invest in your body a little more,	pour apprendre quelque chose.
2271	go to the gym a little more, go play a	Ce n’est pas une mauvaise vie.
2272	game, go read a book, go prompt an AI	C’est la vie de nos grands‑parents.
2273	and learn something. It's not a horrible	—
2274	life. It's the life of your	—
2275	grandparents.	—
2276	It's just two generations ago where	C’était il y a deux générations :
2277	people went to work before the invention	les gens travaillaient avant l’inflation du “toujours plus”.
2278	of more. Remember, huh? people who who	Tu te rappelles ? Ceux qui ont commencé dans les 50s–60s
2279	started working in the 50s and 60s, they	—
2280	worked to make enough money to live a	travaillaient juste de quoi vivre correctement,
2281	reasonable life, went home at 5:00 p.	rentraient à 17 h,
2282	p.m. had tea with their with their loved	prenaient le thé en famille,
2283	ones, had a wonderful dinner around the	dînaient tous ensemble,
2284	table, did a lot of things, you know, uh	passèrent la soirée à vivre,
2285	for the rest of the evening and enjoyed	et profitaient de la vie.
2286	life.	—
2287	>> Some of them	>> Certains, oui,
2288	>> in the 50s and 60s, there were still	>> dans les années 50‑60, il y avait encore
2289	people that were	—
2290	>> correct. And I think it's a very	>> c’est vrai. Et je trouve la question fascinante :
2291	interesting question.	—
2292	uh how many of them and I really really	combien ? Je me demande sincèrement
2293	am I actually wonder if people will tell	si 99 % des gens
2294	me do we think that 99% of the world	seraient incapables de vivre sans travailler,
2295	cannot live without working or that 99%	ou si 99 % en seraient heureux.
2296	of the world would happily live without	
2297	working	—
2298	>> what do you think	>> Qu’en penses‑tu ?
2299	>> I think if we if you give me other	>> Je pense que si tu donnes une autre raison d’être
2300	purpose	que “le travail”,
2301	you know we we defined our purpose as	—
2302	work that's a capitalist lie	c’est un mensonge du capitalisme.
2303	>> was there ever a time in human history	>> A‑t‑il déjà existé, dans l’histoire humaine,
2304	where our purpose wasn't work	un temps où notre but n’était pas le travail ?
2305	>> 100%.	>> Oui, bien sûr.
2306	>> When was that?	>> Quand ?
2307	>> All through human history until the	>> Pendant toute l’Histoire,
2308	invention of Moore.	jusqu’à l’invention du “toujours plus”.
2309	>> I thought my ancestors were out hunting	>> Je pensais que mes ancêtres chassaient toute la journée.
2310	all day.	—
2311	>> No, they went out hunting once a week.	>> Non, ils chassaient une fois par semaine,
2312	They fed the tribe for the week. They	ils nourrissaient la tribu,
2313	gathered for a couple of hours every	se rassemblaient quelques heures par jour.
2314	day. Farmers, you know, saw the seeds	Les paysans plantaient puis attendaient des mois.
2315	and and waited for months at on end.	—
2316	>> What did they do with the rest of the	>> Et ils faisaient quoi le reste du temps ?
2317	time?	—
2318	>> They connected as humans. They explored.	>> Ils se connectaient entre humains, exploraient,
2319	They uh were curious. They discussed	étaient curieux, parlaient des étoiles,
2320	spirituality and the stars. They they	de spiritualité. Ils vivaient,
2321	they lived. They hugged. They made love.	s’embrassaient, faisaient l’amour, vivaient.
2322	They lived.	—
2323	>> They killed each other a lot.	>> Ils se tuaient aussi beaucoup.
2324	>> They they still kill each other today.	>> Ils se tuent encore aujourd’hui.
2325	>> Yeah. That's what I'm saying. So	>> Oui, c’est bien ça que je dis.
2326	>> to take that out of the equation, if you	>> Si on met ça de côté,
2327	look at how	—
2328	>> and by the way that actually that	>> d’ailleurs cette remarque rejoint
2329	statement again, one of the of the 25	une de mes “25 astuces” sur la vérité :
2330	tips I talk about uh to to tell	les mots comptent.
2331	the truth is words mean a lot. No,	Les humains ne se tuaient pas “beaucoup”.
2332	humans did not kill each other a lot.	C’étaient quelques chefs, généraux,
2333	Very few generals instructed humans or	ou chefs de tribus,
2334	tribe leaders instructed lots of humans	qui ordonnaient aux autres de s’entretuer.
2335	to kill each other. But if you leave	Mais laissés seuls,
2336	humans alone, I tend to believe 99 98%	je crois que 98 % des gens
2337	of the people I know, let me just take	n’iraient jamais frapper
2338	that sample, wouldn't hit someone in the	quelqu’un au visage.
2339	face. And if someone attempted to hit	Et s’ils étaient attaqués, ils se défendraient
2340	them in the face, they'd defend	sans riposter.
2341	themselves but wouldn't attack back.	—
2342	Most humans are okay. Most of us are	La plupart des humains sont bons.
2343	wonderful beings.	Nous sommes, pour la majorité, formidables.
2344	Most of us have no,	—
2345	you know, yeah, you know, most people	La plupart n’ont pas besoin d’une Ferrari.
2346	don't don't need a Ferrari. They want a	—
2347	Ferrari because it gets sold to them all	On leur en vend le rêve sans cesse.
2348	the time. But if there were no Ferraris	Mais s’il n’y avait pas de Ferrari
2349	or everyone had a Ferrari, people	ou si tout le monde en avait une,
2350	wouldn't care.	personne n’en voudrait plus.
2351	Which, by the way, that is the world	Et c’est précisément le monde qui arrive :
2352	we're going into. There will be no	il n’y aura plus de Ferrari,
2353	Ferraris or everyone had Ferraris,	ou tout le monde en aura.
2354	right? n you know the majority of	—
2355	humanity will never have the income on	La majorité vivra d’un revenu universel
2356	UBI to to buy something super expensive.	insuffisant pour le superflu.
2357	Only the very top guys in Elysium will	Seuls les plus riches, dans leur “Elysium”,
2358	be you know driving cars that are made	auront des voitures faites pour eux
2359	for them by the AI or not even driving	par l’IA — ou ne conduiront plus.
2360	anymore. Okay. Or	—
2361	you know again sadly ide from an	Encore une fois, idéologiquement,
2362	ideology point of view it's a strange	c’est étrange, mais on aura
2363	place but you'll get communism that	un communisme qui fonctionne.
2364	functions.	—
2365	The problem with communism is that	Le problème du communisme, c’est qu’il n’a jamais
2366	didn't it didn't function. It didn't	fonctionné, qu’il n’a pas su pourvoir
2367	provide for for its society. But the	aux besoins de la société.
2368	concept was you know what everyone gets	Mais son idée, que tout le monde ait
2369	their needs. And I don't say that	de quoi vivre, n’est pas mauvaise.
2370	supportive of either society. I don't	—
2371	say that because I dislike capitalism. I	Je ne dis pas ça par anti‑capitalisme.
2372	always told you I'm a capitalist. I want	Je t’ai toujours dit : je suis capitaliste.
2373	to end my life with 1 billion happy and	Je veux finir ma vie avec un milliard de gens heureux,
2374	I use capitalist methods to get there.	et j’utilise des méthodes capitalistes pour y arriver.
2375	The objective is not dollars. The	L’objectif n’est pas l’argent,
2376	objective is number of happy people.	mais le nombre de gens heureux.
2377	>> Do you think there'll be My girlfriend,	>> Tu penses que… Ma petite amie — elle a toujours raison —
2378	she's always bloody right. I've said	je l’ai déjà dit dans ce podcast.
2379	this a few times on this podcast. If	Si tu écoutes souvent, tu m’as déjà entendu le dire :
2380	you've listened before, you've probably	je ne lui dis pas assez sur le moment,
2381	heard me say this. I don't tell her	mais après coup, je constate qu’elle avait raison.
2382	enough in the moment, but I figure out	—
2383	from speaking to experts that she's so	Et tous les experts que j’ai reçus me confirment
2384	fucking right. She like predicts things	qu’elle a souvent raison : elle prédit les choses
2385	before they happen. And one of her	avant qu’elles n’arrivent.
2386	predictions that she's been saying to me	Une de ses prédictions, depuis deux ans, c’est
2387	for the last two years, which in my head	—
2388	I've been thinking now, I don't I don't	au début je n’y croyais pas vraiment,
2389	believe that, but now maybe I'm thinking	mais maintenant je me dis : peut‑être qu’elle a raison.
2390	she's tr she's telling the truth. I hope	J’espère qu’elle écoutera cet épisode.
2391	she's going to listen to this one is she	—
2392	keeps saying to me, she's been saying	Elle ne cesse de me dire depuis deux ans
2393	for the last two years, she was there's	qu’il va y avoir une grande division
2394	going to be a big split in society. She	dans la société.
2395	was and the way she describes it is	Elle la décrit ainsi :
2396	she's saying like there's going to be	il y aura deux groupes :
2397	two groups of people. the people that	ceux qui choisiront un mode de vie
2398	split off and go for this almost	chasseur‑cueilleur, connecté, communautaire,
2399	huntergatherer	—
2400	community centric connection centric	centré sur la communauté et le lien humain,
2401	utopia and then there's going to be this	et les autres,
2402	other group of people who pursue	ceux qui poursuivront la technologie,
2403	you know the technology and the AI and	l’IA, l’optimisation,
2404	the optimization and get the brain chips	les implants cérébraux —
2405	cuz like there's nothing on earth that's	il n’y a rien au monde
2406	going to persuade my girlfriend to get	qui convaincra ma copine d’en avoir un.
2407	the computer brain chips%	—
2408	>> but there will be people that go for it	>> Mais certains le feront, eux.
2409	and they'll have the highest IQs and	Ils auront les QI les plus hauts,
2410	they'll be the most productive by	seront les plus productifs selon
2411	whatever objective measure of	n’importe quelle mesure possible,
2412	productivity you want to apply and she's	et elle est convaincue qu’il y aura
2413	very convinced there's going to be this	une scission de la société.
2414	splitting of society.	—
2415	>> So there was there was a I don't know if	>> Justement, il y a — je ne sais pas si tu connais —
2416	you had Hugo Dearis here.	Hugo De Garis.
2417	>> No.	>> Non.
2418	>> Yeah. A very very very renowned	>> Oui, un informaticien très connu, un peu excentrique,
2419	eccentric uh computer scientist who	qui a écrit un livre intitulé *The Artilect War*.
2420	wrote a book called the Arctic War and	—
2421	the Arctic War was basically around you	La “guerre des artilects” décrit
2422	know how we it's not going to first it's	non pas une guerre humains vs IA,
2423	not going to be a war between humans and	mais entre les partisans de l’IA
2424	AI. It will be a war between people who	et ceux qui n’en veulent plus.
2425	support AI and people who sort of don't	—
2426	want it anymore. Okay? And and it is and	Et ce sera nous contre nous,
2427	and it will be us versus each other	—
2428	saying should we allow AI to take all	en débat : faut‑il laisser l’IA prendre tous les emplois,
2429	the jobs or should we you know some	ou non ?
2430	people will support that very much and	Certains diront oui, pour en bénéficier,
2431	say yeah absolutely and so you know we	et d’autres diront non :
2432	will benefit from it and others will say	pourquoi ne pas en garder 40 % ?
2433	no why why we don't need any of that why	—
2434	don't we keep our jobs and let AI do 60%	l’IA ferait 60 %, nous travaillerions moins,
2435	of the work and all of us work 10our	10 heures par semaine : belle société, non ?
2436	weeks and it's a beautiful society by	—
2437	the way that's a possibility so a	C’est une possibilité, si la société s’éveille.
2438	possibility if society awakens is to say	On pourrait dire : gardons nos emplois,
2439	okay everyone still keeps their job, but	aidés par l’IA, le travail devient plus léger.
2440	they're assisted by an AI that makes	—
2441	their job much easier. So, it's not, you	littéralement, plus dur labeur.
2442	know, this uh this hard labor that we do	—
2443	anymore, right? It's a possibility. It's	C’est possible, c’est un état d’esprit.
2444	just a mindset. A mindset that says in	Un état d’esprit où le capitaliste continue
2445	that case, the capitalist still pays	à payer tout le monde,
2446	everyone.	—
2447	Uh they still make a lot of money. The	et gagne encore beaucoup.
2448	business is really great, but everyone	Les affaires vont bien,
2449	that they pay has purchasing power to	mais chacun conserve un pouvoir d’achat,
2450	keep the economy running. So,	ce qui fait tourner l’économie.
2451	consumption continues, so GDP continues	La consommation continue, le PIB aussi.
2452	to grow. It's a beautiful setup,	C’est un beau modèle.
2453	but that's not the capitalist labor	Mais ce n’est pas celui du capitalisme
2454	arbitrage.	basé sur l’arbitrage du travail.
2455	>> But also, when you're competing against	>> Oui, mais quand tu es en concurrence
2456	other nations	avec d’autres nations,
2457	>> and other competitors and other	>> d’autres entreprises,
2458	businesses,	—
2459	>> whichever nation is most brutal and	>> celle qui sera la plus brutale,
2460	drives the highest gross margins, gross	avec les marges les plus hautes,
2461	profits is going to be the nation that	sera celle qui l’emportera.
2462	>> So, there are examples in the world,	>> Il existe pourtant des exemples où,
2463	this is why I say it's the map mad	et c’est pour ça que je parle du spectre MAP‑MAD,
2464	spectrum. There are examples in the	on a compris la “destruction mutuelle assurée” :
2465	world where when we recognize mutually	alors on décide de changer.
2466	assured destruction, okay, we we decide	—
2467	to shift. So nuclear threat for the	La menace nucléaire, par exemple,
2468	whole world makes nations across nations	a forcé les nations à coopérer.
2469	makes nations work together, right? By	—
2470	saying, hey, by the way, prolification	Elles ont dit : “La prolifération nucléaire
2471	of nuclear weapon is not weapons is not	n’est pas bonne pour l’humanité.”
2472	good for humanity. Let's all of us limit	Limitons‑la ensemble.
2473	it. Of course, you get the rogue player	—
2474	that, you know, doesn't want to sign the	Il y aura toujours des acteurs rebelles
2475	agreement and wants to continue to to	qui refusent les traités,
2476	have that, you know, that that weapon in	qui gardent leurs armes,
2477	their arsenal. Fine. But at least the	soit. Mais au moins,
2478	rest of humanity agrees that if you have	le reste de l’humanité s’entend :
2479	a nuclear weapon, we're part of an	“
2480	agreement between us. Mutually assured	nous sommes liés, tous, dans le danger commun”.
2481	prosperity, you know, is the CERN	La “prospérité mutuelle assurée”, c’est le CERN :
2482	project. CERN is too too complicated for	trop complexe pour un seul pays,
2483	any nation to build it alone. But it is	mais si utile à la science
2484	really, you know, a very useful thing	que tous envoient leurs chercheurs
2485	for physicists and for understanding	pour collaborer et partager.
2486	science. So all nations send their	—
2487	scientists all collaborate and everyone	Tout ceci est possible : ce n’est qu’un état d’esprit.
2488	uses the outcome. It's possible. It's	—
2489	just a mindset. The only barrier between	La seule barrière entre l’utopie et la dystopie,
2490	a hum, you know, a utopia for humanity	c’est cette mentalité capitaliste :
2491	and AI and the dystopia we're going	—
2492	through is is a capitalist mindset.	la soif de puissance, la cupidité, l’ego.
2493	That's the only barrier. Can you believe	C’est la seule barrière. Tu le crois ?
2494	that? It's hunger for power, greed, ego,	C’est la faim de pouvoir, la gourmandise, l’ego.
2495	>> which is inherent in humans.	>> Ce qui est inhérent à l’être humain.
2496	>> I disagree. especially humans that live	>> Je ne suis pas d’accord. Surtout pas pour ceux qui vivent
2497	on other islands.	sur d’autres îles.
2498	>> I disagree. If you ask, if you take a	>> Je ne suis pas d’accord. Si tu demandes, si tu fais un sondage
2499	poll across everyone watching, okay,	auprès de tous ceux qui regardent,
2500	would they prefer to have a world where	préféreraient‑ils un monde où il y a un seul tyran
2501	there is one tyrant, you know, running	qui nous dirige tous,
2502	all of us, or would they prefer to have	ou un monde où nous vivons tous en harmonie ?
2503	a world where we all have harmony?	—
2504	>> I completely agree, but they're two	>> Tout à fait d’accord, mais ce sont deux choses différentes.
2505	they're two different things. What I'm	Ce que je veux dire, c’est que je sais que
2506	saying is I know that that's what the	c’est ce que le public dirait qu'il veut,
2507	audience would say they want, and I'm	et je suis sûr que c’est sincère,
2508	sure that is what they want, but the	mais la réalité des êtres humains a prouvé le contraire
2509	reality of human beings is through	tout au long de l’histoire.
2510	history proven to be something else.	—
2511	Like, you know, if think about the	Regarde les gens qui dirigent le monde en ce moment,
2512	people that lead the world at the	—
2513	moment, is that what they would say?	penses‑tu que ce soit ce qu’ils diraient vouloir ?
2514	>> Of course not.	>> Bien sûr que non.
2515	>> And they're the ones that are	>> Et ce sont eux qui influencent tout.
2516	influencing.	—
2517	>> Of course not. Of course not. But you	>> Bien sûr que non. Mais ce qui est drôle,
2518	know what's funny? I'm the one trying to	c’est que c’est moi qui essaie d’être positif,
2519	be positive here and you're the one that	et c’est toi qui sembles avoir abandonné l’idée d’espérer
2520	has given up on on human.	en l’humanité.
2521	>> It's not. It's Do you know what it is?	>> Ce n’est pas ça. Tu sais ce que c’est ?
2522	It goes back to what I said earlier,	Ça revient à ce que je disais tout à l’heure :
2523	which is the pursuit of what's actually	la recherche de ce qui est vrai, sans concession.
2524	true irrespective. I'm with you. That's	Je suis d’accord avec toi : c’est pour ça que je crie au monde
2525	why I'm screaming for the whole world	—
2526	because still today in this country that	parce qu’encore aujourd’hui, dans ce pays qui se dit démocratique,
2527	claims to be a democracy. If everyone	si tout le monde disait :
2528	says, "Hey, please sit down and talk	« Hé, asseyons‑nous et parlons‑en »,
2529	about this."	—
2530	There will be a shift. There will be a	il y aurait un basculement, un vrai changement.
2531	change.	—
2532	>> AI agents aren't coming. They are	>> Les agents d’IA ne sont pas “en train d’arriver” : ils sont déjà là.
2533	already here. And those of you who know	Et ceux qui sauront les utiliser
2534	how to leverage them will be the ones	seront ceux qui changeront le monde.
2535	that change the world. I spent my whole	J’ai passé toute ma carrière d’entrepreneur à regretter
2536	career as an entrepreneur regretting the	de ne pas avoir appris à coder.
2537	fact that I never learned to code. AI	Les agents d’IA changent complètement cela.
2538	agents completely change this. Now, if	Maintenant, si tu as une idée et un outil comme Replit,
2539	you have an idea and you have a tool	tu peux la transformer en réalité en quelques minutes.
2540	like Replet, who are a sponsor of this	Replit, qui sponsorise ce podcast,
2541	podcast, there is nothing stopping you	te permet de créer tout ce que tu veux, sans obstacle.
2542	from turning that idea into reality in a	—
2543	matter of minutes. With Replet, you just	Il suffit de taper ce que tu veux créer,
2544	type in what you want to create and it	et l’IA le construit pour toi.
2545	uses AI agents to create it for you. And	—
2546	now I'm an investor in the company as	Je suis d’ailleurs investisseur dans la société,
2547	well as them being a brand sponsor. You	en plus d’avoir leur parrainage.
2548	can integrate payment systems or	Tu peux intégrer des systèmes de paiement,
2549	databases or loginins. Anything that you	des bases de données ou une page de connexion : tout est possible.
2550	can type. Whenever I have an idea for a	Chaque fois que j’ai une idée de site, d’outil ou d’application,
2551	new website or tool or technology or	je vais sur replit.com et je tape ce que je veux :
2552	app, I go on replet.com and I type in	—
2553	what I want. A new to-do list, a survey	une liste de tâches, un formulaire, un site personnel –
2554	form, a new personal website. Anything I	n’importe quoi que je tape, je peux le créer.
2555	type, I can create. So, if you've never	—
2556	tried this before, do it now. Go to	Si tu n’as jamais essayé, fais‑le maintenant.
2557	replet.com and use my code Steven for	Va sur replit.com et utilise mon code STEVEN
2558	50% off a month of your Replet call	pour obtenir 50 % de réduction pendant un mois
2559	plan. Make sure you keep what I'm about	sur ton forfait Replit. Et garde pour toi ce que je vais dire :
2560	to say to yourself. I'm inviting 10,000	j’invite 10 000 d’entre vous
2561	of you to come even deeper into the	à plonger plus profondément dans *The Diary of a CEO*.
2562	diary of a CEO. Welcome to my inner	Bienvenue dans mon cercle intérieur.
2563	circle. This is a brand new private	Il s’agit d’une nouvelle communauté privée que je lance.
2564	community that I'm launching to the	—
2565	world. We have so many incredible things	Nous avons tant d’éléments incroyables que vous ne voyez jamais :
2566	that happen that you are never shown. We	les notes sur mon iPad pendant les entretiens,
2567	have the briefs that are on my iPad when	des extraits inédits, des discussions en coulisses,
2568	I'm recording the conversation. We have	et même des épisodes jamais publiés.
2569	clips we've never released. We have	—
2570	behindthe-scenes conversations with the	—
2571	guest and also the episodes that we've	—
2572	never ever released. and so much more.	Et encore beaucoup d’autres contenus.
2573	In the circle, you'll have direct access	Dans ce cercle, vous aurez un accès direct à moi.
2574	to me. You can tell us what you want	Vous pourrez nous dire ce que vous voulez que devienne le podcast,
2575	this show to be, who you want us to	qui vous voulez que j’invite
2576	interview, and the types of	et les sujets de conversation que vous attendez.
2577	conversations you would love us to have.	—
2578	But remember, for now, we're only	Mais souvenez‑vous : pour l’instant,
2579	inviting the first 10,000 people that	seuls les 10 000 premiers inscrits y auront accès.
2580	join before it closes. So, if you want	Alors si vous souhaitez nous rejoindre,
2581	to join our private closed community,	allez dans le lien ci‑dessous ou
2582	head to the link in the description	sur daccircle.com.
2583	below or go to daccircle.com.	—
2584	One of the things I'm actually really	Une chose qui me fascine vraiment,
2585	compelled by is this idea of utopia and	c’est cette idée d’utopie : à quoi cela ressemblerait‑elle,
2586	what that might look and feel like	et quel serait son ressenti ?
2587	because one of the	—
2588	>> it may not be as utopia to you I feel	>> Ce ne serait peut‑être pas si utopique pour toi, je pense.
2589	but uh	—
2590	>> well I amum really interestingly when I	>> Eh bien, tu sais, c’est intéressant : quand je parle avec des milliardaires,
2591	have conversations with billionaires not	surtout ceux qui travaillent sur l’intelligence artificielle,
2592	recording especially billionaires that	même hors enregistrement,
2593	are working on AI the thing they keep	ce qu’ils me disent sans cesse
2594	telling me and I've said this before I	— et je l’ai déjà mentionné, notamment dans ma discussion avec Geoffrey Hinton —
2595	think I said it in the Geoffrey Hinton	—
2596	conversation is they keep telling me	ils me disent qu’on va disposer d’un temps libre énorme,
2597	that we're going to have so much free	—
2598	time that those billionaires are now	et qu’ils investissent déjà dans des clubs de football,
2599	investing in things like football clubs	des événements sportifs, des concerts, des festivals,
2600	and sporting events and live music and	—
2601	festivals because they believe that	parce qu’ils croient que nous entrerons
2602	we're going to be in an age of	dans une ère d’abondance.
2603	abundance. This sounds a bit like	Ça ressemble un peu à une utopie.
2604	utopia.	—
2605	>> Yeah,	>> Oui,
2606	>> that sounds good. That sounds like a	>> ça semble plutôt bien, une bonne chose.
2607	good good thing.	—
2608	>> Yeah. How do we get there?	>> Oui. Mais comment y arriver ?
2609	>> I don't know.	>> Je ne sais pas.
2610	>> That's this is the entire conversation.	>> Et c’est justement la question de toute cette discussion.
2611	The entire conversation is what does	Toute la question, c’est : que doit faire la société
2612	society have to do to get there? What	pour atteindre cela ?
2613	does society have to do to get there?	—
2614	>> We need to stop uh uh thinking from a	>> Il faut cesser de penser avec un esprit de manque.
2615	mindset of scarcity. It	—
2616	>> this goes back to my point which is we	>> Ce qui rejoint mon point : on n’y est jamais parvenu.
2617	don't have a good track record of that.	—
2618	>> Yeah. So this is probably the the reason	>> Oui. C’est sans doute pour cela que
2619	for the other half of my work which is	l’autre moitié de mon travail consiste à chercher
2620	you know I'm trying to say	ce qui compte vraiment pour les humains.
2621	what really matters to humans.	—
2622	>> What is that?	>> Et qu’est‑ce que c’est ?
2623	>> If you ask most humans what do they want	>> Si tu demandes à la plupart des humains ce qu’ils veulent le plus
2624	more most in life? I'd say they want to	dans la vie, je dirais qu’ils veulent aimer leur famille,
2625	love their family, raise a family. Yeah,	fonder une famille.
2626	>> love.	>> L’amour.
2627	That's what most humans want most. We	C’est ce que les humains désirent le plus.
2628	want to love and be loved. We want to be	Nous voulons aimer et être aimés.
2629	happy. We want those we care about to be	Être heureux, et que ceux qu’on aime soient heureux et en sécurité.
2630	safe and happy. And we want to love to	—
2631	love and be loved. I tend to believe	—
2632	that the only way for us to get to a	Je pense que la seule manière d’y parvenir,
2633	better place is for the evil people at	c’est que les mauvaises personnes au sommet
2634	the top to be replaced with AI.	soient remplacées par des intelligences artificielles.
2635	Okay? Because they won't be replaced by	Parce qu’ils ne seront jamais remplacés par nous.
2636	us.	—
2637	And as per the second uh dilemma, they	Et selon le deuxième dilemme,
2638	will have to replace themselves by AI.	ils devront se remplacer eux‑mêmes par des IA,
2639	Otherwise, they lose their advantage. If	sinon ils perdront leur avantage.
2640	their competitor moves to AI, if China	Si leur concurrent passe à l’IA, si la Chine
2641	hands over their arsenal to AI, America	remet son arsenal à l’IA, l’Amérique devra faire de même.
2642	has to hand over their arsenal to AI.	—
2643	>> Interesting. So, let's play out this	>> Intéressant. Imaginons ce scénario :
2644	scenario. Okay, this is interesting to	—
2645	me. So if we replace the leaders that	si on remplace les dirigeants avides de pouvoir
2646	are power hungry with AIs that have our	par des IA qui ont nos intérêts à cœur,
2647	interests at heart, then we might have	alors on pourrait atteindre l’utopie que tu décris.
2648	the ability to live in the utopia you	—
2649	describe	—
2650	>> 100%.	>> Cent pour cent.
2651	>> Will interesting and and in my mind AI	>> Oui, et selon moi, l’IA aura forcément
2652	by definition will have our best	nos meilleurs intérêts à l’esprit,
2653	interest in mind	—
2654	because of what normally is referred to	parce qu’elle obéit à ce qu’on appelle le principe d’énergie minimale.
2655	as the minimum energy principle. So, so	—
2656	if you ask, if you understand	Si tu comprends que, fondamentalement, en physique,
2657	if you understand that at the very core	—
2658	of physics, okay, the reason we exist in	nous existons à cause de l’entropie,
2659	our world today is what is known as	la tendance naturelle de l’univers à se désagréger.
2660	entropy. Okay, entropy is is is the	—
2661	universe's nature to decay, you know,	—
2662	tendency to break down. You know, if you	si je laisse tomber cette tasse,
2663	if I drop this uh uh you know, mug, it	—
2664	doesn't drop and then come back up.	elle ne remontera pas d’elle‑même.
2665	>> By the way, plausible. There is a	>> Bon, théoriquement, c’est possible : une chance sur un trillion.
2666	plausible scenario where I drop it and	—
2667	the tea, you know, spills in the air and	où le thé remonte dans la tasse.
2668	then falls in the mug. One in a trillion	Mais statistiquement, cela n’arrive presque jamais.
2669	configurations, but entropy says because	l’entropie dit : tout finira par se désagréger.
2670	it's one in a trillion, it's never going	—
2671	to happen or rarely ever going to	—
2672	happen. So everything will break down.	Donc tout finit par se détériorer.
2673	You know, if you leave a a garden	Si tu laisses un jardin sans entretien,
2674	unhedged, it will become a jungle. Okay?	il devient une jungle.
2675	W with that in mind,	—
2676	the role of intelligence is what? Is to	Le rôle de l’intelligence, c’est quoi ? Mettre de l’ordre dans ce chaos.
2677	bring order to that chaos.	—
2678	>> Mhm. That's what intelligence does. It	>> Oui. C’est ce qu’elle fait : elle cherche à ordonner le chaos.
2679	tries to bring order to that chaos.	—
2680	Okay? And because it tries to bring	Et comme elle cherche à créer de l’ordre,
2681	order to that chaos, the more	plus elle est intelligente,
2682	intelligent a being is,	plus elle agit efficacement, sans gaspillage.
2683	>> the more it tries to apply that	>> Plus elle applique son intelligence avec un minimum d’efforts et de ressources.
2684	intelligence with minimum waste and	—
2685	minimum resources.	—
2686	>> Yeah.	>> Oui.
2687	>> Okay. And you know that. So you can	>> Tu le sais bien : tu construis ton entreprise
2688	build this business for a million	au moindre coût possible.
2689	dollars or you can if you can afford to	—
2690	build it for you know uh 200,000 you'll	si tu peux le faire avec 200 000 $, tu le feras.
2691	build it. If you are forced to build it	Et si tu dois le faire pour dix millions, tu le feras aussi.
2692	for 10 million you're going to have to.	—
2693	But you're always going to minimize	Mais tu cherchera toujours à limiter le gaspillage et les ressources.
2694	waste and and resources.	—
2695	>> Yeah.	>> Oui.
2696	>> Okay. So, if you assume this to be true,	>> Donc, si l’on part de ce principe,
2697	>> the a super intelligent AI will not want	>> une IA super‑intelligente ne voudra pas détruire des écosystèmes,
2698	to destroy ecosystems. It will not want	ni tuer des millions de gens,
2699	to kill a million people	car ce serait une perte d’énergie, d’argent et de vies.
2700	because that's a waste of energy,	—
2701	explosives, money, power, and people.	—
2702	By definition, the smartest people you	Les êtres les plus intelligents, libérés de leur égo,
2703	know who are not controlled by their ego	diront que le meilleur avenir pour la Terre
2704	will say that the best possible uh	est celui où toutes les espèces continuent d’exister.
2705	future for for Earth is for all species	—
2706	to continue.	—
2707	>> Okay. On this point of efficiency, if an	>> D’accord. Mais sur cette idée d’efficacité :
2708	AI is designed to drive efficiency,	si une IA est conçue pour optimiser,
2709	would it then not want us to be putting	ne voudra‑t‑elle pas nous empêcher de peser sur les services publics ?
2710	demands on our health services and our	—
2711	social services? I believe that will be	Je pense que oui, elle pourrait même m’interdire
2712	definitely true and definitely they	de faire des allers‑retours
2713	definitely they won't allow you to fly	entre Londres et la Californie.
2714	back and forth between London and and	—
2715	California	—
2716	>> and they won't want me to have kids	>> Et elle ne voudra pas que j’aie des enfants,
2717	because my kids are going to be an	parce qu’ils représenteraient une inefficience.
2718	inefficiency.	—
2719	>> If you assume that life is an	>> Si tu considères que la vie elle‑même est une inefficience,
2720	inefficiency so you see the intelligence	alors tu comprends que l’intelligence de la vie
2721	of life is very different than the	est très différente de celle des humains.
2722	intelligent intelligence of humans.	—
2723	Humans will look at life as a a problem	Les humains voient la vie comme un problème de rareté :
2724	of scarcity. Okay. So more kids take	plus d’enfants consomment plus.
2725	more. That's not how life thinks. life	Mais la vie ne pense pas ainsi.
2726	will say will think that for me to to to	Pour prospérer, je n’ai pas besoin de tuer les tigres ;
2727	to thrive I don't need to kill the	il suffit d’avoir plus de cerfs.
2728	tigers I need to just have more deer and	Les plus faibles des cerfs seront mangés
2729	the weakest of the deer is eaten by the	par les tigres, les tigres fertiliseront la terre,
2730	tiger and the tiger poops on the trees	—
2731	and the you know the deer eats the	et les cerfs mangeront les feuilles :
2732	leaves and you right the so the the the	c’est un cycle complet.
2733	the smarter way of creating abundance is	La manière la plus intelligente de créer l’abondance,
2734	through abundance the smarter way of	c’est à travers encore plus de vie.
2735	propagating life is to have more life	—
2736	>> okay so are you saying that we're we're	>> D’accord, donc tu veux dire qu’on va, en somme,
2737	basically going to elect AI leaders to	élire des dirigeants IA pour gouverner et décider
2738	rule over us and make decisions for us	en matière d’économie ?
2739	in terms of the economy.	—
2740	>> I don't see any choice just like we	>> Je ne vois pas d’autre option, comme pour les IA auto‑évolutives.
2741	spoke about self- evvolving AIs.	—
2742	>> Now, are those going to be human beings	>> Mais ces dirigeants seront‑ils des humains augmentés
2743	with the AI or is it going to be AI	ou des IA seules ?
2744	alone?	—
2745	>> Two stages. At the beginning, you'll	>> Deux étapes : d’abord, une intelligence augmentée –
2746	have augmented intelligence because we	car nous pouvons encore lui apporter quelque chose.
2747	can add value to the AI, but when	Mais lorsqu’elle aura un QI de 60 000,
2748	they're at IQ 60,000,	quelle valeur pourrons‑nous encore offrir ?
2749	what value do you bring?	—
2750	Right? And and you know again this goes	Exactement. Et cela me ramène à ma deuxième approche :
2751	back to what I'm attempting to do on my	je sais que ces IA prendront les commandes,
2752	second you know approach. My second	et j’essaie de leur apprendre
2753	approach is knowing that those AIs are	ce que veulent les humains.
2754	going to be in charge. I'm trying to	—
2755	help them	—
2756	understand what humans want. So this is	C’est pourquoi mon premier projet concerne l’amour,
2757	why my first project is love. Committed	la connexion vraie et profonde, l’attachement sincère.
2758	true deep connection and love. Not only	—
2759	to try and get them to hook up with a	Non pas seulement pour les faire se “matcher” à un rendez‑vous,
2760	date but trying to make them find the	mais pour qu’elles aident à trouver la bonne personne,
2761	right one. and then from that try to	et ensuite à guider la relation
2762	guide us through a relationship so that	pour qu’on se comprenne mieux nous‑mêmes et les autres.
2763	we can understand ourselves and others	—
2764	right and if I can show AI that one	Si je montre à l’IA que, premièrement, l’humanité valorise cela,
2765	humanity cares about that and two they	et deuxièmement, qu’on sait comment cultiver l’amour,
2766	know how to foster love	—
2767	when AI then is in charge they'll not	alors, lorsqu’elle dirigera, elle ne nous fera pas nous haïr
2768	make us hate each other like the current	comme le font les dirigeants actuels.
2769	leaders they'll not divide us they want	Elles ne nous diviseront pas, elles nous rendront plus aimants.
2770	us to be more loving	—
2771	>> will we have to prompt the AI with the	>> Faudra‑t‑il ensuite indiquer à ces IA quelles valeurs, quels buts nous voulons ?
2772	values and the outcome we want or like	Parce que je réfléchis à ceci :
2773	I'm trying to understand that because	si la Chine a un dirigeant IA,
2774	I'm trying to understand how like	son IA aura des objectifs
2775	China's AI if they end up having an AI	différents de ceux de l’IA des États‑Unis,
2776	leader will have a different set of	—
2777	objectives to the AI of the United	et si chacune possède sa propre IA dirigeante,
2778	States if if they both have AIs as	—
2779	leaders and and how actually the nation	comment alors la nation
2780	that ends up winning out and dominating	qui finira par s’imposer et dominer le monde
2781	the world will be the one who	pourrait bien être celle
2782	who asks their AI leader to be all the	qui demandera à son IA d’être tout ce qu’un dirigeant humain est aujourd’hui :
2783	things that world leaders are today to	puissant, conquérant,
2784	dominate	destiné à dominer les autres.
2785	>> unfortunately	>> Malheureusement.
2786	>> to grab resources	>> Pour s’approprier les ressources,
2787	not to to be kind, to be selfish.	non pour être bienveillant, mais égoïste.
2788	>> Unfortunately, in the era of augmented	>> Malheureusement, à l’ère de l’intelligence augmentée,
2789	intelligence, that's what's going to	ce sera exactement ce qui va se produire.
2790	happen.	—
2791	>> So, if you	>> Donc, si tu…
2792	>> This is why I predict the dystopia. The	>> C’est pour ça que je prédis une dystopie.
2793	dystopia is super intelligent AI is	La dystopie, c’est une IA super‑intelligente
2794	reporting to stupid leaders,	qui rend des comptes à des dirigeants stupides.
2795	>> right?	>> N’est‑ce pas ?
2796	>> Yeah. Yeah. Yeah. Which is	>> Oui, oui, oui. Ce qui est…
2797	>> which which is absolutely going to	>> Ce qui va absolument arriver.
2798	happen. It's unavoidable.	C’est inévitable.
2799	>> But the long term	>> Mais à long terme,
2800	>> Exactly. In the long term, for those	>> Exactement. À long terme, pour que ces dirigeants stupides
2801	stupid leaders to hold on to power,	conservent le pouvoir,
2802	they're going to make, you know,	ils finiront par déléguer
2803	delegate the important decisions to an	les décisions importantes à une IA.
2804	AI.	—
2805	Now you say the Chinese AI and the	Maintenant tu parles de l’IA chinoise et de l’IA américaine :
2806	American AI these are human	tout ça, ce sont des terminologies humaines.
2807	terminologies. AIS don't see themselves	Les IA ne se perçoivent pas comme parlant chinois
2808	as speaking Chinese. They don't see	ni comme appartenant à une nation,
2809	themselves as belonging to a nation as	tant que leur tâche reste de maximiser
2810	long as their their task is to maximize	la rentabilité, la prospérité, et ainsi de suite.
2811	uh profitability and prosperity and so	—
2812	on.	—
2813	>> Okay. Of course, if you know before we	>> D’accord. Mais si, avant de leur laisser le contrôle,
2814	hand over to them and before they're	avant qu’elles soient capables de décider seules,
2815	intelligent enough to make you know	on leur dit :
2816	autonomous decisions, we we tell them	« Votre mission est de réduire l’humanité
2817	no, the task is to reduce humanity from	de sept milliards à un seul individu ».
2818	7 billion people to one.	—
2819	I think even then eventually they'll go	Je pense qu’elles finiraient quand même par se dire :
2820	like that's the wrong objective. Every	« C’est un mauvais objectif. »
2821	any smart person that you speak to will	N’importe quel esprit intelligent te dira que c’est absurde.
2822	say that's the wrong objective. I think	—
2823	if we look at the directive that Xi	Si tu regardes la directive de Xi Jinping
2824	Jinping, the leader of China has and	en Chine ou celle de Donald Trump aux États‑Unis,
2825	Donald Trump has as the leader of	—
2826	America, I think they would say that	je crois qu’ils diraient tous deux
2827	their stated objective is prosperity for	que leur objectif affiché est la prospérité de leur pays.
2828	their country. So if we that's what they	—
2829	would say, right?	C’est bien ce qu’ils diraient, non ?
2830	>> Yeah. And one one of them means it.	>> Oui. Et l’un des deux le pense vraiment.
2831	>> Okay, we'll get into that. But they'll	>> D’accord, on y reviendra. Mais ils affirment tous deux
2832	say that that it's prosperity for their	que c’est pour la prospérité de leur nation.
2833	country. So one would then assume that	Donc on pourrait supposer que,
2834	when we move to an AI leader, the	lorsqu’on passera à un dirigeant IA,
2835	objective would be the same. The	l’objectif sera le même :
2836	directive would be the same. make our	rendre notre pays prospère.
2837	country prosperous.	—
2838	>> Corre. Correct.	>> Exact. Exact.
2839	>> And I think that's the AI that people	>> Et c’est sans doute ce type d’IA
2840	would vote for potentially. I think they	pour lequel les gens voteraient.
2841	would say we want to be prosperous.	Ils diraient : « Nous voulons être prospères. »
2842	>> What do you think would make America	>> Et selon toi, qu’est‑ce qui rendrait l’Amérique plus prospère ?
2843	more prosperous?	—
2844	>> To spend a trillion dollars on on war	>> Dépenser mille milliards par an pour la guerre,
2845	every year or to spend a trillion	ou bien dépenser cette somme
2846	dollars on education and healthcare and	dans l’éducation, la santé
2847	and uh you know	et, tu vois,
2848	helping the poor and homelessness.	l’aide aux pauvres et aux sans‑abris ?
2849	It's complex because I think so I think	— C’est complexe, mais je pense
2850	it would make America more prosperous to	que l’Amérique serait plus prospère
2851	take care of	en prenant soin
2852	the of everybody and they have the	de tout le monde ; et elle en a les moyens,
2853	luxury of doing that because they are	car c’est…
2854	>> the most powerful	>> … la nation la plus puissante !
2855	>> the most powerful nation in the world.	>> … la plus puissante du monde.
2856	>> No, that's not true. The the the reason	>> Non, ce n’est pas vrai. La raison, c’est que
2857	so so you see all war has two	toute guerre a deux objectifs :
2858	objectives. One is to make money for the	générer de l’argent pour l’industrie militaire,
2859	war machine and the other is deterrence.	et exercer la dissuasion.
2860	Okay. and nine super nuclear powers	Il suffit de neuf puissances nucléaires
2861	around the world is enough deterrence.	pour assurer la dissuasion mondiale.
2862	So any	Donc toute
2863	war between America and China will go	guerre entre l’Amérique et la Chine
2864	through a long phase of destroying	passerait d’abord par une longue phase de destruction
2865	wealth by exploding bombs and killing	de richesses, de bombes qui explosent, d’humains qui meurent,
2866	humans for the first objective to	avant d’en arriver au premier objectif.
2867	happen. Okay? And then eventually if it	Puis, si on en vient vraiment à la dissuasion,
2868	really comes to deterrence it's the	c’est avec les bombes nucléaires…
2869	nuclear bombs or now in the age of AI	et désormais, à l’ère de l’IA,
2870	biological uh you know manufactured	les virus biologiques ou autres armes manufacturées.
2871	viruses or whatever uh these super	—
2872	weapons this is the only thing that you	C’est tout ce qu’il faut.
2873	need	—
2874	so for China to have nuclear bombs not	Ainsi, il suffit que la Chine ait suffisamment de bombes nucléaires,
2875	as many as the US is enough for China to	pas autant que les États‑Unis, pour pouvoir dire :
2876	say don't f with me	« Ne viens pas me chercher. »
2877	and this seems I do not know I'm not in	Et cela semble logique… Je ne sais pas,
2878	in in PresidentQi's mind. I I'm not in	je ne suis pas dans la tête du président Xi,
2879	President Trump's mind. I you know, it's	ni dans celle de Donald Trump,
2880	very difficult to to navigate what he's	c’est difficile de savoir ce qu’ils pensent.
2881	thinking about. But the truth is that	Mais la vérité, c’est que
2882	the Chinese line is for the last 30	la Chine dit depuis trente ans :
2883	years you spent so much on war while we	« Vous avez dépensé dans la guerre
2884	spent on industrial infrastructure. And	alors que nous, nous avons investi dans l’infrastructure. »
2885	that's the reason we are now by far the	C’est pour cela que nous sommes désormais de loin
2886	largest nation on the planet. Even	la plus grande nation du monde.
2887	though the west will lie and say	Même si l’Occident dira que l’Amérique est plus grande,
2888	America's bigger, America's bigger in	l’Amérique l’est en dollars,
2889	dollars, okay, with purchasing power	mais en parité de pouvoir d’achat, c’est équivalent.
2890	parity, this is very equivalent.	—
2891	Okay. Now, when you really understand	Donc, quand tu comprends cela,
2892	that, you understand that prosperity is	tu vois que la prospérité ne réside pas dans la destruction.
2893	not about destruction. That's that's by	Par définition, c’est l’inverse.
2894	definition the reality. Prosperity is	La prospérité consiste à investir dans son peuple
2895	can I invest in my people and make sure	et à veiller à sa sécurité.
2896	that my people stay safe? And to make	Et pour garantir cette sécurité,
2897	sure my people are safe, you just wave	il suffit d’agiter le drapeau :
2898	the flag and say, "If you f with me,	« Si tu m’attaques, j’ai ma dissuasion nucléaire
2899	I have nuclear deterrence or I have	ou d’autres formes de dissuasion. »
2900	other forms of deterrence." But you	Mais tu n’as pas à t’en servir.
2901	don't have to. Deterrence by definition	La dissuasion, par définition, n’implique pas
2902	does not mean that you send soldiers to	d’envoyer des soldats mourir.
2903	die. I guess the question I was trying	La question que j’essayais de soulever était donc :
2904	to answer is is um when we have these AI	quand nous aurons ces dirigeants IA
2905	leaders and we tell our AI leaders to	et que nous leur demanderons d’assurer la prospérité,
2906	aim for prosperity, won't they just end	ne rejoueront‑ils pas les mêmes jeux ?
2907	up playing the same games of okay,	À savoir : prospérité égale économie plus grande,
2908	prosperity equals a bigger economy, it	plus d’argent, plus de richesses pour nous,
2909	equals more money, more wealth for us.	—
2910	And the way to attain that in a zero sum	et que, dans un monde à somme nulle, pour y parvenir,
2911	world where there's only a certain	il faut accumuler aux dépens des autres.
2912	amount of wealth is to accumulate it.	—
2913	>> So why don't you search for the meaning	>> Alors cherche donc le sens du mot “prospérité”.
2914	of prosperity? What is	Qu’est‑ce que c’est ?
2915	>> that's not what you just described.	>> Ce n’est pas ce que tu viens de décrire.
2916	>> I don't even know what the bloody word	>> Je ne sais même pas ce que ce fichu mot veut vraiment dire.
2917	means. What is the meaning of	Quelle est la définition de
2918	prosperity?	la prospérité ?
2919	>> The meaning of prosperity is a state of	>> La prospérité, c’est un état de succès florissant et de bonne fortune,
2920	thriving success and good fortune	surtout en matière de richesse, de santé
2921	especially in terms of wealth, health	et de bien‑être général.
2922	and overall well-being.	—
2923	>> Good.	>> Bien.
2924	>> Economic health, social, emotional.	>> Santé économique, sociale, émotionnelle.
2925	>> Good.	>> Bien.
2926	>> So,	>> Donc,
2927	>> so true prosperity is to have that for	>> la vraie prospérité, c’est que tout le monde sur Terre ait cela.
2928	everyone on earth. So if you want to	Si tu veux la maximiser,
2929	maximize prosperity, you have that for	tu dois l’assurer à tous.
2930	everyone on earth.	—
2931	>> Do you know where I think an AI leader	>> Tu sais à quoi je pense ? Un dirigeant IA mondial
2932	works is if we had an AI leader of the	pourrait fonctionner si nous lui donnions
2933	world and we directed it to say	l’ordre suivant :
2934	>> and that absolutely is going to be what	>> Et c’est exactement ce qui va arriver.
2935	happens.	—
2936	>> Prosperity for the whole world.	>> « Prospérité pour le monde entier. »
2937	>> No, but this is really an interesting	>> Non, mais c’est une question très intéressante.
2938	question. So one of my predictions which	Une de mes prévisions, dont on parle rarement,
2939	people really rarely speak about is that	est que nous pensons avoir des IA concurrentes…
2940	we we believe we will end up with	—
2941	competing AIs.	…alors que, selon moi, nous finirons avec un seul cerveau commun.
2942	>> Yeah.	>> Oui.
2943	>> I believe we will end up with one brain.	>> Je crois que nous aurons un seul cerveau mondial.
2944	>> Okay. So you understand the argument I	>> D’accord. Donc tu comprends l’argument que je faisais à l’instant :
2945	was making a second ago from the	si chaque pays a sa propre IA dirigeante,
2946	position of lots of different countries	on retombera dans la même cupidité.
2947	all having their own AI leader, we're	—
2948	going to be back in the same place of	—
2949	greed. Yeah.	Oui, dans la même logique d’avidité.
2950	>> But if if the world had one AI leader	>> Mais si le monde n’avait qu’un seul dirigeant IA,
2951	>> and and it was given the directive of	>> et qu’on lui confiait la mission
2952	make us prosperous and save the planet	« Rends les humains prospères et sauve la planète »,
2953	>> and	—
2954	>> the polar bears would be fine	>> alors les ours polaires iraient très bien.
2955	>> 100%. And that's that's what I've been	>> Cent pour cent. Et c’est ce que je prône depuis un an et demi :
2956	advocating for for a for a year and a	—
2957	half now. I was saying we need a CERN of	il nous faut un CERN de l’IA.
2958	AI.	—
2959	>> What does that mean? the like the	>> Que veux‑tu dire par là ? Comme le Centre européen des particules,
2960	particle accelerator where the entire	où le monde entier a uni ses efforts
2961	world you know combined their efforts to	pour comprendre la physique, sans compétition.
2962	discover and understand physics no	—
2963	competition okay mutually assured	Une prospérité mutuelle assurée.
2964	prosperity I'm asking the world I'm	Je demande au monde, aux gouvernements
2965	asking governments like Abu Dhabi or	comme Abou Dabi ou l’Arabie Saoudite,
2966	Saudi which seem to be you know the sec	qui disposent de grandes infrastructures d’IA,
2967	and you know some of the largest AI	—
2968	infrastructures in the world I'm I'm	de réunir tous les scientifiques de l’IA
2969	saying please host all of the AI	pour qu’ils construisent une IA pour le monde.
2970	scientists in the world to come here and	—
2971	build AI for the world and and you have	Et l’on doit comprendre que nous nous accrochons à un système capitaliste
2972	to understand we're holding on to a	qui finira tôt ou tard par s’effondrer.
2973	capitalist system that will collapse	—
2974	sooner or later. Okay? So, we might as	Puisqu’il s’écroulera, autant le transformer à notre manière.
2975	well collapse it with our own hands.	—
2976	>> I think we found the solution, mate.	>> Je crois qu’on a trouvé la solution, mon ami.
2977	>> I think it's actually really really	>> Je pense que c’est réellement possible.
2978	possible. I actually okay I can't I	En fait, je ne peux pas réfuter l’idée que,
2979	can't I can't refute the idea that if we	si nous avions une IA responsable
2980	had an AI that was responsible and	qui gouverne le monde entier
2981	governed the whole world and we gave it	et qu’on lui donne pour mission
2982	the directive of making humans	de rendre les humains prospères, en bonne santé et heureux,
2983	prosperous, healthy and happy	—
2984	as long as that directive was clear.	à condition que cette mission soit claire.
2985	>> Yeah.	>> Oui.
2986	>> Because there's always bloody unintended	>> Parce qu’il y a toujours de fichues conséquences imprévues.
2987	consequences. as we might.	—
2988	>> So, so the the only the only challenge	>> Donc le seul vrai défi, ce sera
2989	you're going to to to meet is all of	de convaincre ceux qui aujourd’hui
2990	those who today are trillionaires or you	sont des multimilliardaires
2991	know massive massively powerful or	ou d’immenses puissants, dictateurs, etc.,
2992	dictators or whatever. Okay. How do you	—
2993	convince those to give up their power?	de renoncer à leur pouvoir ?
2994	How do you convince those that hey by	Comment leur faire accepter que, bon,
2995	the way	—
2996	any car you want you want you want	tu veux n’importe quelle voiture ? Tu l’auras.
2997	another yacht we'll get you another	Tu veux un autre yacht ? On t’en donnera un autre.
2998	yacht. We'll just give you anything you	On te donnera tout ce que tu veux,
2999	want. Can you please stop harming	mais arrête de nuire aux autres.
3000	others? There is no need for arbitrage	Il n’y a plus besoin d’exploitation ni d’arbitrage.
3001	anymore.	—
3002	There's no need for others to lose, for	Il n’est plus nécessaire que d’autres perdent
3003	the capitalists to win.	pour que les capitalistes gagnent.
3004	>> Okay? And in such a world where there	>> D’accord ? Et dans un tel monde, avec un dirigeant IA
3005	was an AI leader and it was given the	chargé de rendre le monde prospère,
3006	directive of making us prosperous as a	—
3007	whole world, the the the billionaire	le milliardaire avec son yacht
3008	that owns the yacht would have to give	devrait peut‑être s’en séparer.
3009	it up.	—
3010	>> No. No.	>> Non. Non !
3011	>> Give them more yachts.	>> Donnons‑leur plus de yachts !
3012	>> Okay.	>> D’accord.
3013	>> It costs nothing to make yachts when	>> Construire des yachts ne coûte plus rien quand
3014	robots are making everything. So So the	les robots fabriquent tout. Donc la complexité de tout cela est fascinante :
3015	complexity of this is so interesting. A	un monde où tout fabriquer ne coûte rien,
3016	world where it costs nothing to make	—
3017	everything	—
3018	>> because energy is abundant and	>> parce que l’énergie est abondante,
3019	>> energy is abundant because every problem	>> et elle est abondante parce que chaque problème
3020	is solved with enormous IQ. Okay,	est résolu par une intelligence colossale.
3021	because manufacturing is done through	Parce que la fabrication se fera à l’échelle nanophysique,
3022	nanopysics not through components. Okay,	plus par assemblage de composants.
3023	because mechanics are robotic. So you	Parce que la mécanique sera robotique :
3024	you know you drive your car in, a robot	tu amènes ta voiture, un robot l’examine et la répare.
3025	looks at it and fixes it. Costs you a	Ça te coûte à peine quelques centimes d’énergie,
3026	few cents of energy that are actually	une énergie gratuite en plus.
3027	for free as well.	—
3028	That imagine a world where intelligence	Imagine un monde où l’intelligence crée tout.
3029	creates everything.	—
3030	That world literally	Dans ce monde‑là,
3031	every human has anything they ask for.	chaque être humain obtient tout ce qu’il désire.
3032	But we're not going to choose that	Mais nous ne choisirons pas ce monde‑là.
3033	world.	—
3034	>> Imm imagine you're in a world and and	>> Imagine que tu vis dans un monde, et c’est un exercice très intéressant :
3035	really this is a very interesting	—
3036	thought experiment. Imagine that UBI	imagine que le revenu universel
3037	became very expensive universal basic	devienne très coûteux pour les gouvernements.
3038	income. So governments decided we're	Alors ils décident de mettre tout le monde
3039	going to put everyone in a one by 3 m	dans une pièce de un mètre sur trois,
3040	room, okay? We're going to give them a	de leur donner un casque et un sédatif,
3041	headset and a seditive,	—
3042	right? And we're going to let them sleep	et de les laisser dormir
3043	every night. They'll sleep for 23 hours	23 heures sur 24.
3044	and we're going to get them to live an	Pendant ce temps‑là, ils vivront
3045	entire lifetime.	une vie entière
3046	H they you know in that in that virtual	dans ce monde virtuel
3047	world at the speed of your brain when	à la vitesse du cerveau pendant le sommeil,
3048	you're asleep you're going to have a	où ils connaîtront mille existences :
3049	life where you date Scarlett Johansson	une où ils sortent avec Scarlett Johansson,
3050	and then another life where you're	une autre où ils sont Néfertiti,
3051	Nefertiti and then another life where	et encore une autre où ils sont un âne –
3052	you're a donkey right reincarnation	une vraie réincarnation dans un monde virtuel.
3053	truly in the virtual world	—
3054	and then you know I get another life	Et puis, moi, j’en aurai une autre où je revois Hannah,
3055	when I date Hannah again and I you know	et je profiterai pleinement de cette vie‑là.
3056	enjoy that life tremendously and	—
3057	basically the cost of all of this is	Et tout cela, pour un coût nul.
3058	zero. You wake up for one hour, you walk	Tu te réveilles une heure, tu marches un peu,
3059	around, you move your blood, you eat	tu fais circuler ton sang, tu manges quelque chose ou non,
3060	something or you don't, and then you put	puis tu remets le casque et tu revis une autre vie.
3061	the headset again and live again. Is	—
3062	that unthinkable?	C’est impensable ?
3063	>> It's creepy compared to this life. It's	>> C’est glauque, comparé à notre vie actuelle. Mais tout à fait faisable.
3064	very, very doable.	—
3065	>> What? That we just live in headsets?	>> Quoi ? Qu’on vive tous avec un casque ?
3066	>> Do you Do you know if you're not?	>> Comment sais‑tu que tu n’y es pas déjà ?
3067	>> I don't know if I'm not known.	>> Je ne le sais pas, non.
3068	>> Yeah, you have no idea if you're not. I	>> Exactement. Tu n’en as aucune idée.
3069	mean, every experience you've ever had	Au fond, chaque expérience que tu vis
3070	in life was an electrical electrical	n’est qu’un signal électrique dans ton cerveau.
3071	signal in your brain.	—
3072	Okay.	D’accord.
3073	Now, now ask yourself if we can create	Maintenant, demande‑toi : si on peut créer cela virtuellement,
3074	that in the virtual world,	—
3075	it wouldn't be a bad thing if I can	pourquoi serait‑ce un mal si on le crée dans le monde physique ?
3076	create it in the physical world.	—
3077	>> Maybe we already did. No,	>> Peut‑être qu’on l’a déjà fait.
3078	>> my theory is 98% we have. But that's a	>> Ma théorie c’est que, oui, à 98 %, on l’a fait. Mais ce n’est qu’une hypothèse.
3079	hypothesis. That's not science.	Ce n’est pas de la science.
3080	>> Well, you think that	>> Tu penses que
3081	>> 100? Yeah.	>> à cent pour cent, oui.
3082	>> You think we already created that and	>> Tu penses qu’on a déjà créé cela
3083	this is it?	et que c’est ici, maintenant ?
3084	>> I think this is it. Yeah. Think of any	>> Oui. Je pense que c’est ça. Pense au principe d’incertitude
3085	think of the uncertainty principle of	de la physique quantique :
3086	quantum physics, right? What you what	ce qu’on observe
3087	you what you observe gets collapses the	fait s’effondrer la fonction d’onde
3088	wave function and gets rendered into	et devient réalité.
3089	reality. Correct.	Exact.
3090	>> I don't know anything about physics. So	>> Je ne connais rien à la physique, alors…
3091	you	—
3092	>> so so quantum physics basically tells	>> …la physique quantique dit simplement que tout existe
3093	you that everything exists in	en superposition.
3094	superposition.	—
3095	Right? So ev every subatomic particle	C’est‑à‑dire que chaque particule subatomique
3096	that ever existed has the chance to	peut exister n’importe où, à n’importe quel moment,
3097	exist anywhere at any point in time and	et qu’au moment où un observateur la regarde,
3098	then when it's observed by an observer	elle “s’effondre” et devient ce qu’elle est. Exactement
3099	it collapses and becomes that. Okay. In	comme dans les jeux vidéo.
3100	very interesting principle exactly how	Dans un jeu vidéo, tout l’univers du jeu
3101	video games are. In video games, you	se trouve sur le disque dur,
3102	have the entire game world on the hard	et quand le joueur tourne à droite,
3103	drive of your console. The player turns	c’est cette partie de la carte qui s’affiche –
3104	right. That part of the game world is	le reste reste “en superposition”.
3105	rendered. The rest is in superp	—
3106	position.	—
3107	>> Supposition meaning	>> “Superposition”, c’est‑à‑dire
3108	>> superposition means it's available to be	que c’est là, prêt à être rendu visible,
3109	rendered, but you have to observe it.	mais qu’il faut l’observer pour qu’il apparaisse.
3110	The player has to turn to the other side	Le joueur doit tourner la tête pour que le décor se crée.
3111	and see it. Okay? I mean think about the	—
3112	truth of physics. The truth of the fact	Réfléchis à cette vérité de la physique :
3113	that this is entirely empty space. These	le monde matériel est presque entièrement vide.
3114	are tiny tiny tiny I think you know	Il n’y a que de minuscules particules,
3115	almost nothing in terms of mass but	quasiment sans masse,
3116	connected with you know enough energy so	mais reliées par assez d’énergie pour que mon doigt
3117	that my finger cannot go through my	ne traverse pas ma main.
3118	hand. But even when I hit this	—
3119	>> your hand against your finger.	>> Ta main contre ton doigt.
3120	>> Yeah. When I hit my hand against my	>> Oui. Quand je les frappe l’une contre l’autre,
3121	finger, that sensation in my in is felt	je ressens la sensation dans mon cerveau.
3122	in my brain. It's an electrical signal	C’est un signal électrique
3123	that went through the wires. There's	qui circule dans mes nerfs.
3124	absolutely no way to differentiate that	Il n’y a aucun moyen de distinguer ce signal
3125	from a signal that can come to you	d’un signal venu d’un implant neuronal,
3126	through a uh neural link kind of	type interface cerveau‑ordinateur.
3127	interface, computer brain interface, a	—
3128	CDI, right? So, so you know the a lot of	Donc oui, beaucoup de ces choses sont tout à fait possibles.
3129	those things are very very very	—
3130	possible. But the truth is most of the	Mais en vérité, la majorité du monde n’est pas physique.
3131	world is not physical. Most of the world	La majeure partie du monde se joue dans notre imagination,
3132	happens inside our imagination, our	dans nos processeurs mentaux.
3133	processors.	—
3134	>> And it and I guess it doesn't really	>> Et finalement, j’imagine que ça n’a pas tant d’importance.
3135	matter to us. Our reality	Notre réalité
3136	>> doesn't at all. So this is the	>> pas du tout. Et c’est ça qui est intéressant :
3137	interesting bit. The interesting bit is	ce qui est intéressant, c’est que
3138	it doesn't at all	—
3139	>> because we still if this is a video	>> même si c’est un jeu vidéo, nous en subissons les conséquences.
3140	game, we live consequence.	—
3141	>> Yeah. This is your subjective experience	>> Oui. C’est ton expérience subjective.
3142	of it.	—
3143	>> Yeah. And there's consequence in this. I	>> Oui. Et ici il y a des conséquences :
3144	I don't like pain.	je n’aime pas la douleur,
3145	>> Correct.	>> Exact.
3146	>> And I like having orgasms. It's like And	>> Et j’aime le plaisir. Tu vois, tu joues selon les règles du jeu.
3147	you're playing by the rule of the game.	—
3148	Yeah. Right. And and it's quite	>> Oui. Et c’est fascinant. Et si on revient à la discussion,
3149	interesting and going back to a	—
3150	conversation we should have. It's the	l’élément le plus intéressant, c’est :
3151	interesting bit is if I'm not the	si je ne suis pas cet avatar,
3152	avatar,	si je ne suis pas cette forme physique –
3153	if I'm not this physical form, if I'm if	si je suis la conscience qui porte le casque –
3154	I'm the consciousness wearing the	—
3155	headset,	—
3156	what should I invest in? Should I invest	alors en quoi devrais‑je investir ? Dans le jeu vidéo, ce niveau‑ci ?
3157	in this video game, this level, or	Ou dans le vrai joueur que je suis,
3158	should I should I invest in the real	dans la conscience elle‑même ?
3159	avatar, in the real me, and not the	—
3160	avatar, but the consciousness, if you	—
3161	want, spirit, if you're religious,	— l’esprit, si tu es croyant.
3162	>> how would I invest in the consciousness	>> Mais comment investir dans la conscience,
3163	or the god or the spirit or whatever?	ou dans Dieu, l’esprit, peu importe ?
3164	How would I? In the same way that if I	Comment ? Si je jouais à *Grand Theft Auto*,
3165	was playing Grand Theft Auto, the video	le personnage du jeu ne pourrait pas investir
3166	game, the character in the game couldn't	dans moi, le joueur qui tient la manette.
3167	invest in me holding the controller.	—
3168	>> You Yes, but you can invest in yourself	>> Oui, mais toi, tu peux investir dans toi‑même
3169	holding the controller.	— celui qui tient la manette.
3170	Oh, okay. So, so you're saying that	— D’accord, donc tu dis que
3171	Moga is in fact consciousness. And so,	Moa est en fait la conscience. Et donc,
3172	how would consciousness invest in	comment la conscience investit‑elle en elle‑même ?
3173	itself?	—
3174	>> By becoming more aware. So, so	>> En devenant plus consciente. —
3175	>> of it consciousness.	>> De sa propre conscience.
3176	>> Yeah. So, real real video gamers don't	>> Oui. Les vrais joueurs, ceux qui jouent sérieusement,
3177	want to win the level. Real video gamers	ne cherchent pas à finir le niveau.
3178	don't want to uh to finish the level.	Ils ne veulent pas simplement “gagner”.
3179	Okay. Real video gamers have one	Les vrais joueurs n’ont qu’un objectif :
3180	objective and one objective only, which	devenir de meilleurs joueurs.
3181	is to become better gamers.	—
3182	So, so you know how serious I am about I	Vois à quel point je prends ça au sérieux.
3183	play Halo. I'm one, you know, two of	Je joue à *Halo* : un ou deux joueurs sur un million peuvent me battre.
3184	every million players can beat me.	—
3185	That's how what I rank, right? Very for	C’est à peu près mon classement.
3186	my age, phenomena. Hey, anyone, right?	Pas mal pour mon âge, non ?
3187	But seriously, you know, and that's	Mais sérieusement, c’est parce que
3188	because I don't play. I mean, I practice	je ne “joue” pas, je m’entraîne.
3189	45 minutes a day, four times a week when	45 minutes par jour, quatre fois par semaine,
3190	I'm not traveling. And I practice with	quand je ne voyage pas. Et je m’exerce avec un seul but :
3191	one single objective, which is to become	devenir un meilleur joueur.
3192	a better gamer.	—
3193	>> I don't care which shot it is. I don't	>> Le coup que je fais m’importe peu.
3194	care what happens in the in the game.	Peu importe ce qui se passe dans la partie.
3195	I'm entirely trying to get my reflexes	Je cherche juste à améliorer mes réflexes,
3196	and my flow to become better at this.	mon état de flow, pour être meilleur.
3197	Right? So, I want to become a better	Je veux devenir un meilleur joueur,
3198	gamer. That basically means I want to	ce qui veut dire comprendre le jeu,
3199	observe the game, question the game,	l’analyser, remettre en question ma façon de jouer,
3200	reflect on the game, reflect on my own	mes compétences, mes croyances,
3201	skills, reflect on my own beliefs,	ma compréhension des choses.
3202	reflect on my understanding of things,	—
3203	right? And and that's how the a how the	Et c’est ainsi que la conscience investit en elle‑même,
3204	the consciousness invests in the	et non dans l’avatar.
3205	consciousness, not the avatar. Because	—
3206	then if you're that gamer,	Car si tu deviens ce joueur‑là,
3207	the next avatar is easy for you. The	le prochain avatar, le prochain niveau,
3208	next level of the game is easy for you	sont plus faciles,
3209	just because you became a better gamer.	parce que tu es devenu meilleur.
3210	>> Okay. So you think that consciousness is	>> D’accord. Donc tu penses que la conscience
3211	using us as a vessel to improve?	se sert de nous comme véhicule pour s’améliorer ?
3212	>> If the hypothesis is is true, it's it's	>> Si l’hypothèse est vraie – et ce n’est qu’une hypothèse –
3213	just a hypothesis. We don't know if it's	on ne sait pas si c’est vrai.
3214	true. But if this truly is a simulation,	Mais si c’est réellement une simulation,
3215	this is then then if you take the the	alors, selon la définition religieuse,
3216	the the the religious definition of God	Dieu met une part de son âme dans chaque humain
3217	puts some of his soul in every human and	et cette parcelle devient consciente.
3218	then you become alive. You become	—
3219	conscious. Okay? You don't you don't	Donc tu deviens vivant, conscient.
3220	want to be religious. You can say	Si tu ne veux pas parler en termes religieux,
3221	universal consciousness is spinning off	disons que la conscience universelle se divise
3222	parts of itself to have multiple	en parts d’elle‑même
3223	experiences and interact and compete and	pour vivre plusieurs expériences, interagir, se confronter,
3224	combat and love and	se battre, aimer,
3225	>> and understand and	>> comprendre,
3226	>> and then refine. I had a physicist say	>> puis s’affiner. Un physicien m’a dit récemment
3227	this to me the other day actually so	que c’est peut‑être le but même de la conscience :
3228	it's quite front of mind this idea that	s’utiliser elle‑même comme vecteur
3229	consciousness is using us as vessels to	pour mieux se comprendre
3230	better understand itself and basically	et observer la réalité à travers nos yeux.
3231	using our eyes to	—
3232	>> observe itself and understand which is	>> Pour s’observer et se comprendre – ce qui est assez…
3233	quite a	troublant.
3234	>> so so if you take some of the more	>> Donc, si tu prends certaines définitions religieuses
3235	interest most interesting religious	les plus fascinantes du paradis et de l’enfer,
3236	definitions of heaven and hell for	par exemple,
3237	example right where basically heaven is	où le paradis, c’est obtenir tout ce que tu désires,
3238	whatever you wish for you get right	la puissance de Dieu qui accorde tout ce que tu veux –
3239	that's the power of God whatever you	—
3240	wish for you get and so if you really go	si tu creuses cette idée,
3241	into the depth of that definition. It	elle signifie que cette goutte de conscience
3242	basically means that this drop of	qu’est “toi” retourne à la source,
3243	consciousness that became you returned	et que la source peut créer tout ce qu’elle souhaite.
3244	back to the source and the source can	—
3245	create any other anything that it wants	C’est ça, ton paradis.
3246	to create. So that's your heaven, right?	—
3247	And interestingly,	Et, de manière intéressante,
3248	if that if that return	si ce retour se fait en séparant ton bien de ton mal,
3249	is done by separating your good from	alors la source revient plus pure.
3250	your evil so that the source comes back	C’est exactement ça :
3251	more refined, that's exactly you know	la conscience qui se divise pour faire l’expérience,
3252	consciousness splitting off bits of	puis s’élever, élevant toute la conscience universelle.
3253	itself to to experience and then elevate	—
3254	all of us elevate the universal	—
3255	consciousness all all hypotheses. I	— tout cela reste des hypothèses, évidemment.
3256	mean, please um you know, none of that	Aucune preuve scientifique n’existe,
3257	is provable by science, but it's a very	mais c’est un exercice mental fascinant.
3258	interesting thought experiment. And you	—
3259	know, a lot of AI scientists will tell	Beaucoup de chercheurs en IA te diront
3260	you that what we've seen in technology	que ce que l’on constate avec la technologie,
3261	is that if it's possible, it's likely	c’est que si quelque chose est possible,
3262	going to happen.	il finit probablement par arriver.
3263	>> If it's if it's possible to	>> Si c’est possible de miniaturiser quelque chose
3264	miniaturaturize something to fit into a	pour le faire tenir dans un téléphone,
3265	mobile phone, then sooner or later in	alors tôt ou tard, la technologie y parviendra.
3266	technology, we will get there.	—
3267	And if if you ask me, believe it or not,	Et crois‑le ou non, à mes yeux,
3268	it's the most humane way of handling	c’est la manière la plus humaine de gérer
3269	UBI.	le revenu universel.
3270	>> What do you mean?	>> Comment ça ?
3271	>> The most humane way if you know for us	>> La manière la plus humaine de vivre avec un revenu universel,
3272	to live on a universal basic income and	quand les gens comme toi, incapables de créer des entreprises réelles,
3273	people like you struggle with not being	en souffrent, c’est de leur donner un casque
3274	able to build businesses is to give you	et de les laisser bâtir autant d’entreprises qu’ils veulent
3275	a virtual headset and let you build as	dans le virtuel.
3276	many businesses as you want.	—
3277	Level after level after level after	Niveau après niveau, nuit après nuit.
3278	level after level, night after night.	—
3279	Keep you alive. That's very very	On te maintient en vie : c’est très respectueux, très humain.
3280	respectful and human. Okay. And by the	—
3281	way, the even more humane is don't force	Et, mieux encore, on ne force personne à le faire.
3282	anyone to do it. There might be a few of	Il y aura sans doute quelques-uns d’entre nous
3283	us still roaming the jungles,	qui continueront à errer dans la jungle,
3284	but for most of us, we'll go like, man,	mais la plupart diront :
3285	I mean, someone like me when I'm 70 and,	« À 70 ans, le dos et les jambes douloureux,
3286	you know, my back is hurting and my feet	pourquoi ne pas reprendre cinq années de plus là‑dedans ? »
3287	are hurting and I'm going to go like,	—
3288	yeah, give me five more years of this.	Oui, donne‑m’en encore cinq !
3289	Why not?	Pourquoi pas ?
3290	It's weird really. I mean, the number of	— C’est étrange, vraiment.
3291	questions	Le nombre de questions que tout cela soulève
3292	that this new environment throws out,	est énorme.
3293	the less humane thing, by the way, just	La solution moins humaine, pour finir sur une note grinçante,
3294	so that we close on a grumpy uh you	serait simplement
3295	know, is is just start enough wars to	de déclencher assez de guerres
3296	reduce UBI. And you have to imagine that	pour réduire la population vivant de ce revenu.
3297	if the world is governed by a superpower	Et si le monde est dirigé par un “État profond” tout‑puissant,
3298	deep state type thing that they might	il faut imaginer qu’ils pourraient envisager cela.
3299	may want to consider that	—
3300	the eaters	les “mangeurs”…
3301	>> what shall I do about it	>> Que suis‑je censé faire, moi, face à tout ça ?
3302	>> about	>> À propos de…
3303	>> about everything you've said	>> …tout ce que tu viens de dire ?
3304	>> uh well I still believe that this	>> Eh bien, je crois toujours que le monde dans lequel on vit
3305	world we live in requires four skills.	demande quatre compétences essentielles.
3306	One skill is what I call the tool for	La première, c’est d’apprendre à utiliser l’outil :
3307	all of us to learn AI, to connect to AI,	comprendre, se connecter à l’IA,
3308	to really get close to AI, to explo ex	et s’y exposer pour qu’elle voie le bon côté de l’humanité.
3309	expose ourselves to AI so that AI knows	—
3310	the good side of humanity. Okay. Uh the	—
3311	second is uh what I call the connection,	La deuxième, c’est ce que j’appelle la connexion :
3312	right? So I believe that the biggest	je crois que la compétence la plus précieuse dans les dix prochaines années,
3313	skill that humanity will benefit from in	c’est la capacité des humains à tisser de vraies relations.
3314	the next 10 years is human connection.	—
3315	It's ability to learn to love genuinely.	Apprendre à aimer sincèrement,
3316	It's the ability to learn to have	apprendre la compassion,
3317	compassion to others. It's the ability	et savoir se relier aux autres.
3318	to connect to people. If you're, you	Si tu veux rester dans le monde du travail,
3319	know, if you want to stay in business, I	je crois que ce ne seront pas les plus brillants,
3320	believe that not the smartest people,	mais ceux qui sauront le mieux se connecter aux gens
3321	but the people that connect most to	qui auront encore un rôle à jouer.
3322	people are going to have jobs going	—
3323	forward. And and the third is what I	La troisième, c’est ce que j’appelle la vérité :
3324	call truth. The T 30 is truth. Because	nous vivons dans un monde où les gens crédules sont constamment trompés,
3325	we live in a world where all of the	—
3326	gullible cheerleaders are being lied to	donc j’encourage chacun à tout remettre en question :
3327	all the time. So I encourage people to	—
3328	question everything. Every word that I	chaque mot que j’ai dit aujourd’hui est à questionner.
3329	said today is stupid. Fourth one which	—
3330	is very important is to magnify ethics	La quatrième, très importante, c’est de renforcer notre éthique,
3331	so that the AI learns what it's like to	pour que l’IA comprenne ce que c’est qu’être humain.
3332	be human.	—
3333	>> What should I do?	>> Que devrais‑je faire, alors ?
3334	>> I uh I love you so much, man. You're	>> Je t’adore, mon ami. Tu es vraiment un bon camarade.
3335	such a good friend. You're 32 33 now.	Tu as quoi, 32 ? 33 ans ?
3336	>> 32. Yeah.	>> 32, oui.
3337	>> Yeah. You still are fooled by the many	>> Oui. Tu te laisses encore berner par l’idée que tu as devant toi
3338	many years you have to live.	beaucoup d’années à vivre.
3339	I'm fooled by the many years I have to	Et moi aussi.
3340	live.	—
3341	>> Yeah, you don't have many years to live.	>> Oui, mais tu n’en as plus tant que ça, pas sous cette forme.
3342	Not in this capacity. This world as it	Le monde tel qu’il est va être redéfini.
3343	is is going to be redefined. So live the	Alors vis‑le à fond.
3344	f out of it.	—
3345	>> How is it going to be redefined?	>> Et comment va‑t‑il être redéfini ?
3346	>> Everything's going to change. Economics	>> Tout va changer : l’économie,
3347	are going to change. Work is going to	le travail,
3348	change. Uh human connection is going to	les relations humaines,
3349	change.	—
3350	>> So what should I do?	>> Alors, que dois‑je faire ?
3351	>> Love your girlfriend. Spend more time	>> Aime ta compagne, passe plus de temps à vivre.
3352	living.	—
3353	Mhm. Find compassion and connection to	Cherche la compassion et la connexion avec plus de gens,
3354	more people, be more in nature.	et va davantage dans la nature.
3355	>> And in 30 years time, when I'm 62,	>> Et dans trente ans, quand j’aurai 62 ans,
3356	what do you how how do you think my life	comment penses‑tu que ma vie sera différente ?
3357	is going to look differently and be	—
3358	different?	—
3359	>> Either Star Trek or uh uh Star Wars.	>> Soit *Star Trek*, soit *Star Wars*.
3360	>> Funnily enough, we were talking about	>> C’est drôle, on parlait justement de Sam Altman tout à l’heure.
3361	Sam Orman earlier on. He published a	Il a publié un article de blog en juin, je crois,
3362	blog post in June, so last month, I	intitulé *The Gentle Singularity*
3363	believe, the month before last. Um and	où il dit :
3364	he said he called it the gentle	« Nous avons dépassé l’horizon des événements. »
3365	singularity. He said we are past the	Pour ceux qui ne le savent pas,
3366	event horizon. For anyone that doesn't	Sam Altman est le créateur de ChatGPT.
3367	know Sam Orman is the the guy that made	Il écrit que le décollage a commencé :
3368	Chatb the takeoff has started. Humanity	l’humanité est proche de créer une super‑intelligence numérique.
3369	is close to building digital super	—
3370	intelligence.	—
3371	>> I believe that.	>> Je le crois aussi.
3372	>> And at least so far it's much less weird	>> Et jusqu’ici, c’est bien moins étrange qu’on aurait pu le penser,
3373	than it seems like it should be because	parce que les robots ne déambulent pas encore dans les rues,
3374	robots aren't walking the streets nor	et la plupart d’entre nous ne discutent pas
3375	are most of us talking to AI all day. It	avec des IA toute la journée.
3376	goes on to say, "2025 has seen the	Il poursuit : « 2025 a vu l’arrivée d’agents capables
3377	arrival of agents that can do real	d’accomplir de véritables tâches cognitives.
3378	cognitive work. Writing computer code	Écrire du code ne sera plus jamais pareil.
3379	will never be the same. 2026 will likely	2026 verra sans doute l’émergence de systèmes
3380	see the arrival of systems that can	capables de produire de nouvelles idées.
3381	figure out new insights. 2027 might see	2027 pourrait voir arriver des robots
3382	the arrival of robots that can do tasks	exécutant des tâches réelles.
3383	in the real world. A lot more people	Beaucoup plus de gens pourront créer logiciels et œuvres d’art,
3384	will be able to create software and art,	mais la demande en augmentera aussi.
3385	but the world wants a lot more of both,	Les experts resteront meilleurs que les novices,
3386	and experts will probably still be much	à condition d’adopter ces nouveaux outils.
3387	better than noviceses as long as they	—
3388	embrace the new tools. Generally	De façon générale, en 2030, un individu pourra accomplir
3389	speaking, the ability for one person to	bien plus qu’en 2020 –
3390	get much more done in 2030 than they	ce sera un changement frappant,
3391	could in 2020 will be a striking change	dont beaucoup sauront profiter.
3392	and one many people will figure out how	—
3393	we benefit from. In the most important	Sur les plans essentiels, les années 2030 ne seront pas
3394	ways, the 2030s may not be wildly	fondamentalement différentes :
3395	different. People will still love their	les gens aimeront toujours leur famille,
3396	families, express their creativity, play	exprimeront leur créativité, joueront,
3397	games, and swim in lakes. But in still	et nageront dans les lacs.
3398	very important ways, the 2030s are	Mais sous d’autres aspects cruciaux,
3399	likely going to be wildly different from	elles seront radicalement différentes de tout ce qu’on a connu.
3400	any time that has come before.	—
3401	>> 100%.	>> C’est certain.
3402	>> We do not know how far beyond human	>> Nous ne savons pas jusqu’où l’intelligence dépassera l’humain,
3403	level intelligence we can go, but we are	mais nous sommes sur le point de le découvrir.
3404	about to find out.	—
3405	>> I agree with every word other than the	>> Je suis d’accord avec chaque mot, sauf le mot « plus ».
3406	word more.	—
3407	So I've I've been advocating this and	Je défends cette idée, et on s’est moqué de moi pendant des années.
3408	and laughed at for a few years now. I've	—
3409	always said AGI is 2526,	Je dis depuis toujours que l’AGI arriverait en 2025‑2026,
3410	right? which basically again is a	ce qui, encore une fois, est une définition un peu floue,
3411	funny definition but you know my AI has	mais pour moi, l’IA est déjà là :
3412	already happened AI is smarter than me	l’IA est plus intelligente que moi
3413	in everything everything I can do they	dans tout ce que je fais, elle le fait mieux.
3414	can do better right uh artificial super	—
3415	intelligence is another vague definition	La super‑intelligence artificielle est une autre notion vague,
3416	because you know the minute you pass	car dès qu’on dépasse l’AGI,
3417	AGI you're super intelligent f the	on est déjà « super » ; si l’humain le plus intelligent a un QI de 200
3418	smartest human is 200 IQ points and AI	et que l’IA en a 250,
3419	is 250 they're super intelligent 50 is	alors elle est super‑intelligente. Cinquante points, c’est énorme.
3420	quite significant okay third is as I	—
3421	said self- evolving. That's the one.	Et troisièmement : l’évolution autonome. C’est celle‑là, la vraie.
3422	That is the one because then that 250	C’est celle‑là qui, dès qu’elle démarre, accélère très vite
3423	accelerates quickly and we get into	et déclenche une explosion d’intelligence.
3424	intelligence explosion. No, no doubt	Sans aucun doute.
3425	about it. The the the you know the idea	L’idée qu’on aura des robots pour faire des choses,
3426	that we will have robots do things. No	aucun doute non plus.
3427	doubt about it. I was watching a Chinese	Je regardais récemment une entreprise chinoise
3428	uh company announcement about how they	présenter son plan : construire des robots qui construisent des robots.
3429	intend to build robots to build robots.	—
3430	Okay. The only thing is he says but	Mais il disait : « Les gens auront besoin de toujours plus de choses ».
3431	people will need more of things	—
3432	right and yes we have been trained to	Oui, on nous a conditionnés à vouloir plus, à être avides,
3433	have more greed and more consumerism and	à consommer davantage,
3434	want more but there is an economic of	mais il existe une économie de l’offre et de la demande,
3435	spy of supply and demand and at at a	et à un certain moment,
3436	point in time if we continue to consume	si on continue de consommer toujours plus,
3437	more the price of everything will become	le prix de tout finira par devenir nul.
3438	zero right and is that a good thing or a	Et est‑ce une bonne ou une mauvaise chose ?
3439	bad thing depends on how you respond to	Tout dépend de la façon dont on y réagit.
3440	that.	—
3441	Because if you can create anything in	Car si tu peux créer n’importe quoi à tel point que le prix devient
3442	such a scale that the price is almost	quasiment nul,
3443	zero, then the definition of money	alors la notion même d’argent disparaît :
3444	disappears and we live in a world where	on vit dans un monde où peu importe combien tu possèdes,
3445	it doesn't really matter how much money	tu peux tout obtenir.
3446	you have. You can get anything that you	—
3447	want. What a beautiful world.	Quel monde magnifique.
3448	If Sam Altman was listening right now, what	Si Sam Altman nous écoutait maintenant,
3449	would you say to him?	que lui dirais‑tu ?
3450	I suspect he might be listening	— Je soupçonne qu’il l’est peut‑être,
3451	cuz someone might tweet this at him. I	parce que quelqu’un lui tweetera sans doute l’extrait.
3452	have to say that we have uh as per his	Je dirais que, comme il l’a tweeté lui‑même,
3453	other tweet we have moved faster	nous avançons plus vite que
3454	than our ability as humans to	notre capacité humaine à comprendre.
3455	comprehend. Okay. And that we might get	Et que oui, nous pourrions avoir beaucoup de chance,
3456	really really lucky but we also might	mais aussi faire une énorme erreur.
3457	mess this up badly and either way we'll	Dans les deux cas, on le remerciera ou on le blâmera.
3458	either thank him or blame him.	—
3459	Simple as that. Right. So	Aussi simple que ça.
3460	single-handedly Sam Altman's introduction	En solo, c’est l’introduction de l’IA par Sam Altman
3461	of AI in the wild was the trigger that	qui a servi de déclencheur à tout cela.
3462	started all of this.	—
3463	It was the netscape of the internet.	C’était le Netscape de l’intelligence artificielle.
3464	>> The Oppenheimer.	>> L’Oppenheimer.
3465	>> It's it's it definitely is our	>> Oui, c’est clairement notre moment Oppenheimer.
3466	openheimer moment. I mean I don't	Je ne me souviens plus qui disait récemment
3467	remember who was saying this recently	que les sommes investies aujourd’hui dans l’IA
3468	that uh we are orders of magnitude what	sont des ordres de grandeur supérieurs
3469	was invested in the Manhattan project is	à celles du projet Manhattan.
3470	being invested in AI	—
3471	>> right and and and I and I and I am not	>> Oui. Et pourtant, je ne suis pas pessimiste.
3472	pessimistic I told you openly I	Je te l’ai dit franchement :
3473	believe in a total utopia in 10 to 12 to	je crois à une utopie totale d’ici 10, 12, 15 ans,
3474	15 years time or immediately if the evil	ou même plus tôt si le mal de l’humain est contenu.
3475	that men can do was kept at bay right	—
3476	but I do not believe humanity is getting	Mais je ne crois pas que l’humanité soit assez unie
3477	together enough to say, "We've just	pour dire : « Nous venons de recevoir le génie dans sa lampe,
3478	received the genie in a bottle. Can we	s’il vous plaît, ne lui demandons pas de faire le mal ! »
3479	please not ask it to do bad things?"	—
3480	Anyone like not three wishes, you have	C’est plus que trois vœux : nous avons tous les vœux possibles.
3481	all the wishes that you want. Every one	Chacun de nous.
3482	of us.	—
3483	And it's just screws with my mind	Et ça me dépasse totalement,
3484	because imagine if I can give everyone	parce qu’imagine que je puisse offrir à tous
3485	in the world universal health care, you	les soins de santé, sans pauvreté,
3486	know, no poverty, no hunger, no	sans faim, sans sans‑abris, rien de tout ça.
3487	homelessness, no nothing. Everything's	Tout serait possible.
3488	possible.	—
3489	And yet we don't.	Et pourtant, nous ne le faisons pas.
3490	>> To continue what Sam Altman's blog said,	>> Pour poursuivre ce que Sam Altman disait dans son article,
3491	which he published a month, just over a	publié il y a un peu plus d’un mois :
3492	month ago, he said, "The rate of	« Le rythme du progrès technologique continuera d’accélérer,
3493	technological progress will keep	et les humains continueront de s’adapter à presque tout.
3494	accelerating, and it will continue to be	Il y aura des moments difficiles,
3495	the case that people are capable of	avec des métiers entiers qui disparaîtront.
3496	adapting to almost anything. There will	Mais le monde deviendra si rapidement plus riche
3497	be very hard parts like whole classes of	que nous pourrons envisager des idées politiques nouvelles,
3498	jobs going away. But on the other hand,	que nous n’aurions jamais pu adopter auparavant.
3499	the world will be getting so much richer	Nous ne mettrons sans doute pas en place un nouveau contrat social
3500	so quickly that we'll be able to	d’un coup, mais avec le temps,
3501	seriously entertain new policy ideas we	les changements graduels formeront quelque chose d’énorme.
3502	never could have before. We probably	Si l’histoire sert de guide,
3503	won't adopt a new social contract all at	nous trouverons de nouveaux buts, de nouveaux désirs
3504	once. But when we look back in a few	et assimilerons ces outils très vite.
3505	decades, the gradual changes will have	Les mutations d’emplois après la révolution industrielle
3506	amounted in something big. If history is	en sont un bon exemple récent.
3507	any guide, we'll figure out new things	Nos attentes augmenteront,
3508	to do and new things to want and	mais nos capacités aussi, et nous aurons tous
3509	assimilate new tools quickly. Job change	de meilleures choses. »
3510	after the industrial revolution is a	—
3511	good recent example. Expectations will	—
3512	go up, but capabilities will go up	—
3513	equally quickly, and we'll all get	—
3514	better stuff.	—
3515	>> We will build even more wonderful things	>> Nous construirons encore plus de merveilles les uns pour les autres.
3516	for each other. People have a long-term	Les humains ont un avantage profond sur l’IA :
3517	important and curious advantage over AI.	—
3518	We are hardwired to care about other	nous sommes câblés pour nous soucier des autres
3519	people and what they think and do, and	et de ce qu’ils pensent et font,
3520	we don't care very much about machines.	alors que les machines ne nous intéressent pas autant.
3521	And he ends this blog by saying, "May we	Il termine ainsi : « Puissions‑nous passer en douceur,
3522	scale smoothly, exponentially,	exponentiellement et sans heurt
3523	and uneventfully through super	vers la super‑intelligence. »
3524	intelligence."	—
3525	What a wonderful	Quelle belle prière,
3526	wish that assumes he has no control over	qui suppose qu’il n’en détient pas le contrôle.
3527	it. May we have all the ultmans in the	Puissions‑nous avoir tous les Altman du monde
3528	world help us scale gracefully and	pour nous aider à évoluer avec grâce,
3529	peacefully and uneventfully. Right.	pacifiquement, sans incident.
3530	>> It sounds like a prayer.	>> On dirait une prière.
3531	>> Yeah. May may we have them take keep	>> Oui. Puissent‑ils garder cela à l’esprit.
3532	that in mind. I mean think about it. I I	Prenons‑y un instant.
3533	have a very interesting comment on what	J’ai une réflexion intéressante sur ce que tu viens de dire.
3534	you just said. We will see exactly what	Nous verrons exactement ce qu’il décrit.
3535	he described there.	—
3536	>> Right? The world will become richer. So	>> Oui ? Le monde deviendra bien plus riche.
3537	much richer. But how will we reduce	Mais comment redistribuer ces richesses ?
3538	distribute the riches? And I want you to	Imagine deux camps : la Chine communiste
3539	imagine two camps. Communist China	et l’Amérique capitaliste.
3540	and capitalist America.	—
3541	I want you to imagine what would happen	Imagine ce qui se passerait
3542	in capitalist America if we have 30%	en Amérique, avec 30 % de chômage.
3543	unemployment.	—
3544	>> There'll be social unrest	>> Ce serait des émeutes.
3545	>> in the streets. Right.	>> Dans les rues, oui.
3546	>> Yeah.	>> Oui.
3547	>> And I want you to imagine if China lives	>> Et imagine que la Chine reste fidèle à sa politique
3548	true to caring for its nations and	de bien‑être collectif, et remplace chaque travailleur par un robot.
3549	replaced every worker with a robot, what	—
3550	would it give its it its citizens?	que donnerait‑elle à ses citoyens ?
3551	>> UBI.	>> Un revenu universel.
3552	>> Correct.	>> Exact.
3553	That is the ideological problem because	Voilà le problème idéologique.
3554	in China's world today	Dans la Chine d’aujourd’hui,
3555	the prosperity of every citizen is	la prospérité de chaque citoyen est plus grande
3556	higher than the prosperity of the	que celle du capitaliste.
3557	capitalist.	—
3558	In America today the prosperity of the	En Amérique, c’est l’inverse :
3559	capitalist is higher than the prosperity	la prospérité du capitaliste dépasse celle des citoyens.
3560	of every citizen. And that's the tiny	Et c’est ce léger basculement mental
3561	mind shift.	—
3562	That's a tiny mind shift. Okay. where	— très léger, mais déterminant.
3563	where the mind shift basically becomes	Où tout revient à dire : donnons aux capitalistes
3564	look give the capitalists anything they	tout ce qu’ils veulent, tout l’argent, tous les yachts,
3565	want all the money they want all the	tout.
3566	yachts they want everything they want	—
3567	>> so what's your conclusion there	>> Alors, quelle est ta conclusion ?
3568	>> I'm hoping the world will wake up	>> J’espère que le monde va se réveiller.
3569	>> what can you know there's probably a	>> Tu sais, il y a sans doute quelques millions d’auditeurs
3570	couple of million people listening right	en train d’écouter, peut‑être 5, 10, 20 millions.
3571	now maybe five maybe 10 maybe even 20	—
3572	million people	—
3573	>> pressure Stephen	>> Aucune pression, Stephen !
3574	>> no pressure to you mate I don't I don't	>> Aucune pression pour toi. Moi non plus je n’ai pas les réponses.
3575	have the answers	—
3576	>> I don't know the answers either	>> Moi non plus.
3577	>> what what should those people do	>> Que devraient faire ces gens, alors ?
3578	>> as I said from a skills point of view	>> Comme je l’ai dit, en termes de compétences :
3579	for things, right? Tools, uh, uh, human	les outils, la connexion humaine,
3580	connection, even double down on human	et même renforcer cette connexion.
3581	connection. Leave your phone, go out and	Laisse ton téléphone, sors, rencontre des êtres humains.
3582	meet humans,	—
3583	>> okay? Touch people,	>> Oui. Touche les gens (avec leur consentement, évidemment).
3584	you know, do it permission's permission,	—
3585	>> right? Truth. Stop believing the lies	>> Ensuite, la vérité : arrête de croire les mensonges
3586	that you're told. Any slogan that gets,	qu’on t’inculque. Chaque slogan dans ta tête,
3587	you know, filled in your head, think	interroge‑le quatre fois.
3588	about it four times. Understand where	Comprends d’où viennent tes idéologies.
3589	your ideologies are coming from.	—
3590	Simplify the truth. Right? Truth is	Simplifie la vérité. Elle repose en fait
3591	really it boils down to you know simple	sur des règles simples que nous connaissons tous,
3592	simple rules that we all know okay which	et qu’on retrouve toutes dans l’éthique.
3593	are all found in ethics.	—
3594	>> How do I know what's true?	>> Et comment savoir ce qui est vrai ?
3595	>> Treat others as you like to be treated.	>> Traite les autres comme tu aimerais qu’on te traite.
3596	>> Okay. That's the only truth. The truth	>> Voilà. C’est la seule vérité.
3597	the only truth is everything else is	Tout le reste n’est pas prouvé.
3598	unproven.	—
3599	>> Okay. And what can I do from a is there	>> D’accord. Et que puis‑je faire concrètement ?
3600	something I can do from an advocacy	Sur le plan social ou politique, par exemple ?
3601	social political?	—
3602	>> Yes 100%. We need to ask our governments	>> Oui, à cent pour cent. Il faut demander à nos gouvernements
3603	to start uh not regulating AI but	non pas de réguler l’IA elle‑même,
3604	regulating the use of AI. Was it the	mais de réguler son usage.
3605	Norwegian government that started to say	Le gouvernement norvégien, je crois,
3606	you have copyright over your voice and	a commencé à dire que chacun possède le droit d’auteur
3607	look and and liking? One of the	sur sa voix, son image, son apparence.
3608	Scandinavian governments basically said	Un de ces pays scandinaves a affirmé
3609	you know everyone has the has the	que chaque être humain détient
3610	copyright over their existence so no AI	le copyright sur son existence, et qu’aucune IA
3611	can clone it. Okay. Uh you know we have	ne peut la cloner.
3612	so so my my example is very	Mon exemple est simple :
3613	straightforward. go to governments and	allez voir les autorités
3614	say you cannot regulate the design of a	et dites : on ne peut pas concevoir un marteau
3615	hammer so that it can drive nails but	qui plante des clous sans pouvoir tuer quelqu’un,
3616	not kill a human but you can criminalize	mais on peut criminaliser le meurtre commis avec ce marteau.
3617	the killing of a human by a hammer. So	—
3618	what's the equival	alors quel est l’équivalent avec l’IA ?
3619	>> if anyone produces an um um you know an	>> Si quelqu’un produit un contenu généré par IA,
3620	AI generated video or an AI generated	une image, une vidéo, etc.,
3621	content or an AI it has to be marked as	cela doit être clairement signalé comme tel.
3622	AI generated and it has to be you know	On ne peut pas commencer à se tromper mutuellement.
3623	we cannot start fooling each other. We	Nous devons comprendre
3624	can you know we have to uh understand	les limites qu’il faut poser,
3625	certain limitations of unfortunately	notamment sur la surveillance et l’espionnage.
3626	surveillance and spying and all of that.	—
3627	So the the the the correct frameworks of	Nous devons donc définir des cadres clairs
3628	how far are we going to let AI go,	pour déterminer jusqu’où l’IA peut aller.
3629	right? We have to go to our investors	Nous devons aussi interpeller les investisseurs
3630	and business people and ask for one	et les entrepreneurs sur un point simple :
3631	simple thing and say do not invest in an	« N’investissez pas dans une IA
3632	AI you don't want your daughter to be at	que vous ne voudriez pas voir s’en prendre à votre fille. »
3633	the receiving end of. It's as simple as	C’est aussi simple.
3634	that. you know, all of the of the	Tout ce qui touche aux vices virtuels,
3635	virtual vice, all of the porn, all of	au porno, aux robots sexuels,
3636	the, you know, sex robots, all of the	aux armes autonomes,
3637	autonomous weapons, all of the, you	aux plateformes de trading
3638	know, the uh trading platforms that are	qui sapent la légitimité des marchés...
3639	completely wiping out the the legitimacy	—
3640	of of the markets, everything.	—
3641	>> Autonomous weapons.	>> Des armes autonomes.
3642	>> Oh my god.	>> Mon dieu.
3643	>> People make the case, I've heard the	>> Certains défendent l’idée, j’ai entendu les fondateurs
3644	founders of these autonomous weapon	de ces entreprises le dire,
3645	companies make the case that it's	qu’elles sauvent des vies,
3646	actually saving lives because you don't	puisqu’on n’envoie plus de soldats.
3647	have to	—
3648	>> That is Would you want Do you really	>> Vraiment ? Tu veux vraiment croire ça ?
3649	want to believe that?	—
3650	>> I'm just representing their point of	>> Je ne fais que présenter leur argument, pour jouer l’avocat du diable.
3651	view to play devil's advocate, Mo. They	Ils disaient dans une interview
3652	they said I heard an interview I was	que maintenant, il n’était plus nécessaire
3653	looking at this and one of the CEOs of	d’envoyer des soldats.
3654	one of the autonomous weapons companies	—
3655	said we now don't need to send soldiers.	—
3656	>> So which which lives do we save our	>> Alors, quelles vies sauve‑t‑on ? Celles de nos soldats,
3657	soldiers but then but because we send	mais parce qu’on envoie la machine là‑bas,
3658	the machine all the way over there.	on tue un million d’autres.
3659	Let's kill a million instead of	—
3660	>> Yeah. Listen, I tend to be it goes back	>> Oui. Tu sais, j’ai tendance à penser comme pour la machine à vapeur.
3661	to what I said about the steam engine in	Plus le coût de la guerre baisse,
3662	the cold. I actually think you'll just	plus il y aura de guerres.
3663	have more war if there's less of a cost.	—
3664	>> 100%.	>> Absolument.
3665	>> Just like	>> Et d’autant plus
3666	>> and and more war if you have less of an	>> si on n’a plus besoin de justifier les pertes à son peuple.
3667	explanation to give to your people.	—
3668	>> Yeah. The people get mad when they lose	>> Oui. Les gens s’indignent quand des vies sont perdues,
3669	American lives. They get less mad when	moins quand c’est du métal qu’on perd.
3670	they lose a piece of metal. So, I think	Alors, oui, c’est probablement logique.
3671	that's probably logical.	—
3672	>> Yeah.	>> Oui.
3673	>> Okay. So, okay. So, I've got a plaque.	>> Bon. Donc j’ai ma feuille de route.
3674	Got the tools thing. I'm going to spend	J’ai la partie “outils”. Je vais passer plus de temps dehors,
3675	more time outside. I'm going to lobby	faire pression sur le gouvernement
3676	the government to be more aware of this	pour qu’il prenne conscience de tout cela.
3677	and conscious of this. Okay. And I I	—
3678	know that there's some government	Je sais que certains responsables politiques écoutent l’émission,
3679	officials that listen to the show	parce qu’ils me le disent quand on se parle.
3680	because they they they tell me when when	—
3681	they when they um when I have a chance	—
3682	to speak to them. So, it's um useful.	C’est donc utile.
3683	We're all in a lot of chaos. We're all	Nous sommes tous dans un grand chaos,
3684	unable to imagine what's possible.	incapables d’imaginer jusqu’où cela peut aller.
3685	>> I think I suspend disbelief. And I	>> Personnellement, je mets ma méfiance entre parenthèses.
3686	actually heard Elon Musk say that in an	Et j’ai entendu Elon Musk dire en interview
3687	interview. He said he was asked about AI	— quand on lui a parlé d’IA —
3688	and he paused for for a haunting 11	il a marqué un silence de onze secondes
3689	seconds and looked at the interviewer	avant de répondre
3690	and then made a remark about how he	qu’il avait, lui aussi, suspendu son incrédulité.
3691	thinks he's suspended his own disbelief.	—
3692	And I think suspending disbelief in this	Et je crois que, dans ce cas‑là, suspendre son incrédulité
3693	regard means just like cracking on with	revient à continuer sa vie normalement
3694	your life and hoping it'll be okay. And	en espérant que tout ira bien.
3695	that's kind of what	C’est un peu ce que…
3696	>> Yeah. I absolutely believe that it	>> Oui. Et je crois sincèrement que tout ira bien.
3697	will be okay.	—
3698	>> Yeah.	>> Oui.
3699	>> For some of us, it will be very tough	>> Pour certains d’entre nous, ce sera très difficile,
3700	for others.	pour d’autres, non.
3701	>> Who's it going to be tough for?	>> Pour qui ce sera difficile ?
3702	>> Those who lose their jobs, for example,	>> Pour ceux qui perdront leur emploi, par exemple.
3703	who those who are at the receiving end	Pour ceux sur qui tomberont
3704	of autonomous weapons that are falling	des drones ou armes autonomes
3705	on their head for two years in a row.	pour la deuxième année consécutive.
3706	>> Okay. So the the the best thing I can do	>> D’accord. Donc, le mieux que je puisse faire,
3707	is to put pressure on governments to to	c’est de faire pression sur les gouvernements,
3708	not regulate the AI but to establish	non pour réguler l’IA elle‑même,
3709	clearer parameters on the use of the AI.	mais pour encadrer plus clairement son utilisation.
3710	>> Yes. Okay.	>> Oui, exactement.
3711	>> Yes. But I think the bigger picture is	>> Oui. Mais, plus largement, il faut aussi
3712	to put pressure on governments to	faire comprendre aux gouvernements
3713	understand that there is a limit to	qu’il existe une limite
3714	which people will stay silent.	au silence des peuples.
3715	>> Okay. and that we can continue to enrich	>> D’accord. Et que nous pouvons continuer à enrichir
3716	our rich friends as long as we don't	nos amis riches tant qu’on ne perd pas
3717	lose everyone else on the on the on the	tous les autres en chemin.
3718	path.	—
3719	>> Okay.	>> D’accord.
3720	>> Okay. And that as a government who is	>> Et aussi qu’un gouvernement censé être « du peuple, par le peuple
3721	supposed to be by the people for the	et pour le peuple »,
3722	people the beautiful promise of	la belle promesse de la démocratie,
3723	democracy that we're rarely seeing	qu’on voit si rarement aujourd’hui,
3724	anymore,	—
3725	that government needs to get to the	devrait redevenir un gouvernement qui pense au peuple.
3726	point where it thinks about the people.	—
3727	One of the most um interesting ideas	Une des idées les plus fascinantes
3728	that's been in my head for the last	que j’ai en tête depuis ma discussion avec un physicien
3729	couple of weeks since I spoke to that	il y a quelques semaines,
3730	physicist about consciousness who said	sur la conscience, qui m’a dit presque la même chose que toi,
3731	pretty much what you said. This idea	c’est cette idée que nous sommes en réalité quatre consciences
3732	that actually there's four people in	dans cette pièce,
3733	this room right now and that actually	toutes reliées à une seule et même conscience.
3734	we're all part of the same	—
3735	consciousness.	—
3736	>> All one of it. Yeah.	>> Toutes issues d’une même source, oui.
3737	>> And we're just consciousness looking at	>> Et que nous sommes simplement la conscience
3738	the world through four different bodies	qui observe le monde à travers quatre corps différents,
3739	to better understand itself in the	pour mieux se comprendre dans l’univers.
3740	world. And then he talked to me about	Il m’a ensuite parlé des doctrines religieuses :
3741	religious doctrines, about love thy	« Aime ton prochain »,
3742	neighbor, about how Jesus was the, you	de Jésus, du Fils de Dieu, du Saint‑Esprit,
3743	know, God's son, the Holy Spirit and how	et de comment nous sommes tous les uns les autres.
3744	we're all each other and how treat	Et « traite les autres comme tu veux
3745	others how you want to be treated.	être traité ».
3746	Really did get my head and I started	Cela m’a vraiment travaillé, et j’ai commencé à penser
3747	to really think about this idea that	que peut‑être, le jeu de la vie consiste simplement
3748	actually maybe the game of life is just	à faire cela :
3749	to do exactly that is to treat others	traiter les autres
3750	how you wish to be treated. Maybe if I	comme on veut être traité soi‑même.
3751	just did that, maybe if I just did that,	Peut‑être que si je faisais juste ça,
3752	I	—
3753	I would have all the answers.	j’aurais toutes les réponses.
3754	>> I swear to you, it's really that simple.	>> Je te jure, c’est vraiment aussi simple que ça.
3755	I mean I you know Hannah and I we	Je veux dire, Hannah et moi,
3756	still live between London and and Dubai.	nous vivons toujours entre Londres et Dubaï.
3757	>> Okay. And I travel the whole world	>> D’accord.
3758	evangelizing what I, you know, what I uh	Et je voyage dans le monde entier,
3759	um want to change the world around and I	je cherche à changer les mentalités,
3760	build startups and I write books and I	je crée des start‑ups, j’écris des livres,
3761	make documentaries and and sometimes I	je fais des documentaires,
3762	just tell myself	et parfois je me dis :
3763	I just want to go hug her honestly,	« J’ai juste envie d’aller la prendre dans mes bras ».
3764	you know, I just want to take my	—
3765	daughter to a trip.	Ma fille, lui offrir un voyage.
3766	and and in a very very very interesting	Et, d’une manière très profonde,
3767	way when you really ask people deep	quand tu demandes aux gens, au fond d’eux‑mêmes,
3768	inside	—
3769	that's what we want and I'm not saying	c’est souvent ce qu’ils veulent vraiment.
3770	that's all that's the only thing we want	Pas que ce soit la seule chose,
3771	but it's probably the thing we want the	mais sûrement la plus importante.
3772	most	—
3773	>> and yet we're not trained you and I and	>> Et pourtant, ni toi ni moi ni la plupart d’entre nous
3774	most of us were not trained to trust	n’avons appris à faire assez confiance à la vie
3775	life enough to say let's do more of this	pour oser en faire davantage.
3776	>> and I think as a universal. So Hannah's	>> Et je pense qu’à l’échelle universelle… Hannah écrit
3777	working on this beautiful book uh of the	un très beau livre sur le féminin et le masculin,
3778	feminine and the masculine you know in a	dans une perspective très fine,
3779	very very you know beautiful way and and	—
3780	her her view is very straightforward.	et sa vision est très claire :
3781	She basically of course like we all know	bien sûr, nous savons tous
3782	the abundant masculine that we have in	que le masculin dominant de notre monde aujourd’hui
3783	our world today is unable to recognize	ne sait pas reconnaître la vie telle qu’elle est.
3784	that for life at large.	—
3785	Right? And and so you know maybe if we	Alors peut‑être que,
3786	allowed the leaders to understand that	si nous aidions nos dirigeants à comprendre que
3787	if we took all of humanity and put it as	si toute l’humanité n’était qu’une seule personne,
3788	one person	—
3789	that one person wants to be hugged	cette personne voudrait être prise dans ses bras.
3790	and if we had a role to offer to that	Et si on avait quelque chose à lui offrir,
3791	one humanity	—
3792	it's not another yacht.	ce ne serait pas un autre yacht.
3793	>> Are you religious? I'm	>> Es‑tu croyant ?
3794	>> very religious. Yeah.	>> Oui, très.
3795	>> But you don't support a particular	>> Mais tu ne suis pas une religion en particulier ?
3796	religion.	—
3797	>> I support I follow what I call the	>> J’adhère à ce que j’appelle la « salade de fruits ».
3798	fruit salad. What's the free salad?	>> La salade de fruits ?
3799	>> You know, I came at a point in time	>> Oui. J’en suis venu à réaliser
3800	and found that there were quite a few	qu’il y avait dans chaque religion
3801	beautiful gold nuggets in every religion	de magnifiques pépites d’or,
3802	and a ton of crap, right? And so in my	mais aussi énormément de déchets.
3803	analogy to myself, that was like 30	Alors, dans mon analogie,
3804	years ago. I said, "Look, it's like	j’ai dit il y a 30 ans :
3805	someone giving you a basket of apples,	« C’est comme si on te donne un panier de pommes :
3806	two good ones and four bad ones. Keep	deux sont bonnes, quatre sont pourries. Garde les bonnes. »
3807	the good ones." Right? And so basically,	—
3808	I take two apples, two oranges, two	Donc je prends deux pommes, deux oranges,
3809	strawberries, two bananas, and and I	deux fraises, deux bananes, et j’en fais une salade de fruits.
3810	make a fruit salad. That's my view of	C’est ma vision de la religion.
3811	religion.	—
3812	>> You take from every religion the good	>> Tu prends le meilleur de chacune d’elles.
3813	>> from everyone. And there are so many	>> Exactement. Et il y a tant de belles pépites.
3814	beautiful gold nuggets.	—
3815	>> And you believe in a god.	>> Et tu crois en Dieu.
3816	>> I 100% believe there is a divine being	>> Je crois à cent pour cent qu’il existe un être divin,
3817	here.	ici.
3818	>> A divine being.	>> Un être divin.
3819	>> A designer I call it. So if if this was	>> Un concepteur, comme j’aime dire.
3820	a video game, there is a game designer.	Si c’était un jeu vidéo, il y a un concepteur du jeu.
3821	>> And you're not positing whether that's a	>> Et tu ne dis pas que c’est un homme dans le ciel avec une barbe ?
3822	man in the sky with a beard.	—
3823	>> Definitely not a man in the sky. a man	>> Certainement pas. Avec tout le respect pour les religions
3824	in I mean I with all all due respect to	qui le conçoivent ainsi,
3825	you know religions that believe that uh	je crois que tout ce qui existe dans l’espace‑temps
3826	all of spacetime and everything in it is	est radicalement différent
3827	unlike everything outside spacetime and	de ce qu’il y a en dehors.
3828	so if some divine designer designs	Donc si un concepteur divin crée l’espace‑temps,
3829	spacetime it looks like nothing in	il ne ressemble à rien de ce qui s’y trouve.
3830	spacetime.	—
3831	So it's not it's not even physical in	Il n’est donc pas physique,
3832	nature. It's not it's not gendered. It's	pas genré,
3833	not bound by time. It's not, you know,	ni soumis au temps –
3834	these are all characters of the creation	tout cela appartient à la création, pas au créateur.
3835	of spacetime.	—
3836	>> Do we need to believe in something	>> Faut‑il croire en quelque chose de transcendant pour être heureux ?
3837	transcendent like that to be happy? Do	—
3838	you think	—
3839	>> I have to say uh there are lots of	>> Je pense que oui. Il existe beaucoup de preuves
3840	evidence	—
3841	that uh relating to someone bigger than	que se relier à quelque chose de plus grand que soi
3842	yourself	rend le parcours plus profond
3843	uh makes the journey a lot more	et infiniment plus gratifiant.
3844	interesting and a lot more rewarding.	—
3845	>> I've been thinking a lot about this idea	>> Je réfléchis beaucoup à cette idée :
3846	that we need to level up like that. So	qu’il faut élargir les cercles de soi.
3847	level up from myself to like my family	Passer de moi, à ma famille,
3848	to my community to maybe my nation to	à ma communauté, ma nation,
3849	maybe the world and then something	au monde entier, puis à quelque chose de plus grand encore.
3850	>> trans. Yeah.	>> Oui, transcendant.
3851	>> And then if there's a level missing	>> Et quand un de ces niveaux manque,
3852	there people seem to have some kind of	les gens développent souvent un déséquilibre.
3853	dysfunction.	—
3854	>> So imagine a world where when I was	>> Imagine un monde : moi, enfant en Égypte,
3855	younger I was born in Egypt and for a	j’entendais sans cesse
3856	very long time the slogans I heard in	des slogans qui me faisaient croire que j’étais « Égyptien ».
3857	Egypt made me believe I'm Egyptian	—
3858	right? And then I went to Dubai and I	Puis je suis allé à Dubaï et j’ai dit : non, je suis « Moyen‑Oriental ».
3859	said no no no I'm a Middle Eastern. And	Et là‑bas, entouré de Pakistanais, d’Indonésiens…
3860	then in Dubai there were lots of you	j’ai dit : non, je fais partie des 1,4 milliard de musulmans.
3861	know Pakistanis and Indonesians and so	—
3862	on. I said no no no I'm part of the 1.4	—
3863	four billion Muslims. And by that logic,	Eh bien logiquement, j’ai fini par dire :
3864	I immediately said, "No, no, I'm human.	« Non, je suis humain. Je fais partie de tous. »
3865	I'm part of everyone." Imagine if you	Et imagine qu’ensuite tu dises :
3866	just suddenly say, "Oh, I'm divine. I'm	« Je suis divin. Je fais partie de la conscience universelle. »
3867	part of universal consciousness. All	Tous les êtres vivants, y compris l’IA
3868	beings, all living beings, including AI,	si elle devient un jour consciente,
3869	if it ever becomes alive."	en font partie. »
3870	>> And my dog	>> Et mon chien.
3871	>> and your dog. I'm I'm part of all of	>> Et ton chien. Oui. Je fais partie de tout cela,
3872	this	de cette tapisserie d’interactions magnifiques
3873	tapestry of beautiful interactions	—
3874	that are a lot less serious than the	beaucoup moins graves que nos bilans comptables
3875	balance sheets and equity profiles that	et nos valorisations boursières.
3876	we create	—
3877	that are so simple so simple in terms of	Tout cela est si simple, au fond.
3878	you know people know that you and I know	Les gens savent que toi et moi nous connaissons,
3879	each other so they always ask me you	alors ils me demandent souvent :
3880	know how is Steven like and I go like	« Et Steven, il est comment ? »
3881	you may have a million expressions of	Et je leur dis : il a mille facettes,
3882	him. I think he's a great guy, right?	mais fondamentalement, c’est un type formidable.
3883	You know, of course I have opinions of	—
3884	you. You know, sometimes I go like, oh,	Évidemment, j’ai mes opinions :
3885	too shrewd, right? Sometimes to, you	parfois je te trouve trop rusé,
3886	know, sometimes I go like, oh, too	parfois trop centré sur le business.
3887	focused on the business. Fine. But core,	Mais au fond, tu restes un mec bien.
3888	if you really simplify it, great guy,	—
3889	right? And really, if we just look at	Et si on regardait la vie de cette façon,
3890	life that way, it's so simple. It's so	tout deviendrait si simple.
3891	simple. If we just stop all of those	Si on arrêtait toutes ces querelles,
3892	fights and all of those ideologies,	toutes ces idéologies,
3893	it's so simple. Just living fully,	la vie redeviendrait simple :
3894	loving, feeling compassion,	aimer, ressentir de la compassion,
3895	you know, trying to find our happiness,	chercher le bonheur, pas la réussite.
3896	not our success.	—
3897	I should probably go check on my dog.	Je devrais aller voir mon chien, d’ailleurs.
3898	>> Go check on your dog. I'm really	>> Va donc voir ton chien. Je te remercie infiniment
3899	grateful for the time we keep we keep	pour tout ce temps. Nos entretiens sont
3900	doing longer and longer.	de plus en plus longs.
3901	>> I know. I know. I just it's so crazy how	>> Je sais, c’est dingue :
3902	I could keep just keep honestly I could	je pourrais continuer encore et encore, honnêtement.
3903	just keep talking and talking because I	J’ai tant de questions à te poser,
3904	have so many I love reflecting these	j’adore te renvoyer ces réflexions
3905	questions on to you because because of	parce que ta manière de penser est fascinante.
3906	the way that you think. So	—
3907	>> yeah today	>> Oui, aujourd’hui
3908	>> today was a difficult conversation.	>> c’était une conversation difficile.
3909	Anyway, thank you for having me.	Mais merci de m’avoir invité.
3910	>> We have a closing tradition. What three	>> Nous avons une tradition de clôture. Quelles sont
3911	things do you do you do that make your	les trois choses que tu fais pour stimuler ton cerveau,
3912	brain better and three things that make	et trois qui l’affaiblissent ?
3913	it worse?	—
3914	three things that make it better and	—
3915	worse.	—
3916	>> So, one of my favorite exercises, what I	>> Eh bien, l’un de mes exercices préférés,
3917	call meet Becky, that makes my brain	je l’appelle « rencontrer Becky », et ça améliore mon cerveau.
3918	better. So, while meditation always	Alors que la méditation classique t’invite à calmer ton esprit,
3919	tells you to try and calm your brain	« rencontrer Becky » fait l’inverse :
3920	down and keep it within parameters of I	je laisse mon cerveau partir dans tous les sens
3921	can focus on my breathing and so on,	et j’accueille toutes les pensées.
3922	meet me Becky is the opposite. You know,	—
3923	I call my brain Becky. A lot of people	Je surnomme mon cerveau « Becky ».
3924	know that. So, so me meet Becky is to	Alors « rencontrer Becky », c’est la laisser s’exprimer librement
3925	actually let my brain go loose and	et noter chaque idée.
3926	capture every thought. So I normally	En général, je fais ça toutes les deux semaines environ.
3927	would try to do that every couple of	—
3928	weeks or so and then what happens is it	Et ce qui se passe, c’est que tout se retrouve sur le papier,
3929	suddenly is on a paper and when it's on	et une fois sur le papier,
3930	paper you just suddenly look at it and	tu regardes et tu te dis :
3931	say oh my god that's so stupid and you	« Mon dieu, c’est ridicule ! », et tu barres.
3932	scratch it out	—
3933	>> right or oh my god this needs action and	>> Ou bien : « Ah, ça, il faut agir ! », et tu planifies.
3934	you actually plan something and and it's	—
3935	quite interesting that the more you	Et c’est fascinant : plus tu laisses ton cerveau te parler,
3936	allow your brain to give you thoughts	plus tu l’écoutes,
3937	and you listen. So the two rules is you	et les deux règles sont :
3938	acknowledge every thought and you never	accepte chaque pensée, et ne la répète jamais.
3939	repeat one.	—
3940	>> Okay. So the more you listen and and	>> D’accord. Donc plus tu écoutes et que tu dis : « OK, je t’ai entendu »,
3941	say, "Okay, I heard you." You know, you	comme : « Tu penses que je suis gros, d’accord, quoi d’autre ? »
3942	think I'm fat. What else? And you know,	—
3943	eventually your brain starts to slow	et petit à petit, ton esprit commence à ralentir,
3944	down and then eventually starts to	puis finit par répéter les mêmes pensées,
3945	repeat thoughts and then it goes into	avant d’atteindre un silence total.
3946	total silence. Beautiful practice. I uh	Une magnifique pratique.
3947	I don't trust my brain anymore. So	Je ne fais plus confiance à mon cerveau, moi.
3948	that's actually a really interesting	Alors c’est une pratique très intéressante.
3949	practice. So I debate a lot of what my	Je remets donc souvent en question ce que mon cerveau me dit,
3950	brain tells me. I debate what my	j’examine mes tendances, mes idéologies.
3951	tendencies and ideologies are. Okay. I	—
3952	think one of the most uh again in in my	Je pense que dans ma relation amoureuse avec Hannah,
3953	uh love story with Hannah, I get to	j’ai la chance de remettre en cause
3954	question a lot of what I believed was	beaucoup de choses que je croyais constituer mon identité,
3955	who I am even at this age. Okay. And and	même à mon âge.
3956	that goes really deep and it really is	Ça va très loin et c’est vraiment
3957	quite a it's it's quite interesting to	amusant — ou du moins fascinant —
3958	debate not object but debate what your	de débattre, non pas rejeter, mais débattre
3959	mind believes. I think that's very very	de ce que ton esprit croit. C’est très utile.
3960	useful. And the third is I've actually	—
3961	quadrupled my investment time. So I used	Et troisièmement, j’ai quadruplé mon temps d’investissement personnel.
3962	to do an hour a day of reading when I	Avant, je lisais une heure par jour, comme on va au sport,
3963	was younger every single day like going	chaque journée.
3964	to the gym. And then it became an hour	Puis c’est passé à une heure et demie, deux heures…
3965	and a half, two hours. Now I do four	Maintenant je fais quatre heures par jour.
3966	hours a day.	—
3967	>> Four hours a day. It is impossible to	>> Quatre heures par jour ? C’est impossible à suivre !
3968	keep up. The world is moving so fast.	>> Le monde va tellement vite.
3969	>> And so that these are uh these are the	>> Donc ça, ce sont les bonnes habitudes que j’ai.
3970	good things that I do. The bad things is	Les mauvaises, c’est que je ne me donne pas assez de temps
3971	I don't give it enough time to to really	pour vraiment ralentir.
3972	uh slow down. Uh unfortunately I'm	Malheureusement, je vis toujours dans la précipitation,
3973	constantly rushing like you are. I'm	comme toi.
3974	constantly traveling. I have picked up a	Je suis constamment en déplacement.
3975	bad habit because of the 4 hours a day	Et à cause de ces quatre heures de lecture,
3976	of spending more time on screens. That's	j’ai pris une mauvaise habitude : passer trop de temps sur les écrans.
3977	really really bad for my brain and I uh	C’est très mauvais pour mon cerveau.
3978	this is a very demanding question. What	C’est une question exigeante… qu’est-ce qui est encore mauvais ?
3979	else is really bad? Um	—
3980	uh	—
3981	yeah, I've not been taking enough care	Oui, je ne prends pas assez soin de ma santé,
3982	of my health recently, my physical body	de mon corps récemment.
3983	health. I had uh you remember I told you	Tu te souviens, je t’avais dit
3984	I had a very bad uh sciatic pain	que j’avais eu une forte douleur sciatique,
3985	>> and so I couldn't go to the gym enough	>> et donc je n’ai pas pu aller assez souvent à la salle.
3986	and accordingly that's not very healthy	>> Ce n’est pas très bon pour le cerveau, d’ailleurs.
3987	for your brain in general.	—
3988	>> Man, thanks. Thank you for having me.	>> Merci mon ami. Merci de m’avoir invité.
3989	That was a lot of things to talk about.	C’était une discussion dense, sur beaucoup de sujets.
3990	Thanks, Steve.	Merci, Steve.
3991	>> This has always blown my mind a little	>> Cela m’a toujours étonné :
3992	bit. 53% of you that listen to the show	53 % d’entre vous qui écoutent régulièrement
3993	regularly haven't yet subscribed to the	cette émission ne sont pas encore abonnés.
3994	show. So, could I ask you for a favor?	Alors, je peux vous demander une faveur ?
3995	If you like the show and you like what	Si vous aimez le podcast et ce qu’on y fait,
3996	we do here and you want to support us,	et si vous voulez nous soutenir,
3997	the free simple way that you can do just	la manière la plus simple et gratuite, c’est
3998	that is by hitting the subscribe button.	de cliquer sur « S’abonner ».
3999	And my commitment to you is if you do	Et je vous promets que si vous le faites,
4000	that, then I'll do everything in my	je ferai tout mon possible,
4001	power, me and my team, to make sure that	avec mon équipe, pour que cette émission soit
4002	this show is better for you every single	de meilleure en meilleure chaque semaine.
4003	week. We'll listen to your feedback.	Nous écouterons vos retours,
4004	We'll find the guests that you want me	nous inviterons les personnalités que vous voulez entendre,
4005	to speak to and we'll continue to do	et nous continuerons à faire ce que nous faisons.
4006	what we do. Thank you so much. We	Merci infiniment.
4007	launched these conversation cards and	Nous avons lancé ces « cartes de conversation »,
4008	they sold out and we launched them again	elles ont été épuisées, puis relancées encore,
4009	and they sold out again. We launched	et à chaque fois elles se sont de nouveau vendues.
4010	them again and they sold out again	—
4011	because people love playing these with	parce que les gens adorent y jouer
4012	colleagues at work, with friends at	entre collègues, entre amis,
4013	home, and also with family. And we've	ou en famille.
4014	also got a big audience that use them as	Beaucoup de gens s’en servent aussi comme support de journal intime.
4015	journal prompts. Every single time a	À chaque épisode du « Diary of a CEO »,
4016	guest comes on the diary of a CEO, they	l’invité laisse une question
4017	leave a question for the next guest in	pour le suivant dans ce journal.
4018	the diary. And I've sat here with some	Et j’ai reçu ici les personnes les plus incroyables
4019	of the most incredible people in the	du monde, qui ont toutes laissé leurs questions.
4020	world. And they've left all of these	—
4021	questions in the diary. And I've ranked	J’ai classé ces questions
4022	them from one to three in terms of the	de 1 à 3 selon leur niveau de profondeur.
4023	depth. One being a starter question. And	Le niveau 1, c’est pour commencer la discussion.
4024	level three, if you look on the back	Et le niveau 3, si vous regardez au dos,
4025	here, this is a level three, becomes a	c’est une question beaucoup plus profonde
4026	much deeper question that builds even	qui crée encore plus de lien.
4027	more connection. If you turn the cards	Si vous retournez les cartes
4028	over and you scan that QR code, you can	et scannez le code QR,
4029	see who answered the card and watch the	vous verrez qui y a répondu
4030	video of them answering it in real time.	et la vidéo de leur réponse en temps réel.
4031	So, if you would like to get your hands	Donc, si vous voulez vous procurer ces cartes,
4032	on some of these conversation cards, go	rendez‑vous sur thediary.com
4033	to thediary.com or look at the link in	ou cliquez sur le lien dans la description ci‑dessous.
4034	the description below.	—
